{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "batch_size = 1024\n",
    "#dataset = \"MNIST\"\n",
    "#dataset = \"CIFAR10\"\n",
    "dataset = \"CIFAR100\"\n",
    "#dataset = \"FMNIST\"\n",
    "\n",
    "if(dataset == \"MNIST\"):\n",
    "    # 1) MNIST Dataset & Dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "    input_size = (1, 32, 32)\n",
    "\n",
    "if(dataset == \"FMNIST\"):\n",
    "    # 1) MNIST Dataset & Dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset  = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "    input_size = (1, 32, 32)\n",
    "    \n",
    "if(dataset == \"CIFAR10\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    input_size = (3, 32, 32)\n",
    "\n",
    "if(dataset == \"CIFAR100\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = [x for x in range(100)]\n",
    "\n",
    "    input_size = (3, 32, 32)\n",
    "    \n",
    "if(dataset == \"IMAGENET\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageNet(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.ImageNet(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = [x for x in range(100)]\n",
    "\n",
    "    input_size = (3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: a batch of images in shape (B, C, H, W) or a single image in (C, H, W).\n",
    "    \"\"\"\n",
    "    # If it's a batch of images (4D), make a grid first\n",
    "    if len(img_tensor.shape) == 4:\n",
    "        img_tensor = torchvision.utils.make_grid(img_tensor)\n",
    "    # Unnormalize\n",
    "    #img_tensor = unnormalize(img_tensor)\n",
    "    # Convert to numpy\n",
    "    npimg = img_tensor.numpy()\n",
    "    # Transpose from (C, H, W) to (H, W, C)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_bit(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(x)\n",
    "\n",
    "def bit_to_param(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_float_truncate(x: torch.Tensor, e_bits_int: int, m_bits_int: int, scale_int: int) -> torch.Tensor:\n",
    "\n",
    "    sign = x.sign()\n",
    "    abs_x = x.abs().clamp(min=1e-45) / 2**scale_int\n",
    "\n",
    "    #recover the floatint point representation\n",
    "    #exponent \\in {-2**7,..,2**7-1}\n",
    "    #mantissa \\in {1.0,...,2.0}\n",
    "\n",
    "    exponent = torch.floor(torch.log2(abs_x)).clamp(min=1e-45)\n",
    "    mantissa = abs_x / (2**exponent)\n",
    "    \n",
    "    #print(\"exp: \", exponent)\n",
    "    #print(\"mantissa: \", mantissa)\n",
    "\n",
    "    # truncate exponent\n",
    "    # lets parameterize the exponent as a constant value + a variable value\n",
    "    # the constant part is 2**7-1 in standar floating point, but we will learn it\n",
    "    # the variable part \\in {0,..,2**8-1}\n",
    "    # lets say exponent = v_exponent - 2**(bits-1)-1 + c_exponent\n",
    "    # so v_exponent = exponent + 2**(bits-1)-1 - c_exponent\n",
    "    c_exponent = 0 #scale_int\n",
    "    z_exponent = (2**(e_bits_int-1)-1)\n",
    "    if e_bits_int == 0:\n",
    "        z_exponent = 0\n",
    "    v_exponent = exponent + z_exponent - c_exponent\n",
    "    \n",
    "    #print(\"v exponent: \", v_exponent)\n",
    "    \n",
    "    # the valriable part is clamped to the alloted bits\n",
    "    q_min = torch.tensor(float(0)).to(x.device)\n",
    "    q_max = 2**e_bits_int-1\n",
    "    q_v_exponent = torch.clamp(v_exponent, q_min, q_max)\n",
    "    q_exponent = q_v_exponent - z_exponent + c_exponent\n",
    "\n",
    "    #print(\"q v exponent: \", q_v_exponent)\n",
    "    #print(\"q exponent: \", q_exponent)\n",
    "\n",
    "    # truncate mantissa\n",
    "    # this just removes the less significant bits\n",
    "    m_scale = 2.0 ** m_bits_int\n",
    "    q_mantissa = torch.floor(mantissa * m_scale) / m_scale\n",
    "\n",
    "    #print(\"q mantissa \", q_mantissa)\n",
    "\n",
    "    # from quantized floatint point to float\n",
    "    fq_x = sign * (2**q_exponent) * q_mantissa * 2**scale_int\n",
    "    return fq_x\n",
    "\n",
    "class FakeFloatFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e_bits_param, m_bits_param, scale_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, e_bits_param, m_bits_param, scale_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).item())\n",
    "        s_int = int(torch.round(scale_param).item())\n",
    "\n",
    "        out = fake_float_truncate(x, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "        #if(m_bits_int == 0):\n",
    "        #    print(\"input\")\n",
    "        #    print(x)\n",
    "        #    print(\"output\")\n",
    "        #    print(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, e_bits_param, m_bits_param, scale_param = ctx.saved_tensors\n",
    "        \n",
    "        e_bits = param_to_bit(e_bits_param)\n",
    "        m_bits = param_to_bit(m_bits_param)\n",
    "        scale = scale_param\n",
    "                \n",
    "        e_bits_int = int(torch.round(e_bits).item())\n",
    "        m_bits_int = int(torch.round(m_bits).item())\n",
    "        scale_int = int(torch.round(scale).item())\n",
    "\n",
    "        #print(\"shape x: \", x.shape)\n",
    "        #print(\"shape grad_output: \", grad_output.shape)\n",
    "\n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output\n",
    "        \n",
    "        # 1) Gradient wrt x: approximate with central difference\n",
    "        \"\"\"\n",
    "        grad_x = None\n",
    "        if True:\n",
    "            delta = 0.01            \n",
    "\n",
    "            f_plus2  = fake_float_truncate(x + 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_plus   = fake_float_truncate(x + 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus  = fake_float_truncate(x - 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus2 = fake_float_truncate(x - 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "            grad_x = grad_output * der\n",
    "        \"\"\"\n",
    "                \n",
    "        # 2) Gradient wrt e_bits: approximate with central difference\n",
    "        grad_e_bits = None\n",
    "        if e_bits_param.requires_grad:\n",
    "            \n",
    "            if(e_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int    , m_bits_int, scale_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int + 2, m_bits_int, scale_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int - 1, m_bits_int, scale_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int - 2, m_bits_int, scale_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_e_bits = grad_output * der * e_bits\n",
    "        \n",
    "        # 3) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_m_bits = None\n",
    "        if m_bits_param.requires_grad:\n",
    "            \n",
    "            if(m_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int    , scale_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int + 2, scale_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int - 1, scale_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int - 2, scale_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_m_bits = grad_output * der * m_bits\n",
    "       \n",
    "        # 4) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_scale_bits = None\n",
    "        if scale_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int + 2)\n",
    "            f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int + 1)\n",
    "            f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int - 1)\n",
    "            f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int - 2)\n",
    "            \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0 \n",
    "            grad_scale_bits = grad_output * der\n",
    "             \n",
    "        return grad_x, grad_e_bits, grad_m_bits, grad_scale_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_bits  6  m_bits  10  scale  -1\n",
      "in:  tensor([-27.9264])  out:  tensor([-27.9219])\n",
      "e_bits  3  m_bits  18  scale  0\n",
      "in:  tensor([45.8856])  out:  tensor([22.9428])\n",
      "e_bits  4  m_bits  26  scale  1\n",
      "in:  tensor([12.1772])  out:  tensor([12.1772])\n",
      "e_bits  2  m_bits  11  scale  -1\n",
      "in:  tensor([24.9425])  out:  tensor([3.1172])\n",
      "e_bits  2  m_bits  26  scale  1\n",
      "in:  tensor([-24.0082])  out:  tensor([-12.0041])\n",
      "e_bits  5  m_bits  15  scale  2\n",
      "in:  tensor([-7.9340])  out:  tensor([-7.9340])\n",
      "e_bits  1  m_bits  1  scale  2\n",
      "in:  tensor([-43.8377])  out:  tensor([-8.])\n",
      "e_bits  9  m_bits  13  scale  0\n",
      "in:  tensor([48.0057])  out:  tensor([48.0039])\n",
      "e_bits  6  m_bits  23  scale  -1\n",
      "in:  tensor([47.3988])  out:  tensor([47.3988])\n",
      "e_bits  2  m_bits  8  scale  0\n",
      "in:  tensor([16.4251])  out:  tensor([4.0938])\n",
      "e_bits  3  m_bits  3  scale  2\n",
      "in:  tensor([3.6521])  out:  tensor([3.5000])\n",
      "e_bits  2  m_bits  4  scale  2\n",
      "in:  tensor([24.1169])  out:  tensor([24.])\n",
      "e_bits  7  m_bits  22  scale  -2\n",
      "in:  tensor([36.9990])  out:  tensor([36.9989])\n",
      "e_bits  6  m_bits  8  scale  2\n",
      "in:  tensor([-38.4575])  out:  tensor([-38.3750])\n",
      "e_bits  2  m_bits  24  scale  2\n",
      "in:  tensor([29.2173])  out:  tensor([29.2173])\n",
      "e_bits  6  m_bits  16  scale  -1\n",
      "in:  tensor([-18.1061])  out:  tensor([-18.1060])\n",
      "e_bits  6  m_bits  23  scale  1\n",
      "in:  tensor([19.8932])  out:  tensor([19.8932])\n",
      "e_bits  3  m_bits  24  scale  -1\n",
      "in:  tensor([-27.0127])  out:  tensor([-13.5064])\n",
      "e_bits  4  m_bits  4  scale  0\n",
      "in:  tensor([-45.6190])  out:  tensor([-44.])\n",
      "e_bits  6  m_bits  4  scale  1\n",
      "in:  tensor([-39.6935])  out:  tensor([-38.])\n",
      "e_bits  7  m_bits  26  scale  -1\n",
      "in:  tensor([-16.6420])  out:  tensor([-16.6420])\n",
      "e_bits  4  m_bits  15  scale  0\n",
      "in:  tensor([22.3355])  out:  tensor([22.3354])\n",
      "e_bits  10  m_bits  11  scale  -1\n",
      "in:  tensor([-28.2958])  out:  tensor([-28.2891])\n",
      "e_bits  6  m_bits  18  scale  2\n",
      "in:  tensor([-34.6390])  out:  tensor([-34.6389])\n",
      "e_bits  3  m_bits  10  scale  -2\n",
      "in:  tensor([12.9420])  out:  tensor([6.4688])\n",
      "e_bits  3  m_bits  16  scale  2\n",
      "in:  tensor([-40.2457])  out:  tensor([-40.2456])\n",
      "e_bits  3  m_bits  21  scale  -1\n",
      "in:  tensor([-38.3823])  out:  tensor([-9.5956])\n",
      "e_bits  3  m_bits  10  scale  -1\n",
      "in:  tensor([29.4577])  out:  tensor([14.7266])\n",
      "e_bits  2  m_bits  2  scale  -1\n",
      "in:  tensor([-27.0459])  out:  tensor([-3.])\n",
      "e_bits  7  m_bits  10  scale  0\n",
      "in:  tensor([-21.3160])  out:  tensor([-21.3125])\n",
      "e_bits  9  m_bits  6  scale  1\n",
      "in:  tensor([2.4115])  out:  tensor([2.4062])\n",
      "e_bits  1  m_bits  7  scale  2\n",
      "in:  tensor([-3.9009])  out:  tensor([-3.8750])\n",
      "e_bits  5  m_bits  2  scale  -2\n",
      "in:  tensor([-17.4146])  out:  tensor([-16.])\n",
      "e_bits  6  m_bits  7  scale  -1\n",
      "in:  tensor([24.7520])  out:  tensor([24.7500])\n",
      "e_bits  4  m_bits  10  scale  -2\n",
      "in:  tensor([-10.1309])  out:  tensor([-10.1250])\n",
      "e_bits  2  m_bits  27  scale  -2\n",
      "in:  tensor([-14.1230])  out:  tensor([-1.7654])\n",
      "e_bits  0  m_bits  21  scale  1\n",
      "in:  tensor([-6.8768])  out:  tensor([-3.4384])\n",
      "e_bits  5  m_bits  12  scale  0\n",
      "in:  tensor([-26.6341])  out:  tensor([-26.6328])\n",
      "e_bits  0  m_bits  22  scale  -2\n",
      "in:  tensor([38.2107])  out:  tensor([0.2985])\n",
      "e_bits  3  m_bits  6  scale  -1\n",
      "in:  tensor([25.4144])  out:  tensor([12.6250])\n",
      "e_bits  6  m_bits  30  scale  0\n",
      "in:  tensor([8.1059])  out:  tensor([8.1059])\n",
      "e_bits  7  m_bits  22  scale  0\n",
      "in:  tensor([42.8792])  out:  tensor([42.8792])\n",
      "e_bits  8  m_bits  23  scale  1\n",
      "in:  tensor([40.7199])  out:  tensor([40.7199])\n",
      "e_bits  5  m_bits  4  scale  2\n",
      "in:  tensor([43.7085])  out:  tensor([42.])\n",
      "e_bits  0  m_bits  25  scale  2\n",
      "in:  tensor([-40.5733])  out:  tensor([-5.0717])\n",
      "e_bits  2  m_bits  12  scale  2\n",
      "in:  tensor([42.4142])  out:  tensor([21.2070])\n",
      "e_bits  9  m_bits  17  scale  -1\n",
      "in:  tensor([-1.7940])  out:  tensor([-1.7940])\n",
      "e_bits  4  m_bits  11  scale  2\n",
      "in:  tensor([-7.2758])  out:  tensor([-7.2754])\n",
      "e_bits  6  m_bits  16  scale  1\n",
      "in:  tensor([-36.5511])  out:  tensor([-36.5508])\n",
      "e_bits  8  m_bits  28  scale  1\n",
      "in:  tensor([48.0641])  out:  tensor([48.0641])\n",
      "e_bits  10  m_bits  19  scale  0\n",
      "in:  tensor([49.4877])  out:  tensor([49.4877])\n",
      "e_bits  5  m_bits  15  scale  1\n",
      "in:  tensor([36.9821])  out:  tensor([36.9814])\n",
      "e_bits  4  m_bits  5  scale  -2\n",
      "in:  tensor([13.7560])  out:  tensor([13.7500])\n",
      "e_bits  9  m_bits  4  scale  2\n",
      "in:  tensor([17.3207])  out:  tensor([17.])\n",
      "e_bits  9  m_bits  21  scale  -1\n",
      "in:  tensor([-1.0047])  out:  tensor([-1.0047])\n",
      "e_bits  5  m_bits  7  scale  0\n",
      "in:  tensor([40.4136])  out:  tensor([40.2500])\n",
      "e_bits  6  m_bits  23  scale  1\n",
      "in:  tensor([35.7299])  out:  tensor([35.7299])\n",
      "e_bits  8  m_bits  29  scale  -2\n",
      "in:  tensor([-21.2458])  out:  tensor([-21.2458])\n",
      "e_bits  8  m_bits  20  scale  1\n",
      "in:  tensor([35.2822])  out:  tensor([35.2822])\n",
      "e_bits  6  m_bits  21  scale  -2\n",
      "in:  tensor([13.3854])  out:  tensor([13.3854])\n",
      "e_bits  0  m_bits  29  scale  -2\n",
      "in:  tensor([-19.4921])  out:  tensor([-0.3046])\n",
      "e_bits  0  m_bits  18  scale  0\n",
      "in:  tensor([-10.6101])  out:  tensor([-1.3263])\n",
      "e_bits  8  m_bits  23  scale  -1\n",
      "in:  tensor([-32.2051])  out:  tensor([-32.2051])\n",
      "e_bits  1  m_bits  1  scale  -1\n",
      "in:  tensor([-47.7611])  out:  tensor([-1.])\n",
      "e_bits  0  m_bits  21  scale  -2\n",
      "in:  tensor([24.3161])  out:  tensor([0.3799])\n",
      "e_bits  4  m_bits  4  scale  -1\n",
      "in:  tensor([28.2257])  out:  tensor([28.])\n",
      "e_bits  2  m_bits  14  scale  0\n",
      "in:  tensor([36.1514])  out:  tensor([4.5188])\n",
      "e_bits  4  m_bits  6  scale  1\n",
      "in:  tensor([19.5885])  out:  tensor([19.5000])\n",
      "e_bits  3  m_bits  11  scale  -2\n",
      "in:  tensor([13.7974])  out:  tensor([6.8984])\n",
      "e_bits  7  m_bits  20  scale  -2\n",
      "in:  tensor([42.1497])  out:  tensor([42.1496])\n",
      "e_bits  0  m_bits  6  scale  -2\n",
      "in:  tensor([41.2677])  out:  tensor([0.3203])\n",
      "e_bits  3  m_bits  1  scale  -2\n",
      "in:  tensor([18.9904])  out:  tensor([4.])\n",
      "e_bits  7  m_bits  10  scale  1\n",
      "in:  tensor([-6.8901])  out:  tensor([-6.8867])\n",
      "e_bits  2  m_bits  18  scale  0\n",
      "in:  tensor([46.2756])  out:  tensor([5.7844])\n",
      "e_bits  9  m_bits  8  scale  1\n",
      "in:  tensor([-8.2194])  out:  tensor([-8.2188])\n",
      "e_bits  6  m_bits  28  scale  -2\n",
      "in:  tensor([-0.2055])  out:  tensor([-0.2055])\n",
      "e_bits  2  m_bits  14  scale  0\n",
      "in:  tensor([-20.9932])  out:  tensor([-5.2483])\n",
      "e_bits  2  m_bits  30  scale  2\n",
      "in:  tensor([-34.9164])  out:  tensor([-17.4582])\n",
      "e_bits  5  m_bits  30  scale  -2\n",
      "in:  tensor([25.6324])  out:  tensor([25.6324])\n",
      "e_bits  1  m_bits  26  scale  0\n",
      "in:  tensor([41.5398])  out:  tensor([2.5962])\n",
      "e_bits  10  m_bits  24  scale  -2\n",
      "in:  tensor([-39.8941])  out:  tensor([-39.8941])\n",
      "e_bits  7  m_bits  27  scale  -2\n",
      "in:  tensor([22.7174])  out:  tensor([22.7174])\n",
      "e_bits  0  m_bits  13  scale  1\n",
      "in:  tensor([-24.5694])  out:  tensor([-3.0710])\n",
      "e_bits  2  m_bits  3  scale  1\n",
      "in:  tensor([39.8468])  out:  tensor([9.])\n",
      "e_bits  7  m_bits  19  scale  0\n",
      "in:  tensor([19.0382])  out:  tensor([19.0382])\n",
      "e_bits  4  m_bits  18  scale  0\n",
      "in:  tensor([-47.4132])  out:  tensor([-47.4132])\n",
      "e_bits  1  m_bits  25  scale  0\n",
      "in:  tensor([27.2475])  out:  tensor([3.4059])\n",
      "e_bits  2  m_bits  14  scale  -1\n",
      "in:  tensor([-10.0313])  out:  tensor([-2.5078])\n",
      "e_bits  5  m_bits  8  scale  2\n",
      "in:  tensor([32.9816])  out:  tensor([32.8750])\n",
      "e_bits  9  m_bits  21  scale  2\n",
      "in:  tensor([49.3222])  out:  tensor([49.3222])\n",
      "e_bits  2  m_bits  6  scale  0\n",
      "in:  tensor([32.8391])  out:  tensor([4.0625])\n",
      "e_bits  2  m_bits  19  scale  0\n",
      "in:  tensor([-15.3943])  out:  tensor([-7.6971])\n",
      "e_bits  3  m_bits  16  scale  -1\n",
      "in:  tensor([43.3838])  out:  tensor([10.8458])\n",
      "e_bits  4  m_bits  6  scale  1\n",
      "in:  tensor([-17.3333])  out:  tensor([-17.2500])\n",
      "e_bits  8  m_bits  27  scale  0\n",
      "in:  tensor([10.8968])  out:  tensor([10.8968])\n",
      "e_bits  7  m_bits  15  scale  0\n",
      "in:  tensor([22.0431])  out:  tensor([22.0430])\n",
      "e_bits  0  m_bits  7  scale  2\n",
      "in:  tensor([-32.7890])  out:  tensor([-4.0938])\n",
      "e_bits  2  m_bits  19  scale  -1\n",
      "in:  tensor([2.9452])  out:  tensor([2.9452])\n",
      "e_bits  3  m_bits  22  scale  -1\n",
      "in:  tensor([34.6056])  out:  tensor([8.6514])\n",
      "e_bits  10  m_bits  20  scale  2\n",
      "in:  tensor([-46.2142])  out:  tensor([-46.2142])\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    in_test = (torch.rand(1) - 0.5)*100.0\n",
    "    e_bits = int(torch.round(torch.rand(1)*10).item())\n",
    "    m_bits = int(torch.round(torch.rand(1)*30).item())\n",
    "    scale = int(torch.round((torch.rand(1) - 0.5)*5.0).item())\n",
    "    out_test = fake_float_truncate(in_test, e_bits, m_bits, scale)\n",
    "    print(\"e_bits \", e_bits, \" m_bits \", m_bits, \" scale \", scale)\n",
    "    print(\"in: \", in_test, \" out: \", out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_fixed_truncate2(x: torch.Tensor, bits_int: int, scale_int: int, zero_point_int: int) -> torch.Tensor:\n",
    "    \n",
    "    qmin = 0\n",
    "    qmax = 2**bits_int - 1\n",
    "    \n",
    "    #from float to fixed point, and quantize accordingly\n",
    "    q_x = torch.clamp(torch.round(x * 2**(scale_int + bits_int//2) + 2**(bits_int-1) + zero_point_int), qmin, qmax)\n",
    "\n",
    "    # from quantized fixed point to float\n",
    "    fq_x = (q_x - 2**(bits_int-1) - zero_point_int) / 2**(scale_int + bits_int//2)\n",
    "        \n",
    "    return fq_x\n",
    "\n",
    "def fake_fixed_truncate(x: torch.Tensor, bits_int: int, scale_int: int, zero_point_int: int) -> torch.Tensor:\n",
    "    \n",
    "    qmin = 0\n",
    "    qmax = 2**bits_int - 1\n",
    "    \n",
    "    mantissa = x * 2**(scale_int + bits_int//2) + zero_point_int + 2**(bits_int-1)\n",
    "    \n",
    "    #from float to fixed point, and quantize accordingly\n",
    "    q_x = torch.clamp(torch.round(mantissa), qmin, qmax)\n",
    "\n",
    "    # from quantized fixed point to float\n",
    "    fq_x = (q_x - 2**(bits_int-1) - zero_point_int) / 2**(scale_int + bits_int//2)\n",
    "        \n",
    "    return fq_x\n",
    "\n",
    "class FakeFixedFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, bits_param, scale_param, zero_point_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, bits_param, scale_param, zero_point_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        bits_int = int(torch.round(param_to_bit(bits_param)).item())\n",
    "        scale_int = int(torch.round(scale_param).item())\n",
    "        zero_point_int = int(torch.round(zero_point_param).item())\n",
    "\n",
    "        out = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int)\n",
    "        \n",
    "        #print(\"input\")\n",
    "        #print(x)\n",
    "        #print(\"output\")\n",
    "        #print(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, bits_param, scale_param, zero_point_param = ctx.saved_tensors\n",
    "        \n",
    "        bits = param_to_bit(bits_param)\n",
    "        scale = scale_param\n",
    "        zero_point = zero_point_param\n",
    "                \n",
    "        bits_int = int(torch.round(bits).item())\n",
    "        scale_int = int(torch.round(scale).item())\n",
    "        zero_point_int = int(torch.round(zero_point).item())\n",
    "\n",
    "        #print(\"shape x: \", x.shape)\n",
    "        #print(\"shape grad_output: \", grad_output.shape)\n",
    "\n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output\n",
    "        \n",
    "        # 1) Gradient wrt x: approximate with central difference\n",
    "        \"\"\"\n",
    "        grad_x = None\n",
    "        if True:\n",
    "            delta = 0.01            \n",
    "\n",
    "            f_plus2  = fake_float_truncate(x + 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_plus   = fake_float_truncate(x + 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus  = fake_float_truncate(x - 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus2 = fake_float_truncate(x - 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "            grad_x = grad_output * der\n",
    "        \"\"\"\n",
    "                \n",
    "        # 2) Gradient wrt bits: approximate with central difference\n",
    "        grad_bits = None\n",
    "        if bits_param.requires_grad:\n",
    "            if(bits_int < 2):\n",
    "                f_plus   = fake_fixed_truncate(x, bits_int + 1, scale_int, zero_point_int)\n",
    "                f_minus  = fake_fixed_truncate(x, bits_int    , scale_int, zero_point_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_fixed_truncate(x, bits_int + 2, scale_int, zero_point_int)\n",
    "                f_plus   = fake_fixed_truncate(x, bits_int + 1, scale_int, zero_point_int)\n",
    "                f_minus  = fake_fixed_truncate(x, bits_int - 1, scale_int, zero_point_int)\n",
    "                f_minus2 = fake_fixed_truncate(x, bits_int - 2, scale_int, zero_point_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_bits = grad_output * der * bits\n",
    "        \n",
    "        # 3) Gradient wrt scale: approximate with central difference\n",
    "        grad_scale_bits = None\n",
    "        if scale_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_fixed_truncate(x, bits_int, scale_int + 2, zero_point_int)\n",
    "            f_plus   = fake_fixed_truncate(x, bits_int, scale_int + 1, zero_point_int)\n",
    "            f_minus  = fake_fixed_truncate(x, bits_int, scale_int - 1, zero_point_int)\n",
    "            f_minus2 = fake_fixed_truncate(x, bits_int, scale_int - 2, zero_point_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_scale_bits = grad_output * der\n",
    "       \n",
    "        # 4) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_zero_point_bits = None\n",
    "        if zero_point_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int + 2)\n",
    "            f_plus   = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int + 1)\n",
    "            f_minus  = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int - 1)\n",
    "            f_minus2 = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int - 2)\n",
    "            \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0 \n",
    "            grad_zero_point_bits = grad_output * der\n",
    "             \n",
    "        return grad_x, grad_bits, grad_scale_bits, grad_zero_point_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits  30  scale  2  zero_point  0\n",
      "in:  tensor([40.9733])  out  tensor([40.9731])\n",
      "bits  1  scale  0  zero_point  0\n",
      "in:  tensor([17.7812])  out  tensor([0.])\n",
      "bits  19  scale  0  zero_point  0\n",
      "in:  tensor([21.3523])  out  tensor([21.3516])\n",
      "bits  1  scale  2  zero_point  0\n",
      "in:  tensor([34.5062])  out  tensor([0.])\n",
      "bits  10  scale  1  zero_point  0\n",
      "in:  tensor([11.3831])  out  tensor([7.9844])\n",
      "bits  27  scale  1  zero_point  0\n",
      "in:  tensor([22.2408])  out  tensor([22.2407])\n",
      "bits  24  scale  1  zero_point  0\n",
      "in:  tensor([-12.3456])  out  tensor([-12.3457])\n",
      "bits  12  scale  2  zero_point  0\n",
      "in:  tensor([-6.2339])  out  tensor([-6.2344])\n",
      "bits  11  scale  -2  zero_point  0\n",
      "in:  tensor([43.2922])  out  tensor([43.2500])\n",
      "bits  25  scale  0  zero_point  0\n",
      "in:  tensor([21.3528])  out  tensor([21.3525])\n",
      "bits  25  scale  2  zero_point  0\n",
      "in:  tensor([3.1001])  out  tensor([3.1001])\n",
      "bits  7  scale  0  zero_point  0\n",
      "in:  tensor([-44.8267])  out  tensor([-8.])\n",
      "bits  22  scale  -1  zero_point  0\n",
      "in:  tensor([-35.6248])  out  tensor([-35.6250])\n",
      "bits  14  scale  1  zero_point  0\n",
      "in:  tensor([46.6885])  out  tensor([31.9961])\n",
      "bits  19  scale  0  zero_point  0\n",
      "in:  tensor([13.6550])  out  tensor([13.6543])\n",
      "bits  21  scale  0  zero_point  0\n",
      "in:  tensor([-23.0291])  out  tensor([-23.0293])\n",
      "bits  32  scale  -1  zero_point  0\n",
      "in:  tensor([-22.3460])  out  tensor([-22.3477])\n",
      "bits  11  scale  -1  zero_point  0\n",
      "in:  tensor([-37.2973])  out  tensor([-37.3125])\n",
      "bits  21  scale  0  zero_point  0\n",
      "in:  tensor([21.9743])  out  tensor([21.9746])\n",
      "bits  4  scale  1  zero_point  0\n",
      "in:  tensor([-10.1933])  out  tensor([-1.])\n",
      "bits  29  scale  -1  zero_point  0\n",
      "in:  tensor([4.9856])  out  tensor([4.9844])\n",
      "bits  7  scale  1  zero_point  0\n",
      "in:  tensor([30.1205])  out  tensor([3.9375])\n",
      "bits  17  scale  1  zero_point  0\n",
      "in:  tensor([31.5724])  out  tensor([31.5723])\n",
      "bits  11  scale  -2  zero_point  0\n",
      "in:  tensor([18.3585])  out  tensor([18.3750])\n",
      "bits  22  scale  -2  zero_point  0\n",
      "in:  tensor([-23.6844])  out  tensor([-23.6836])\n",
      "bits  19  scale  -1  zero_point  0\n",
      "in:  tensor([37.5221])  out  tensor([37.5234])\n",
      "bits  28  scale  -1  zero_point  0\n",
      "in:  tensor([30.2132])  out  tensor([30.2129])\n",
      "bits  14  scale  -1  zero_point  0\n",
      "in:  tensor([-39.1253])  out  tensor([-39.1250])\n",
      "bits  9  scale  -2  zero_point  0\n",
      "in:  tensor([36.8622])  out  tensor([36.7500])\n",
      "bits  9  scale  -1  zero_point  0\n",
      "in:  tensor([-24.0920])  out  tensor([-24.1250])\n",
      "bits  15  scale  -1  zero_point  0\n",
      "in:  tensor([43.7922])  out  tensor([43.7969])\n",
      "bits  26  scale  1  zero_point  0\n",
      "in:  tensor([-10.6681])  out  tensor([-10.6681])\n",
      "bits  30  scale  1  zero_point  0\n",
      "in:  tensor([29.1883])  out  tensor([29.1885])\n",
      "bits  14  scale  0  zero_point  0\n",
      "in:  tensor([5.8420])  out  tensor([5.8438])\n",
      "bits  8  scale  0  zero_point  0\n",
      "in:  tensor([-4.9377])  out  tensor([-4.9375])\n",
      "bits  25  scale  -2  zero_point  0\n",
      "in:  tensor([-22.4716])  out  tensor([-22.4717])\n",
      "bits  15  scale  0  zero_point  0\n",
      "in:  tensor([-42.5850])  out  tensor([-42.5859])\n",
      "bits  21  scale  0  zero_point  0\n",
      "in:  tensor([-32.7790])  out  tensor([-32.7793])\n",
      "bits  0  scale  -1  zero_point  0\n",
      "in:  tensor([43.1935])  out  tensor([-1.])\n",
      "bits  7  scale  -2  zero_point  0\n",
      "in:  tensor([4.6102])  out  tensor([4.5000])\n",
      "bits  16  scale  -2  zero_point  0\n",
      "in:  tensor([31.4324])  out  tensor([31.4375])\n",
      "bits  23  scale  -1  zero_point  0\n",
      "in:  tensor([46.7050])  out  tensor([46.7051])\n",
      "bits  26  scale  -1  zero_point  0\n",
      "in:  tensor([-18.6233])  out  tensor([-18.6235])\n",
      "bits  2  scale  -1  zero_point  0\n",
      "in:  tensor([-34.1078])  out  tensor([-2.])\n",
      "bits  3  scale  0  zero_point  0\n",
      "in:  tensor([39.6163])  out  tensor([1.5000])\n",
      "bits  20  scale  1  zero_point  0\n",
      "in:  tensor([-0.6012])  out  tensor([-0.6011])\n",
      "bits  9  scale  0  zero_point  0\n",
      "in:  tensor([18.8337])  out  tensor([15.9375])\n",
      "bits  18  scale  -1  zero_point  0\n",
      "in:  tensor([-34.4347])  out  tensor([-34.4336])\n",
      "bits  21  scale  2  zero_point  0\n",
      "in:  tensor([34.4954])  out  tensor([34.4954])\n",
      "bits  11  scale  0  zero_point  0\n",
      "in:  tensor([5.0523])  out  tensor([5.0625])\n",
      "bits  13  scale  2  zero_point  0\n",
      "in:  tensor([47.8228])  out  tensor([15.9961])\n",
      "bits  4  scale  2  zero_point  0\n",
      "in:  tensor([47.8728])  out  tensor([0.4375])\n",
      "bits  5  scale  1  zero_point  0\n",
      "in:  tensor([-21.8464])  out  tensor([-2.])\n",
      "bits  15  scale  0  zero_point  0\n",
      "in:  tensor([-39.5419])  out  tensor([-39.5391])\n",
      "bits  12  scale  2  zero_point  0\n",
      "in:  tensor([8.2088])  out  tensor([7.9961])\n",
      "bits  5  scale  0  zero_point  0\n",
      "in:  tensor([-38.7854])  out  tensor([-4.])\n",
      "bits  13  scale  -1  zero_point  0\n",
      "in:  tensor([18.7368])  out  tensor([18.7500])\n",
      "bits  27  scale  1  zero_point  0\n",
      "in:  tensor([24.9692])  out  tensor([24.9692])\n",
      "bits  29  scale  2  zero_point  0\n",
      "in:  tensor([-44.9119])  out  tensor([-44.9119])\n",
      "bits  28  scale  0  zero_point  0\n",
      "in:  tensor([-27.8561])  out  tensor([-27.8560])\n",
      "bits  28  scale  2  zero_point  0\n",
      "in:  tensor([-24.4440])  out  tensor([-24.4440])\n",
      "bits  27  scale  0  zero_point  0\n",
      "in:  tensor([36.0119])  out  tensor([36.0117])\n",
      "bits  29  scale  -2  zero_point  0\n",
      "in:  tensor([30.0711])  out  tensor([30.0703])\n",
      "bits  1  scale  -2  zero_point  0\n",
      "in:  tensor([-45.3565])  out  tensor([-4.])\n",
      "bits  27  scale  -2  zero_point  0\n",
      "in:  tensor([8.6463])  out  tensor([8.6445])\n",
      "bits  24  scale  1  zero_point  0\n",
      "in:  tensor([45.8136])  out  tensor([45.8136])\n",
      "bits  8  scale  0  zero_point  0\n",
      "in:  tensor([49.7478])  out  tensor([7.9375])\n",
      "bits  18  scale  0  zero_point  0\n",
      "in:  tensor([-8.2848])  out  tensor([-8.2852])\n",
      "bits  20  scale  -1  zero_point  0\n",
      "in:  tensor([25.0425])  out  tensor([25.0430])\n",
      "bits  1  scale  2  zero_point  0\n",
      "in:  tensor([5.7835])  out  tensor([0.])\n",
      "bits  18  scale  1  zero_point  0\n",
      "in:  tensor([-45.0221])  out  tensor([-45.0225])\n",
      "bits  6  scale  1  zero_point  0\n",
      "in:  tensor([37.2505])  out  tensor([1.9375])\n",
      "bits  4  scale  0  zero_point  0\n",
      "in:  tensor([-22.6027])  out  tensor([-2.])\n",
      "bits  17  scale  2  zero_point  0\n",
      "in:  tensor([-4.1772])  out  tensor([-4.1768])\n",
      "bits  8  scale  0  zero_point  0\n",
      "in:  tensor([31.3328])  out  tensor([7.9375])\n",
      "bits  10  scale  0  zero_point  0\n",
      "in:  tensor([49.9663])  out  tensor([15.9688])\n",
      "bits  29  scale  0  zero_point  0\n",
      "in:  tensor([22.8429])  out  tensor([22.8438])\n",
      "bits  5  scale  -1  zero_point  0\n",
      "in:  tensor([-7.7356])  out  tensor([-7.5000])\n",
      "bits  14  scale  2  zero_point  0\n",
      "in:  tensor([19.4282])  out  tensor([15.9980])\n",
      "bits  16  scale  -2  zero_point  0\n",
      "in:  tensor([-35.6513])  out  tensor([-35.6562])\n",
      "bits  28  scale  2  zero_point  0\n",
      "in:  tensor([37.0555])  out  tensor([37.0554])\n",
      "bits  7  scale  -1  zero_point  0\n",
      "in:  tensor([-34.8211])  out  tensor([-16.])\n",
      "bits  7  scale  -1  zero_point  0\n",
      "in:  tensor([-6.4194])  out  tensor([-6.5000])\n",
      "bits  5  scale  -2  zero_point  0\n",
      "in:  tensor([47.8686])  out  tensor([15.])\n",
      "bits  22  scale  -2  zero_point  0\n",
      "in:  tensor([-33.4678])  out  tensor([-33.4688])\n",
      "bits  25  scale  1  zero_point  0\n",
      "in:  tensor([-8.4350])  out  tensor([-8.4351])\n",
      "bits  5  scale  0  zero_point  0\n",
      "in:  tensor([5.9639])  out  tensor([3.7500])\n",
      "bits  18  scale  -1  zero_point  0\n",
      "in:  tensor([16.9161])  out  tensor([16.9180])\n",
      "bits  2  scale  0  zero_point  0\n",
      "in:  tensor([-20.1068])  out  tensor([-1.])\n",
      "bits  25  scale  1  zero_point  0\n",
      "in:  tensor([-37.2341])  out  tensor([-37.2341])\n",
      "bits  8  scale  2  zero_point  0\n",
      "in:  tensor([-14.3239])  out  tensor([-2.])\n",
      "bits  12  scale  2  zero_point  0\n",
      "in:  tensor([3.9740])  out  tensor([3.9727])\n",
      "bits  13  scale  1  zero_point  0\n",
      "in:  tensor([-35.2394])  out  tensor([-32.])\n",
      "bits  8  scale  -2  zero_point  0\n",
      "in:  tensor([-20.7957])  out  tensor([-20.7500])\n",
      "bits  7  scale  2  zero_point  0\n",
      "in:  tensor([3.5334])  out  tensor([1.9688])\n",
      "bits  29  scale  2  zero_point  0\n",
      "in:  tensor([34.4277])  out  tensor([34.4277])\n",
      "bits  14  scale  0  zero_point  0\n",
      "in:  tensor([19.2741])  out  tensor([19.2734])\n",
      "bits  18  scale  -1  zero_point  0\n",
      "in:  tensor([-3.9731])  out  tensor([-3.9727])\n",
      "bits  2  scale  1  zero_point  0\n",
      "in:  tensor([17.6484])  out  tensor([0.2500])\n",
      "bits  30  scale  -2  zero_point  0\n",
      "in:  tensor([3.7641])  out  tensor([3.7656])\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    bits = int(torch.round(torch.rand(1)*32).item())\n",
    "    scale = int(torch.round((torch.rand(1) - 0.5)*5.0).item())\n",
    "    zero_point = int(torch.round((torch.rand(1) - 0.5)*0.0).item())\n",
    "    in_test = (torch.rand(1)-0.5)*100.0\n",
    "    out_test = fake_fixed_truncate(in_test, bits, scale, zero_point)\n",
    "    print(\"bits \", bits, \" scale \", scale, \" zero_point \", zero_point)\n",
    "    print(\"in: \", in_test, \" out \", out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### differentiable Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoundSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        return grad_output\n",
    "    \n",
    "class RoundFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.round(input + 2*delta)\n",
    "        f_plus   = torch.round(input + 1*delta)\n",
    "        f_minus  = torch.round(input - 1*delta)\n",
    "        f_minus2 = torch.round(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "class RoundSIG(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function that does a hard round in forward,\n",
    "    but uses a sigmoid-based approximation for the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, alpha=10.0):\n",
    "        \"\"\"\n",
    "        Forward pass: returns torch.round(input).\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.alpha = alpha\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: approximate the gradient of round(x)\n",
    "        with the derivative of a sigmoid centered at the fractional midpoint (0.5).\n",
    "        \"\"\"\n",
    "        (input,) = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "\n",
    "        # Fractional part\n",
    "        frac = input - torch.floor(input)\n",
    "\n",
    "        # Sigmoid of (fractional_part - 0.5), scaled by alpha\n",
    "        s = torch.sigmoid(alpha * (frac - 0.5))\n",
    "\n",
    "        # Derivative of sigmoid = alpha * s * (1 - s)\n",
    "        grad_input = alpha * s * (1 - s) * grad_output\n",
    "        return grad_input, None  # alpha is not a tensor that requires grad\n",
    "    \n",
    "def diff_round(x):\n",
    "    return RoundSTE.apply(x)\n",
    "    #return RoundFDE.apply(x)\n",
    "    #return RoundSIG.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable Floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloorSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass uses standard floor\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-through pass: just return the gradient as-is\n",
    "        return grad_output\n",
    "\n",
    "class FloorFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.floor(input + 2*delta)\n",
    "        f_plus   = torch.floor(input + 1*delta)\n",
    "        f_minus  = torch.floor(input - 1*delta)\n",
    "        f_minus2 = torch.floor(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "def diff_floor(input):\n",
    "    return FloorSTE.apply(input)\n",
    "    #return FloorFDE.apply(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxObserver(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We store running min/max\n",
    "        self.register_buffer(\"min_val\", torch.tensor(float(\"inf\")))\n",
    "        self.register_buffer(\"max_val\", torch.tensor(float(\"-inf\")))\n",
    "        # You could also store averaging stats, etc.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update running min/max\n",
    "        self.min_val = torch.min(self.min_val, x.detach().min())\n",
    "        self.max_val = torch.max(self.max_val, x.detach().max())\n",
    "        return x  # Just pass through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed point quanizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, observer, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.observer = observer\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        \n",
    "        # 1) Get min/max from observer\n",
    "        min_val = self.observer.min_val\n",
    "        max_val = self.observer.max_val\n",
    "\n",
    "        # If they're not valid, skip\n",
    "        #if min_val >= max_val:\n",
    "        #    return x\n",
    "\n",
    "        # 2) Compute scale and zero_point\n",
    "        # For an unsigned 4-bit range, we can hold values 0..15\n",
    "        # qmin, qmax = 0, (1 << b_int) - 1  # e.g. 0..15\n",
    "        qmin, qmax = torch.tensor(float(0)), 2**b_int - 1  # e.g. 0..15\n",
    "        \n",
    "        qmin = qmin.to(x.device)\n",
    "        #qmax = qmax.to(x.device)\n",
    "        max_val = max_val.to(x.device)\n",
    "        min_val = min_val.to(x.device)\n",
    "\n",
    "        # Typical formula for scale/zero-point:\n",
    "        scale = (max_val - min_val) / float(qmax - qmin)\n",
    "        zero_point = qmin - diff_round(min_val / scale)\n",
    "\n",
    "        # 3) Quantize (in floating point)\n",
    "        # clamp to range of [qmin, qmax]\n",
    "        q_x = torch.clamp(diff_round(x / scale + zero_point), qmin, qmax)\n",
    "\n",
    "        # 4) Dequantize back to float\n",
    "        fq_x = (q_x - zero_point) * scale\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"bits: \", self.bits.detach().item())\n",
    "        \n",
    "class FixedPointFakeQuantize2(nn.Module):\n",
    "    def __init__(self, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(bits//2)), requires_grad=requires_grad)\n",
    "        self.zero_point = nn.Parameter(torch.tensor(float(2**(bits//2-1)-1)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        bits_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        zero_point_int = diff_round(self.zero_point)\n",
    "        \n",
    "        qmin = torch.tensor(float(0)).to(x.device)\n",
    "        qmax = 2**bits_int - 1  # e.g. 0..15\n",
    "        \n",
    "        #from float to fixed point, and quantize accordingly\n",
    "        q_x = torch.clamp(diff_round(x * 2**scale_int + zero_point_int), qmin, qmax)\n",
    "\n",
    "        # from quantized fixed point to float\n",
    "        fq_x = (q_x - zero_point_int) / 2**scale_int\n",
    "        \n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"bits: \", self.bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())\n",
    "        print(\"zero point: \", self.zero_point.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatingPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, m_bits=23, e_bits=8, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.e_bits = nn.Parameter(torch.tensor(float(e_bits)), requires_grad=requires_grad)\n",
    "        self.m_bits = nn.Parameter(torch.tensor(float(m_bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(0)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e_bits_int = torch.clamp(diff_round(self.e_bits), 0, 32)\n",
    "        m_bits_int = torch.clamp(diff_round(self.m_bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        \n",
    "        sign = x.sign()\n",
    "        abs_x = x.abs().clamp(min=1e-45)\n",
    "\n",
    "        #recover the floatint point representation\n",
    "        #exponent \\in {-2**7,..,2**7-1}\n",
    "        #mantissa \\in {1.0,...,2.0}\n",
    "\n",
    "        exponent = diff_floor(torch.log2(abs_x)).clamp(min=1e-45)\n",
    "        mantissa = abs_x / (2**exponent)\n",
    "    \n",
    "        # truncate exponent\n",
    "        # lets parameterize the exponent as a constant value + a variable value\n",
    "        # the constant part is 2**7-1 in standar floating point, but we will learn it\n",
    "        # the variable part \\in {0,..,2**8-1}\n",
    "        # lets say exponent = v_exponent - 2**(bits-1)-1 + c_exponent\n",
    "        # so v_exponent = exponent + 2**(bits-1)-1 - c_exponent\n",
    "        c_exponent = scale_int\n",
    "        v_exponent = exponent + (2**(e_bits_int-1)-1) - c_exponent\n",
    "        \n",
    "        # the valriable part is clamped to the alloted bits\n",
    "        q_min = torch.tensor(float(0)).to(x.device)\n",
    "        q_max = 2**e_bits_int-1\n",
    "        q_exponent = torch.clamp(v_exponent, q_min, q_max) - (2**(e_bits_int-1)-1) + c_exponent\n",
    "    \n",
    "        # truncate mantissa\n",
    "        # this just removes the less significant bits\n",
    "        m_scale = 2.0 ** m_bits_int\n",
    "        q_mantissa = diff_floor(mantissa * m_scale) / m_scale\n",
    "    \n",
    "        # from quantized floatint point to float\n",
    "        fq_x = sign * (2**q_exponent) * q_mantissa\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.e_bits, self.m_bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"e_bits: \", self.e_bits.detach().item())\n",
    "        print(\"m_bits: \", self.m_bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float32 example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * input_size[1]//8 * input_size[2]//8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantWrapper(nn.Module):\n",
    "    def __init__(self, module, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.observer = MinMaxObserver()\n",
    "        self.fake_quant_input = FixedPointFakeQuantize(self.observer, requires_grad=optimizeQuant)\n",
    "        self.fake_quant_weight = FixedPointFakeQuantize(self.observer, requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_input = FixedPointFakeQuantize2(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_weight = FixedPointFakeQuantize2(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_input = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_weight = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.observer(x)\n",
    "        x = self.fake_quant_input(x)\n",
    "        w = self.fake_quant_weight(self.module.weight)\n",
    "        b = self.module.bias\n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            return F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            return F.linear(x, w, b)\n",
    "        else:\n",
    "            return self.module(x)\n",
    "        \n",
    "    def getBits(self):\n",
    "        return self.fake_quant_input.getBits() + self.fake_quant_weight.getBits()\n",
    "        #return self.fake_quant_weight.getBits()\n",
    "    \n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        self.fake_quant_input.printParams()\n",
    "        print(\"weight quant params: \")\n",
    "        self.fake_quant_weight.printParams()\n",
    "\n",
    "class QuantWrapperFloatingPoint(nn.Module):\n",
    "    def __init__(self, module, e_bits=5, m_bits=10, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "        self.input_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        self.input_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        self.input_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "        \n",
    "        self.weight_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "        \n",
    "        #self.bias_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=False)\n",
    "        #self.bias_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=False)\n",
    "        #self.bias_scale = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        \n",
    "        #self.output_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        #self.output_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        #self.output_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = FakeFloatFunction.apply(x,   self.input_e_bits_param, self.input_m_bits_param, self.input_scale)\n",
    "        \n",
    "        if hasattr(self.module, 'weight') and self.module.weight != None:\n",
    "            w = FakeFloatFunction.apply(self.module.weight, self.weight_e_bits_param, self.weight_m_bits_param, self.weight_scale)\n",
    "        else:\n",
    "            w = None\n",
    "        \n",
    "        if hasattr(self.module, 'bias') and self.module.bias != None:\n",
    "            b = self.module.bias #FakeFloatFunction.apply(self.module.bias, self.bias_e_bits_param, self.bias_m_bits_param, self.bias_scale)\n",
    "        else:\n",
    "            b = None\n",
    "        \n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            out = F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            out = F.linear(x, w, b)\n",
    "        #else:\n",
    "        #    out = self.module(x)\n",
    "        \n",
    "        #out = FakeFloatFunction.apply(out, self.output_e_bits_param, self.output_m_bits_param, self.output_scale)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def getBits(self):\n",
    "        return [param_to_bit(self.input_e_bits_param) + param_to_bit(self.input_m_bits_param) + 1, param_to_bit(self.weight_e_bits_param) + param_to_bit(self.weight_m_bits_param) + 1]\n",
    "        #return [param_to_bit(self.weight_e_bits_param) + param_to_bit(self.weight_m_bits_param) + 1, param_to_bit(self.bias_e_bits_param) + param_to_bit(self.bias_m_bits_param) + 1, param_to_bit(self.output_e_bits_param) + param_to_bit(self.output_m_bits_param) + 1]\n",
    "        #return [param_to_bit(self.weight_e_bits_param) + param_to_bit(self.weight_m_bits_param) + 1, param_to_bit(self.output_e_bits_param) + param_to_bit(self.output_m_bits_param) + 1]\n",
    "\n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        print(\"e bits: \", param_to_bit(self.input_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.input_m_bits_param).detach().item(), \" scale \", self.input_scale.detach().item())\n",
    "        print(\"weight quant params: \")\n",
    "        print(\"e bits \", param_to_bit(self.weight_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.weight_m_bits_param).detach().item(), \" scale \", self.weight_scale.detach().item())\n",
    "        #print(\"bias quant params: \")\n",
    "        #print(\"e bits \", param_to_bit(self.bias_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.bias_m_bits_param).detach().item(), \" scale \", self.bias_scale.detach().item())\n",
    "        #print(\"output quant params: \")\n",
    "        #print(\"e bits: \", param_to_bit(self.output_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.output_m_bits_param).detach().item(), \" scale \", self.output_scale.detach().item())\n",
    "        \n",
    "class QuantWrapperFixedPoint(nn.Module):\n",
    "    def __init__(self, module, bits=32, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "        self.input_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        self.input_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.input_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "    \n",
    "        self.weight_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.weight_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        \n",
    "        self.bias_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=False)\n",
    "        self.bias_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=False)\n",
    "        self.bias_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=False)\n",
    "    \n",
    "        #self.output_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        #self.output_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        #self.output_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "            \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = FakeFixedFunction.apply(x, self.input_bits_param, self.input_scale, self.input_zero_point)\n",
    "        \n",
    "        if hasattr(self.module, 'weight') and self.module.weight != None:\n",
    "            w = FakeFixedFunction.apply(self.module.weight, self.weight_bits_param, self.weight_scale, self.weight_zero_point)\n",
    "        else:\n",
    "            w = None\n",
    "\n",
    "        if hasattr(self.module, 'bias') and self.module.bias != None:\n",
    "            b = FakeFixedFunction.apply(self.module.bias, self.bias_bits_param, self.bias_scale, self.bias_zero_point)\n",
    "        else:\n",
    "            b = None\n",
    "        \n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            out = F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            out = F.linear(x, w, b)\n",
    "        #else:\n",
    "        #    out = self.module(x)\n",
    "        \n",
    "        #out = FakeFixedFunction.apply(out, self.output_bits_param, self.output_scale, self.output_zero_point)    \n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def getBits(self):\n",
    "        return [param_to_bit(self.input_bits_param), param_to_bit(self.weight_bits_param)]\n",
    "        #return [param_to_bit(self.weight_bits_param), param_to_bit(self.bias_bits_param), param_to_bit(self.output_bits_param)]\n",
    "        #return [param_to_bit(self.weight_bits_param), param_to_bit(self.output_bits_param)]\n",
    "\n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.input_bits_param).detach().item(), \" scale \", self.input_scale.detach().item(), \" zero point \", self.input_zero_point.detach().item())\n",
    "        print(\"weight quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.weight_bits_param).detach().item(), \" scale \", self.weight_scale.detach().item(), \" zero point \", self.weight_zero_point.detach().item())\n",
    "        print(\"bias quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.bias_bits_param).detach().item(), \" scale \", self.bias_scale.detach().item(), \" zero point \", self.bias_zero_point.detach().item())\n",
    "        #print(\"output quant params: \")\n",
    "        #print(\"bits: \", param_to_bit(self.output_bits_param).detach().item(), \" scale \", self.output_scale.detach().item(), \" zero point \", self.output_zero_point.detach().item())\n",
    "        \n",
    "class QuantSimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, QuantClass, num_classes=10, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        #self.input = QuantClass(nn.Identity(), optimizeQuant=optimizeQuant)\n",
    "        self.conv1 = QuantClass(nn.Conv2d(input_size[0], 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = QuantClass(nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = QuantClass(nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = QuantClass(nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5 = QuantClass(nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = QuantClass(nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.fc7 = QuantClass(nn.Linear(256 * input_size[1]//4 * input_size[2]//4, 512), optimizeQuant=optimizeQuant)\n",
    "        self.bn7 = nn.BatchNorm1d(512)\n",
    "        self.fc8 = QuantClass(nn.Linear(512, 512), optimizeQuant=optimizeQuant)\n",
    "        self.bn8 = nn.BatchNorm1d(512)\n",
    "        self.fc9 = QuantClass(nn.Linear(512, num_classes), optimizeQuant=optimizeQuant)\n",
    "        #self.output = QuantClass(nn.Identity(), optimizeQuant=optimizeQuant)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = 2.0 * x - torch.tensor([1.0], device=x.device)\n",
    "        #x = self.input(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class squared_hinge_loss(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, predictions, targets):\n",
    "        ctx.save_for_backward(predictions, targets)\n",
    "        output = 1. - predictions.mul(targets)\n",
    "        output[output.le(0.)] = 0.\n",
    "        loss = torch.mean(output.mul(output))\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        predictions, targets = ctx.saved_tensors\n",
    "        output = 1. - predictions.mul(targets)\n",
    "        output[output.le(0.)] = 0.\n",
    "        grad_output.resize_as_(predictions).copy_(targets).mul_(-2.).mul_(output)\n",
    "        grad_output.mul_(output.ne(0).float())\n",
    "        grad_output.div_(predictions.numel())\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "class SqrHingeLoss(nn.Module):\n",
    "    # Squared Hinge Loss\n",
    "    def __init__(self):\n",
    "        super(SqrHingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return squared_hinge_loss.apply(input, target)\n",
    "    \n",
    "def label_smoothing_loss(pred, target, smoothing=0.1):\n",
    "    confidence = 1.0 - smoothing\n",
    "    log_probs = F.log_softmax(pred, dim=-1)\n",
    "    nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "    nll_loss = nll_loss.squeeze(1)\n",
    "    smooth_loss = -log_probs.mean(dim=-1)\n",
    "    loss = confidence * nll_loss + smoothing * smooth_loss\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwidth_squared(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += bit ** 2\n",
    "                c += 1\n",
    "    if c == 0:\n",
    "        return 0\n",
    "    return s/c\n",
    "\n",
    "def bitwidth_sum(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += bit\n",
    "                c += 1\n",
    "    if c==0:\n",
    "        return 0\n",
    "    return s/c\n",
    "\n",
    "def bitwidth_round_sum(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += torch.round(bit)\n",
    "                c += 1\n",
    "    if c==0:\n",
    "        return 0\n",
    "    return s/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBitWidths(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            print(\"module: \", name)\n",
    "            module.printQuantParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, bit_width_criterion, scheduler=None, lambda_bw=0.2):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        if isinstance(criterion, SqrHingeLoss):\n",
    "            target = target.unsqueeze(1)\n",
    "            target_onehot = torch.Tensor(target.size(0), len(classes)).to(device, non_blocking=True)\n",
    "            target_onehot.fill_(-1)\n",
    "            target_onehot.scatter_(1, target, 1)\n",
    "            target = target.squeeze()\n",
    "            target = target_onehot\n",
    "                    \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_ce = criterion(output, target)\n",
    "        penalty_bw = bit_width_criterion(model) \n",
    "        loss = loss_ce + lambda_bw*penalty_bw\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #if batch_idx % 200 == 0:\n",
    "        #    print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "        #          f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Train set: Average loss: {train_loss:.4f}\")\n",
    "\n",
    "def test(model, device, test_loader, criterion, bit_width_criterion, lambda_bw=0.2):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    penalty_bw = bit_width_criterion(model) \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            if isinstance(criterion, SqrHingeLoss):\n",
    "                target = target.unsqueeze(1)\n",
    "                target_onehot = torch.Tensor(target.size(0), len(classes)).to(device, non_blocking=True)\n",
    "                target_onehot.fill_(-1)\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                target = target.squeeze()\n",
    "                target = target_onehot\n",
    "\n",
    "            loss_ce = criterion(output, target)\n",
    "            loss = loss_ce + lambda_bw*penalty_bw\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy} ({100.0*accuracy:.2f}%) bit penalty {penalty_bw}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using divice  cuda\n",
      "epoch:  1\n",
      "Train set: Average loss: 0.2553\n",
      "Test set: Average loss: 3.1734, Accuracy: 0.5906 (59.06%) bit penalty 4.561777591705322\n",
      "epoch:  2\n",
      "Train set: Average loss: 0.1631\n",
      "Test set: Average loss: 3.0480, Accuracy: 0.5993 (59.93%) bit penalty 4.609384059906006\n",
      "epoch:  3\n",
      "Train set: Average loss: 0.1980\n",
      "Test set: Average loss: 3.2592, Accuracy: 0.581 (58.10%) bit penalty 4.662621974945068\n",
      "epoch:  4\n",
      "Train set: Average loss: 0.1686\n",
      "Test set: Average loss: 3.1671, Accuracy: 0.5898 (58.98%) bit penalty 4.697837829589844\n",
      "epoch:  5\n",
      "Train set: Average loss: 0.1721\n",
      "Test set: Average loss: 3.1303, Accuracy: 0.583 (58.30%) bit penalty 4.718672275543213\n",
      "epoch:  6\n",
      "Train set: Average loss: 0.1663\n",
      "Test set: Average loss: 3.3717, Accuracy: 0.5867 (58.67%) bit penalty 4.716749668121338\n",
      "epoch:  7\n",
      "Train set: Average loss: 0.1656\n",
      "Test set: Average loss: 3.3048, Accuracy: 0.5828 (58.28%) bit penalty 4.765940189361572\n",
      "epoch:  8\n",
      "Train set: Average loss: 0.1637\n",
      "Test set: Average loss: 3.3159, Accuracy: 0.5773 (57.73%) bit penalty 4.764254093170166\n",
      "epoch:  9\n",
      "Train set: Average loss: 0.1550\n",
      "Test set: Average loss: 3.5459, Accuracy: 0.5683 (56.83%) bit penalty 4.750905513763428\n",
      "epoch:  10\n",
      "Train set: Average loss: 0.1519\n",
      "Test set: Average loss: 3.2472, Accuracy: 0.5876 (58.76%) bit penalty 4.806395530700684\n",
      "epoch:  11\n",
      "Train set: Average loss: 0.1467\n",
      "Test set: Average loss: 3.3566, Accuracy: 0.5887 (58.87%) bit penalty 4.832861423492432\n",
      "epoch:  12\n",
      "Train set: Average loss: 0.1445\n",
      "Test set: Average loss: 3.4108, Accuracy: 0.5932 (59.32%) bit penalty 4.814791679382324\n",
      "epoch:  13\n",
      "Train set: Average loss: 0.1395\n",
      "Test set: Average loss: 3.3129, Accuracy: 0.5801 (58.01%) bit penalty 4.81974458694458\n",
      "epoch:  14\n",
      "Train set: Average loss: 0.1348\n",
      "Test set: Average loss: 3.1014, Accuracy: 0.5863 (58.63%) bit penalty 4.827239990234375\n",
      "epoch:  15\n",
      "Train set: Average loss: 0.1344\n",
      "Test set: Average loss: 3.1353, Accuracy: 0.6053 (60.53%) bit penalty 4.830009460449219\n",
      "epoch:  16\n",
      "Train set: Average loss: 0.1372\n",
      "Test set: Average loss: 3.3192, Accuracy: 0.5964 (59.64%) bit penalty 4.809326171875\n",
      "epoch:  17\n",
      "Train set: Average loss: 0.1446\n",
      "Test set: Average loss: 3.0673, Accuracy: 0.604 (60.40%) bit penalty 4.862890720367432\n",
      "epoch:  18\n",
      "Train set: Average loss: 0.1385\n",
      "Test set: Average loss: 3.0391, Accuracy: 0.6052 (60.52%) bit penalty 4.917960166931152\n",
      "epoch:  19\n",
      "Train set: Average loss: 0.1276\n",
      "Test set: Average loss: 3.7105, Accuracy: 0.5798 (57.98%) bit penalty 4.907337188720703\n",
      "epoch:  20\n",
      "Train set: Average loss: 0.1309\n",
      "Test set: Average loss: 3.7417, Accuracy: 0.5893 (58.93%) bit penalty 4.932239532470703\n",
      "epoch:  21\n",
      "Train set: Average loss: 0.1206\n",
      "Test set: Average loss: 3.3622, Accuracy: 0.5997 (59.97%) bit penalty 4.9078545570373535\n",
      "epoch:  22\n",
      "Train set: Average loss: 0.1210\n",
      "Test set: Average loss: 3.1527, Accuracy: 0.5848 (58.48%) bit penalty 4.866701602935791\n",
      "epoch:  23\n",
      "Train set: Average loss: 0.1153\n",
      "Test set: Average loss: 3.2086, Accuracy: 0.6032 (60.32%) bit penalty 4.859124660491943\n",
      "epoch:  24\n",
      "Train set: Average loss: 0.1159\n",
      "Test set: Average loss: 3.6694, Accuracy: 0.5985 (59.85%) bit penalty 4.869166374206543\n",
      "epoch:  25\n",
      "Train set: Average loss: 0.1106\n",
      "Test set: Average loss: 3.7472, Accuracy: 0.5808 (58.08%) bit penalty 4.8387131690979\n",
      "epoch:  26\n",
      "Train set: Average loss: 0.1156\n",
      "Test set: Average loss: 3.4772, Accuracy: 0.593 (59.30%) bit penalty 4.8043437004089355\n",
      "epoch:  27\n",
      "Train set: Average loss: 0.1230\n",
      "Test set: Average loss: 3.2814, Accuracy: 0.6091 (60.91%) bit penalty 4.777658939361572\n",
      "epoch:  28\n",
      "Train set: Average loss: 0.1088\n",
      "Test set: Average loss: 3.2520, Accuracy: 0.6062 (60.62%) bit penalty 4.827898025512695\n",
      "epoch:  29\n",
      "Train set: Average loss: 0.1064\n",
      "Test set: Average loss: 3.2467, Accuracy: 0.6049 (60.49%) bit penalty 4.845725059509277\n",
      "epoch:  30\n",
      "Train set: Average loss: 0.0999\n",
      "Test set: Average loss: 3.6167, Accuracy: 0.6049 (60.49%) bit penalty 4.848657608032227\n",
      "epoch:  31\n",
      "Train set: Average loss: 0.1011\n",
      "Test set: Average loss: 3.9708, Accuracy: 0.5679 (56.79%) bit penalty 4.821107387542725\n",
      "epoch:  32\n",
      "Train set: Average loss: 0.0993\n",
      "Test set: Average loss: 3.7198, Accuracy: 0.5877 (58.77%) bit penalty 4.817197322845459\n",
      "epoch:  33\n",
      "Train set: Average loss: 0.0986\n",
      "Test set: Average loss: 3.4423, Accuracy: 0.5751 (57.51%) bit penalty 4.824127197265625\n",
      "epoch:  34\n",
      "Train set: Average loss: 0.0956\n",
      "Test set: Average loss: 3.4242, Accuracy: 0.6021 (60.21%) bit penalty 4.804813861846924\n",
      "epoch:  35\n",
      "Train set: Average loss: 0.0982\n",
      "Test set: Average loss: 3.4662, Accuracy: 0.5921 (59.21%) bit penalty 4.770597457885742\n",
      "epoch:  36\n",
      "Train set: Average loss: 0.0885\n",
      "Test set: Average loss: 3.7618, Accuracy: 0.6099 (60.99%) bit penalty 4.753927707672119\n",
      "epoch:  37\n",
      "Train set: Average loss: 0.0909\n",
      "Test set: Average loss: 3.9739, Accuracy: 0.5903 (59.03%) bit penalty 4.751987934112549\n",
      "epoch:  38\n",
      "Train set: Average loss: 0.0930\n",
      "Test set: Average loss: 3.7083, Accuracy: 0.5991 (59.91%) bit penalty 4.721060276031494\n",
      "epoch:  39\n",
      "Train set: Average loss: 0.0901\n",
      "Test set: Average loss: 3.9189, Accuracy: 0.6119 (61.19%) bit penalty 4.727004528045654\n",
      "epoch:  40\n",
      "Train set: Average loss: 0.0884\n",
      "Test set: Average loss: 3.7978, Accuracy: 0.6146 (61.46%) bit penalty 4.73638391494751\n",
      "epoch:  41\n",
      "Train set: Average loss: 0.0856\n",
      "Test set: Average loss: 3.5018, Accuracy: 0.5908 (59.08%) bit penalty 4.719124794006348\n",
      "epoch:  42\n",
      "Train set: Average loss: 0.0841\n",
      "Test set: Average loss: 3.1639, Accuracy: 0.6158 (61.58%) bit penalty 4.727908611297607\n",
      "epoch:  43\n",
      "Train set: Average loss: 0.0827\n",
      "Test set: Average loss: 4.1327, Accuracy: 0.592 (59.20%) bit penalty 4.726355075836182\n",
      "epoch:  44\n",
      "Train set: Average loss: 0.0787\n",
      "Test set: Average loss: 3.6758, Accuracy: 0.6153 (61.53%) bit penalty 4.702500343322754\n",
      "epoch:  45\n",
      "Train set: Average loss: 0.0760\n",
      "Test set: Average loss: 3.5832, Accuracy: 0.6048 (60.48%) bit penalty 4.672861576080322\n",
      "epoch:  46\n",
      "Train set: Average loss: 0.0796\n",
      "Test set: Average loss: 3.7929, Accuracy: 0.6154 (61.54%) bit penalty 4.66536283493042\n",
      "epoch:  47\n",
      "Train set: Average loss: 0.0754\n",
      "Test set: Average loss: 3.9174, Accuracy: 0.5927 (59.27%) bit penalty 4.65518045425415\n",
      "epoch:  48\n",
      "Train set: Average loss: 0.0745\n",
      "Test set: Average loss: 3.4869, Accuracy: 0.6211 (62.11%) bit penalty 4.6381659507751465\n",
      "epoch:  49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_width_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_bw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion, bit_width_criterion, scheduler, lambda_bw)\u001b[0m\n\u001b[1;32m     18\u001b[0m penalty_bw \u001b[38;5;241m=\u001b[39m bit_width_criterion(model) \n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_ce \u001b[38;5;241m+\u001b[39m lambda_bw\u001b[38;5;241m*\u001b[39mpenalty_bw\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"using divice \", device)\n",
    "\n",
    "QuantClass = QuantWrapperFloatingPoint\n",
    "#QuantClass = QuantWrapperFixedPoint\n",
    "\n",
    "base_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_base_model.pth\"\n",
    "best_accuracy_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_best_accuracy_model.pth\"\n",
    "less_bits_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_less_bits_model.pth\"\n",
    "\n",
    "load_model_path = None\n",
    "if(os.path.isfile(best_accuracy_model_path)):\n",
    "    load_model_path = best_accuracy_model_path\n",
    "elif(os.path.isfile(base_model_path)):\n",
    "    load_model_path = base_model_path   \n",
    "else:\n",
    "    best_accuracy_model_path = base_model_path\n",
    "    less_bits_model_path = base_model_path\n",
    "\n",
    "if(load_model_path):\n",
    "    # Create model\n",
    "    # model = SimpleQuantizedMLP(e_bits=4.0, m_bits=4.0, num_classes=len(classes)).to(device)\n",
    "    model = QuantSimpleCIFAR10Model(QuantClass, num_classes=len(classes), optimizeQuant=True).to(device)\n",
    "    #model = SimpleCIFAR10Model(num_classes=len(classes)).to(device)\n",
    "    model.load_state_dict(torch.load(load_model_path, weights_only=True))\n",
    "else:\n",
    "    model = QuantSimpleCIFAR10Model(QuantClass, num_classes=len(classes), optimizeQuant=False).to(device)\n",
    "\n",
    "#criterion = SqrHingeLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = label_smoothing_loss\n",
    "\n",
    "bit_width_criterion = bitwidth_sum\n",
    "#bit_width_criterion = bitwidth_squared\n",
    "\n",
    "lambda_bw = 0.01\n",
    "\n",
    "# Create optimizer (SGD or Adam)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)  # Adjusted Cosine Annealing with warm-up strategy\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_penalty_bw = 100000.0\n",
    "# Train for some epochs\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(\"epoch: \", epoch)\n",
    "    train(model, device, train_loader, optimizer, criterion, bit_width_criterion, epoch, lambda_bw)\n",
    "    if scheduler != None:\n",
    "        scheduler.step()\n",
    "    accuracy = test(model, device, test_loader, criterion, bit_width_criterion, lambda_bw)\n",
    "    penalty_bw = bitwidth_round_sum(model)\n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), best_accuracy_model_path)\n",
    "    if(penalty_bw < best_penalty_bw):\n",
    "        best_penalty_bw = penalty_bw\n",
    "        torch.save(model.state_dict(), less_bits_model_path)\n",
    "    #printBitWidths(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 5.9866, Accuracy: 0.639 (63.90%) bit penalty 16.0\n",
      "Test set: Average loss: 4.3682, Accuracy: 0.6211 (62.11%) bit penalty 4.6381659507751465\n",
      "Test set: Average loss: 4.1451, Accuracy: 0.581 (58.10%) bit penalty 4.662621974945068\n",
      "base model accuracy:  0.639  penalty bw  16.0\n",
      "best accuracy model accuracy:  0.6211  penalty bw  4.666666507720947\n",
      "less bits model accuracy:  0.581  penalty bw  4.5\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(base_model_path, weights_only=True))\n",
    "base_accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "base_penalty_bw = bitwidth_round_sum(model).detach().item()\n",
    "model.load_state_dict(torch.load(best_accuracy_model_path, weights_only=True))\n",
    "best_accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "best_accuracy_penalty_bw = bitwidth_round_sum(model).detach().item()\n",
    "model.load_state_dict(torch.load(less_bits_model_path, weights_only=True))\n",
    "lest_bits_accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "less_bits_penalty_bw = bitwidth_round_sum(model).detach().item()\n",
    "print(\"base model accuracy: \", base_accuracy, \" penalty bw \", base_penalty_bw)\n",
    "print(\"best accuracy model accuracy: \", best_accuracy, \" penalty bw \", best_accuracy_penalty_bw)\n",
    "print(\"less bits model accuracy: \", lest_bits_accuracy, \" penalty bw \", less_bits_penalty_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5321432948112488  m bits  4.590126037597656  scale  -1.2342294454574585\n",
      "weight quant params: \n",
      "e bits  0.1193634569644928  m bits  4.112362861633301  scale  -1.4552290439605713\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.572547197341919  m bits  2.7645862102508545  scale  0.5354794859886169\n",
      "weight quant params: \n",
      "e bits  0.5405311584472656  m bits  2.664247751235962  scale  -0.47992226481437683\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.6842734813690186  m bits  2.787001132965088  scale  0.4996720850467682\n",
      "weight quant params: \n",
      "e bits  0.3148345649242401  m bits  2.550835132598877  scale  -0.3662202060222626\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  0.799573540687561  m bits  2.5952608585357666  scale  0.5090738534927368\n",
      "weight quant params: \n",
      "e bits  0.5180565118789673  m bits  1.7071263790130615  scale  -0.6054772734642029\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  1.5738312005996704  m bits  2.541541814804077  scale  0.5461331009864807\n",
      "weight quant params: \n",
      "e bits  0.5062785744667053  m bits  3.2730250358581543  scale  -0.4908255934715271\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.502830445766449  m bits  2.5076513290405273  scale  0.6524743437767029\n",
      "weight quant params: \n",
      "e bits  0.6008571982383728  m bits  1.7708408832550049  scale  -1.443602442741394\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5028907656669617  m bits  2.5331192016601562  scale  0.5866749286651611\n",
      "weight quant params: \n",
      "e bits  0.2738700211048126  m bits  2.7258002758026123  scale  -0.24016587436199188\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.5036090612411499  m bits  3.4928696155548096  scale  1.0878899097442627\n",
      "weight quant params: \n",
      "e bits  1.5444587469100952  m bits  2.914997100830078  scale  -1.509565830230713\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.6143393516540527  m bits  2.8331215381622314  scale  0.49817222356796265\n",
      "weight quant params: \n",
      "e bits  1.5247776508331299  m bits  2.333613634109497  scale  -1.619659185409546\n"
     ]
    }
   ],
   "source": [
    "less_bits_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_best_accuracy_model.pth\"\n",
    "pickle_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}.pkl\"\n",
    "state_dict = torch.load(best_accuracy_model_path, weights_only=True)\n",
    "state_dict_numpy = {}\n",
    "\n",
    "for key in state_dict:\n",
    "    #print(f\"{key}: {type(state_dict[key])}\")\n",
    "    state_dict_numpy[key] = state_dict[key].cpu().detach().numpy().tolist()\n",
    "    #print(key)\n",
    "    #print(state_dict_numpy[key])\n",
    "\n",
    "with open(\"mnist_cnn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(state_dict_numpy, f)\n",
    "\n",
    "printBitWidths(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
