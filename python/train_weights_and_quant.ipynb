{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 2) CIFAR-10 dataset\\ntrain_transform = transforms.Compose([\\n    #transforms.RandomCrop(32, padding=4),\\n    transforms.RandomHorizontalFlip(),\\n    #transforms.RandomVerticalFlip(),\\n    #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\\n    #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465),\\n                         (0.2470, 0.2435, 0.2616))  # mean, std\\n])\\n\\n# Transformations for testing: just convert and normalize\\ntest_transform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465),\\n                         (0.2470, 0.2435, 0.2616))\\n])\\n\\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\\n\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\\n\\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\\n\\ninput_size = (3, 32, 32)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# 1) MNIST Dataset & Dataloaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "input_size = (1, 32, 32)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# 2) CIFAR-10 dataset\n",
    "train_transform = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "])\n",
    "\n",
    "# Transformations for testing: just convert and normalize\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "input_size = (3, 32, 32)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.42421296..2.8214867].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: a batch of images in shape (B, C, H, W) or a single image in (C, H, W).\n",
    "    \"\"\"\n",
    "    # If it's a batch of images (4D), make a grid first\n",
    "    if len(img_tensor.shape) == 4:\n",
    "        img_tensor = torchvision.utils.make_grid(img_tensor)\n",
    "    # Unnormalize\n",
    "    #img_tensor = unnormalize(img_tensor)\n",
    "    # Convert to numpy\n",
    "    npimg = img_tensor.numpy()\n",
    "    # Transpose from (C, H, W) to (H, W, C)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_bit(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(x)\n",
    "\n",
    "def bit_to_param(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(x)\n",
    "\n",
    "def fake_float_truncate(x: torch.Tensor, e_bits: int, m_bits: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate 'float' with e_bits exponent bits and m_bits mantissa bits.\n",
    "    Simplified approach: unbiased exponent in integer range + truncated mantissa.\n",
    "    \"\"\"\n",
    "    eps = 1e-45\n",
    "    abs_x = x.abs().clamp(min=eps)\n",
    "    sign = x.sign()\n",
    "    \n",
    "    # exponent\n",
    "    e = torch.floor(torch.log2(abs_x))\n",
    "    min_e = -(2**(e_bits)) + 1\n",
    "    max_e =  (2**(e_bits)) - 1\n",
    "    e_clamped = torch.clamp(e, min_e, max_e)\n",
    "    \n",
    "    # fraction in [1,2) if x >= eps\n",
    "    frac = abs_x / (2.0 ** e_clamped)\n",
    "    \n",
    "    # truncate mantissa\n",
    "    scale = 2.0 ** m_bits\n",
    "    frac_trunc = torch.floor(frac * scale) / scale\n",
    "    \n",
    "    return sign * (2.0 ** e_clamped) * frac_trunc\n",
    "\n",
    "class FakeFloatFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e_bits_param, m_bits_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, e_bits_param, m_bits_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).clamp(min=0.0).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).clamp(min=0.0).item())\n",
    "        \n",
    "        out = fake_float_truncate(x, e_bits_int, m_bits_int)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, e_bits_param, m_bits_param = ctx.saved_tensors\n",
    "        \n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).clamp(min=0.0).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).clamp(min=0.0).item())\n",
    "        \n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output.clone()\n",
    "        \n",
    "        # 2) Gradient wrt e_bits: approximate with central difference\n",
    "        grad_e_bits = None\n",
    "        if e_bits_param.requires_grad:\n",
    "            delta = 0.1\n",
    "            e_plus2_int  = int(torch.round(param_to_bit(e_bits_param + 2*delta)).clamp(min=0.0).item())\n",
    "            e_plus_int   = int(torch.round(param_to_bit(e_bits_param +   delta)).clamp(min=0.0).item())\n",
    "            e_minus_int  = int(torch.round(param_to_bit(e_bits_param -   delta)).clamp(min=0.0).item())\n",
    "            e_minus2_int = int(torch.round(param_to_bit(e_bits_param - 2*delta)).clamp(min=0.0).item())\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_plus2_int,  m_bits_int)\n",
    "            f_plus   = fake_float_truncate(x, e_plus_int,   m_bits_int)\n",
    "            f_minus  = fake_float_truncate(x, e_minus_int,  m_bits_int)\n",
    "            f_minus2 = fake_float_truncate(x, e_minus2_int, m_bits_int)\n",
    "            \n",
    "            #diff_e = (f_plus - f_minus) * grad_output\n",
    "            #grad_e_bits = diff_e.sum() / (2.0 * delta)\n",
    "            \n",
    "            diff_e = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) * grad_output\n",
    "            grad_e_bits = diff_e.sum() / (12.0 * delta)\n",
    "        \n",
    "        # 3) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_m_bits = None\n",
    "        if m_bits_param.requires_grad:\n",
    "            delta = 0.1\n",
    "            m_plus2_int  = int(torch.round(param_to_bit(m_bits_param + 2*delta)).clamp(min=0.0).item())\n",
    "            m_plus_int   = int(torch.round(param_to_bit(m_bits_param +   delta)).clamp(min=0.0).item())\n",
    "            m_minus_int  = int(torch.round(param_to_bit(m_bits_param -   delta)).clamp(min=0.0).item())\n",
    "            m_minus2_int = int(torch.round(param_to_bit(m_bits_param - 2*delta)).clamp(min=0.0).item())\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_bits_int, m_plus2_int)\n",
    "            f_plus   = fake_float_truncate(x, e_bits_int, m_plus_int)\n",
    "            f_minus  = fake_float_truncate(x, e_bits_int, m_minus_int)\n",
    "            f_minus2 = fake_float_truncate(x, e_bits_int, m_minus2_int)\n",
    "            \n",
    "            #diff_e = (f_plus - f_minus) * grad_output\n",
    "            #grad_e_bits = diff_e.sum() / (2.0 * delta)\n",
    "            \n",
    "            diff_e = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) * grad_output\n",
    "            grad_m_bits = diff_e.sum() / (12.0 * delta)\n",
    "        \n",
    "        return grad_x, grad_e_bits, grad_m_bits\n",
    "\n",
    "def differentiable_truncation(x: torch.Tensor, lower: float, upper: float, sharpness: float=10.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Differentiable truncation function for PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        lower (float): Lower truncation limit.\n",
    "        upper (float): Upper truncation limit.\n",
    "        sharpness (float): Controls the sharpness of the truncation. Higher values\n",
    "                           make the truncation more like a hard truncation.\n",
    "                           \n",
    "    Returns:\n",
    "        torch.Tensor: Truncated tensor with smooth gradients.\n",
    "    \"\"\"\n",
    "    smooth_lower = lower + F.softplus(-sharpness * (x - lower)) / sharpness\n",
    "    smooth_upper = upper - F.softplus(-sharpness * (upper - x)) / sharpness\n",
    "    return torch.min(torch.max(x, smooth_lower), smooth_upper)\n",
    "\n",
    "def fake_float_diff_truncate(x: torch.Tensor, e_bits_param: torch.Tensor, m_bits_param: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate 'float' with e_bits exponent bits and m_bits mantissa bits.\n",
    "    Simplified approach: unbiased exponent in integer range + truncated mantissa.\n",
    "    \"\"\"\n",
    "    \n",
    "    e_bits = param_to_bit(e_bits_param)\n",
    "    m_bits = param_to_bit(m_bits_param)\n",
    "    \n",
    "    eps = 1e-45\n",
    "    abs_x = x.abs().clamp(min=eps)\n",
    "    sign = x.sign()\n",
    "    \n",
    "    # exponent\n",
    "    e = torch.log2(abs_x)\n",
    "\n",
    "    #min_e = -(2**(e_bits)) + 1\n",
    "    #max_e =  (2**(e_bits)) - 1 \n",
    "    #e_clamped = torch.clamp(e, min_e, max_e)\n",
    "\n",
    "    e_bits_minus = 2 ** (e_bits - 1)\n",
    "    e_bits_plus = 2 ** (e_bits + 1)  \n",
    "    e_clamped = differentiable_truncation(e, e_bits_minus, e_bits_plus)\n",
    "\n",
    "    # fraction in [1,2) if x >= eps\n",
    "    frac = abs_x / (2.0 ** e_clamped)\n",
    "    \n",
    "    # truncate mantissa    \n",
    "    #scale = 2.0 ** m_bits\n",
    "    #frac_trunc = torch.floor(frac * scale) / scale\n",
    "\n",
    "    m_bits_minus = 2 ** (m_bits - 1)\n",
    "    m_bits_plus = 2 ** (m_bits + 1)  \n",
    "    frac_trunc = differentiable_truncation(frac, m_bits_minus, m_bits_plus)\n",
    "    \n",
    "    return sign * (2.0 ** e_clamped) * frac_trunc\n",
    "\n",
    "class FakeFloatDiffFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e_bits_param, m_bits_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, e_bits_param, m_bits_param)\n",
    "        \n",
    "        out = fake_float_truncate(x, e_bits_param, m_bits_param)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * input_size[1]//8 * input_size[2]//8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleCIFAR10Model2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * input_size[1]//4 * input_size[2]//4, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)Tensor\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SimpleQuantizedMLP(nn.Module):\n",
    "    def __init__(self, e_bits=4.0, m_bits=4.0, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Now we make them trainable:\n",
    "        self.e_bits = nn.Parameter(torch.tensor(e_bits))\n",
    "        self.m_bits = nn.Parameter(torch.tensor(m_bits))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        w1 = FakeFloatFunction.apply(self.fc1.weight, self.e_bits, self.m_bits)\n",
    "        b1 = FakeFloatFunction.apply(self.fc1.bias,   self.e_bits, self.m_bits)\n",
    "        x  = F.relu(F.linear(x, w1, b1))\n",
    "\n",
    "        w2 = FakeFloatFunction.apply(self.fc2.weight, self.e_bits, self.m_bits)\n",
    "        b2 = FakeFloatFunction.apply(self.fc2.bias,   self.e_bits, self.m_bits)\n",
    "        x  = F.linear(x, w2, b2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SimpleQuantizedCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_size[0],  out_channels=32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,  out_channels=64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)        \n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64,  out_channels=128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(128)        \n",
    "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * input_size[1]//8 * input_size[2]//8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "        learnInput = True\n",
    "        learnInter = False\n",
    "        inputsE = 8.0\n",
    "        inputsM = 23.0\n",
    "        learnConvBias = False\n",
    "        learnConvWeights = True\n",
    "        convE = 8.0\n",
    "        convM = 23.0\n",
    "        learnLineBias = True\n",
    "        learnLineWeights = True\n",
    "        lineE = 8.0\n",
    "        lineM = 23.0\n",
    "        \n",
    "        self.i_e_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInput),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),\n",
    "        ])\n",
    "        \n",
    "        self.i_m_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInput),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),\n",
    "        ])\n",
    "        \n",
    "        self.w_e_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvWeights),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineWeights),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineWeights),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineWeights),\n",
    "        ])\n",
    "        \n",
    "        self.w_m_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvWeights),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineWeights),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineWeights),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineWeights),\n",
    "        ])\n",
    "          \n",
    "        self.b_e_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvBias),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineBias),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineBias),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineBias),\n",
    "        ])\n",
    "        \n",
    "        self.b_m_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvBias),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvBias),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineBias),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineBias),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineBias),\n",
    "        ])\n",
    "              \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = FakeFloatFunction.apply(x, self.i_e_bits_param[0], self.i_m_bits_param[0])\n",
    "        w1 = FakeFloatFunction.apply(self.conv1.weight, self.w_e_bits_param[0], self.w_m_bits_param[0])\n",
    "        b1 = FakeFloatFunction.apply(self.conv1.bias,   self.b_e_bits_param[0], self.b_m_bits_param[0]) if self.conv1.bias is not None else None\n",
    "        #x = fake_float_diff_truncate(x, self.i_e_bits_param[0], self.i_m_bits_param[0])\n",
    "        #w1 = fake_float_diff_truncate(self.conv1.weight, self.w_e_bits_param[0], self.w_m_bits_param[0])\n",
    "        #b1 = None #fake_float_diff_truncate(self.conv1.bias,   self.b_e_bits_param[0], self.b_m_bits_param[0]) if self.conv1.bias is not None else None\n",
    "        x = F.conv2d(x, w1, b1, stride=1, padding=1)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2)  # 32x32 -> 16x16\n",
    "        \n",
    "        #x = FakeFloatFunction.apply(x, self.i_e_bits_param[1], self.i_m_bits_param[1])\n",
    "        w2 = FakeFloatFunction.apply(self.conv2.weight, self.w_e_bits_param[1], self.w_m_bits_param[1])\n",
    "        b2 = FakeFloatFunction.apply(self.conv2.bias,   self.b_e_bits_param[1], self.b_m_bits_param[1]) if self.conv2.bias is not None else None\n",
    "        #w2 = fake_float_diff_truncate(self.conv2.weight, self.w_e_bits_param[1], self.w_m_bits_param[1])\n",
    "        #b2 = None #fake_float_diff_truncate(self.conv2.bias,   self.b_e_bits_param[1], self.b_m_bits_param[1]) if self.conv2.bias is not None else None\n",
    "        x = F.conv2d(x, w2, b2, stride=1, padding=1)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)  # 16x16 -> 8x8\n",
    "        \n",
    "        w3 = FakeFloatFunction.apply(self.conv3.weight, self.w_e_bits_param[2], self.w_m_bits_param[2])\n",
    "        b3 = FakeFloatFunction.apply(self.conv3.bias,   self.b_e_bits_param[2], self.b_m_bits_param[2]) if self.conv3.bias is not None else None\n",
    "        #x = fake_float_diff_truncate(x, self.i_e_bits_param[0], self.i_m_bits_param[0])\n",
    "        #w1 = fake_float_diff_truncate(self.conv1.weight, self.w_e_bits_param[0], self.w_m_bits_param[0])\n",
    "        #b1 = None #fake_float_diff_truncate(self.conv1.bias,   self.b_e_bits_param[0], self.b_m_bits_param[0]) if self.conv1.bias is not None else None\n",
    "        x = F.conv2d(x, w3, b3, stride=1, padding=1)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2)  # 32x32 -> 16x16\n",
    "        \n",
    "        #x = FakeFloatFunction.apply(x, self.i_e_bits_param[1], self.i_m_bits_param[1])\n",
    "        w4 = FakeFloatFunction.apply(self.conv4.weight, self.w_e_bits_param[3], self.w_m_bits_param[3])\n",
    "        b4 = FakeFloatFunction.apply(self.conv4.bias,   self.b_e_bits_param[3], self.b_m_bits_param[3]) if self.conv4.bias is not None else None\n",
    "        #w2 = fake_float_diff_truncate(self.conv2.weight, self.w_e_bits_param[1], self.w_m_bits_param[1])\n",
    "        #b2 = None #fake_float_diff_truncate(self.conv2.bias,   self.b_e_bits_param[1], self.b_m_bits_param[1]) if self.conv2.bias is not None else None\n",
    "        x = F.conv2d(x, w4, b4, stride=1, padding=1)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)  # 16x16 -> 8x8\n",
    "        \n",
    "        w5 = FakeFloatFunction.apply(self.conv5.weight, self.w_e_bits_param[4], self.w_m_bits_param[4])\n",
    "        b5 = FakeFloatFunction.apply(self.conv5.bias,   self.b_e_bits_param[4], self.b_m_bits_param[4]) if self.conv5.bias is not None else None\n",
    "        #x = fake_float_diff_truncate(x, self.i_e_bits_param[0], self.i_m_bits_param[0])\n",
    "        #w1 = fake_float_diff_truncate(self.conv1.weight, self.w_e_bits_param[0], self.w_m_bits_param[0])\n",
    "        #b1 = None #fake_float_diff_truncate(self.conv1.bias,   self.b_e_bits_param[0], self.b_m_bits_param[0]) if self.conv1.bias is not None else None\n",
    "        x = F.conv2d(x, w5, b5, stride=1, padding=1)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2)  # 32x32 -> 16x16\n",
    "        \n",
    "        #x = FakeFloatFunction.apply(x, self.i_e_bits_param[1], self.i_m_bits_param[1])\n",
    "        w6 = FakeFloatFunction.apply(self.conv6.weight, self.w_e_bits_param[5], self.w_m_bits_param[5])\n",
    "        b6 = FakeFloatFunction.apply(self.conv6.bias,   self.b_e_bits_param[5], self.b_m_bits_param[5]) if self.conv6.bias is not None else None\n",
    "        #w2 = fake_float_diff_truncate(self.conv2.weight, self.w_e_bits_param[1], self.w_m_bits_param[1])\n",
    "        #b2 = None #fake_float_diff_truncate(self.conv2.bias,   self.b_e_bits_param[1], self.b_m_bits_param[1]) if self.conv2.bias is not None else None\n",
    "        x = F.conv2d(x, w6, b6, stride=1, padding=1)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)  # 16x16 -> 8x8\n",
    "        \n",
    "        x  = x.view(x.size(0), -1)\n",
    "        \n",
    "        #x = FakeFloatFunction.apply(x, self.i_e_bits_param[2], self.i_m_bits_param[2])\n",
    "        w_fc1 = FakeFloatFunction.apply(self.fc1.weight, self.w_e_bits_param[6], self.w_m_bits_param[6])\n",
    "        b_fc1 = FakeFloatFunction.apply(self.fc1.bias,   self.b_e_bits_param[6], self.b_m_bits_param[6])\n",
    "        #w_fc1 = fake_float_diff_truncate(self.fc1.weight, self.w_e_bits_param[2], self.w_m_bits_param[2])\n",
    "        #b_fc1 = fake_float_diff_truncate(self.fc1.bias,   self.b_e_bits_param[2], self.b_m_bits_param[2])\n",
    "        x = F.linear(x, w_fc1, b_fc1)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "\n",
    "        #x = FakeFloatFunction.apply(x, self.i_e_bits_param[3], self.i_m_bits_param[3])\n",
    "        w_fc2 = FakeFloatFunction.apply(self.fc2.weight, self.w_e_bits_param[7], self.w_m_bits_param[7])\n",
    "        b_fc2 = FakeFloatFunction.apply(self.fc2.bias,   self.b_e_bits_param[7], self.b_m_bits_param[7])\n",
    "        #w_fc2 = fake_float_diff_truncate(self.fc2.weight, self.w_e_bits_param[3], self.w_m_bits_param[3])\n",
    "        #b_fc2 = fake_float_diff_truncate(self.fc2.bias,   self.b_e_bits_param[3], self.b_m_bits_param[3])\n",
    "        x  = F.linear(x, w_fc2, b_fc2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #x = FakeFloatFunction.apply(x, self.i_e_bits_param[3], self.i_m_bits_param[3])\n",
    "        w_fc3 = FakeFloatFunction.apply(self.fc3.weight, self.w_e_bits_param[8], self.w_m_bits_param[8])\n",
    "        b_fc3 = FakeFloatFunction.apply(self.fc3.bias,   self.b_e_bits_param[8], self.b_m_bits_param[8])\n",
    "        #w_fc2 = fake_float_diff_truncate(self.fc2.weight, self.w_e_bits_param[3], self.w_m_bits_param[3])\n",
    "        #b_fc2 = fake_float_diff_truncate(self.fc2.bias,   self.b_e_bits_param[3], self.b_m_bits_param[3])\n",
    "        x  = F.linear(x, w_fc3, b_fc3)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def printBitWidths(self):\n",
    "        for i, (eb, mb) in enumerate(zip(self.i_e_bits_param, self.i_m_bits_param)):\n",
    "            print(f\"Layer {i} input e_bits (float) = {param_to_bit(eb).item()},  m_bits (float) = {param_to_bit(mb).item()}\")\n",
    "        for i, (eb, mb) in enumerate(zip(self.w_e_bits_param, self.w_m_bits_param)):\n",
    "            print(f\"Layer {i} weight e_bits (float) = {param_to_bit(eb).item()},  m_bits (float) = {param_to_bit(mb).item()}\")\n",
    "        for i, (eb, mb) in enumerate(zip(self.b_e_bits_param, self.b_m_bits_param)):\n",
    "            print(f\"Layer {i} bias e_bits (float) = {param_to_bit(eb).item()},  m_bits (float) = {param_to_bit(mb).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwidth_sum(model):\n",
    "    \"\"\"\n",
    "    Computes a penalty term for the bitwidth parameters in 'model'.\n",
    "    'lambda_bw' is the weight/scale for this regularization.\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "    \n",
    "    # If the model has multiple layers with e_bits and m_bits in a ParameterList:\n",
    "    for eb, mb in zip(model.w_e_bits_param, model.w_m_bits_param):\n",
    "        # Option A: Penalize the raw float value (the \"continuous\" version)\n",
    "        penalty += eb + mb\n",
    "        \n",
    "        # Option B (alternative): Penalize the rounded integer version\n",
    "        # penalty += torch.round(eb) + torch.round(mb)\n",
    "    \n",
    "    for eb, mb in zip(model.b_e_bits_param, model.b_m_bits_param):\n",
    "        penalty += eb + mb\n",
    "\n",
    "    for eb, mb in zip(model.i_e_bits_param, model.i_m_bits_param):\n",
    "        penalty += eb + mb\n",
    "                    \n",
    "    return penalty\n",
    "\n",
    "\n",
    "def bitwidth_squared(model):\n",
    "    \"\"\"\n",
    "    Computes a penalty term for the bitwidth parameters in 'model'.\n",
    "    'lambda_bw' is the weight/scale for this regularization.\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "    \n",
    "    # If the model has multiple layers with e_bits and m_bits in a ParameterList:\n",
    "    for eb, mb in zip(model.w_e_bits_param, model.w_m_bits_param):\n",
    "        # Option A: Penalize the raw float value (the \"continuous\" version)\n",
    "        eb_ = eb - bit_to_param(torch.tensor(0.25))\n",
    "        mb_ = mb - bit_to_param(torch.tensor(0.25))\n",
    "        penalty += eb_*eb_ + mb_*mb_\n",
    "        \n",
    "        # Option B (alternative): Penalize the rounded integer version\n",
    "        # penalty += torch.round(eb) + torch.round(mb)\n",
    "    \n",
    "    for eb, mb in zip(model.b_e_bits_param, model.b_m_bits_param):\n",
    "        eb_ = eb - bit_to_param(torch.tensor(0.25))\n",
    "        mb_ = mb - bit_to_param(torch.tensor(0.25))\n",
    "        penalty += eb_*eb_ + mb_*mb_\n",
    "\n",
    "    for eb, mb in zip(model.i_e_bits_param, model.i_m_bits_param):\n",
    "        eb_ = eb - bit_to_param(torch.tensor(0.25))\n",
    "        mb_ = mb - bit_to_param(torch.tensor(0.25))\n",
    "        penalty += eb_*eb_ + mb_*mb_\n",
    "                    \n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, lambda_bw=1.0e-3):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_ce = F.cross_entropy(output, target)\n",
    "        penalty_bw = bitwidth_squared(model) \n",
    "        #penalty_bw = bitwidth_sum(model) \n",
    "        loss = loss_ce + lambda_bw*penalty_bw\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #if batch_idx % 200 == 0:\n",
    "        #    print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "        #          f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Train set: Average loss: {train_loss:.4f}\")\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy} ({100.0*accuracy:.2f}%)\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using divice  cpu\n",
      "Train set: Average loss: 0.8216\n",
      "Test set: Average loss: 0.0367, Accuracy: 0.9891 (98.91%)\n",
      "Layer 0 input e_bits (float) = 3.318047046661377,  m_bits (float) = 9.403782844543457\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 3.3180413246154785,  m_bits (float) = 9.40336799621582\n",
      "Layer 1 weight e_bits (float) = 3.318028450012207,  m_bits (float) = 9.405367851257324\n",
      "Layer 2 weight e_bits (float) = 3.318044662475586,  m_bits (float) = 9.40533447265625\n",
      "Layer 3 weight e_bits (float) = 3.31803822517395,  m_bits (float) = 9.405374526977539\n",
      "Layer 4 weight e_bits (float) = 3.3180441856384277,  m_bits (float) = 9.405468940734863\n",
      "Layer 5 weight e_bits (float) = 3.3180460929870605,  m_bits (float) = 9.404964447021484\n",
      "Layer 6 weight e_bits (float) = 3.318058729171753,  m_bits (float) = 9.402820587158203\n",
      "Layer 7 weight e_bits (float) = 3.318054437637329,  m_bits (float) = 9.4017915725708\n",
      "Layer 8 weight e_bits (float) = 3.3180670738220215,  m_bits (float) = 9.403112411499023\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 3.318047285079956,  m_bits (float) = 9.405341148376465\n",
      "Layer 7 bias e_bits (float) = 3.318047046661377,  m_bits (float) = 9.405341148376465\n",
      "Layer 8 bias e_bits (float) = 3.318047046661377,  m_bits (float) = 9.4053316116333\n",
      "Train set: Average loss: 0.6752\n",
      "Test set: Average loss: 0.0305, Accuracy: 0.9905 (99.05%)\n",
      "Layer 0 input e_bits (float) = 1.5523943901062012,  m_bits (float) = 4.167752265930176\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 1.6149996519088745,  m_bits (float) = 4.234645843505859\n",
      "Layer 1 weight e_bits (float) = 1.5651671886444092,  m_bits (float) = 4.1947760581970215\n",
      "Layer 2 weight e_bits (float) = 1.6031705141067505,  m_bits (float) = 4.198105812072754\n",
      "Layer 3 weight e_bits (float) = 1.5263326168060303,  m_bits (float) = 4.184349060058594\n",
      "Layer 4 weight e_bits (float) = 1.5387725830078125,  m_bits (float) = 4.1721320152282715\n",
      "Layer 5 weight e_bits (float) = 1.5311284065246582,  m_bits (float) = 4.160853862762451\n",
      "Layer 6 weight e_bits (float) = 1.5182499885559082,  m_bits (float) = 4.131186008453369\n",
      "Layer 7 weight e_bits (float) = 1.5396873950958252,  m_bits (float) = 4.167468070983887\n",
      "Layer 8 weight e_bits (float) = 1.545058250427246,  m_bits (float) = 4.28253698348999\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 1.539197564125061,  m_bits (float) = 4.173730373382568\n",
      "Layer 7 bias e_bits (float) = 1.5405023097991943,  m_bits (float) = 4.173915863037109\n",
      "Layer 8 bias e_bits (float) = 1.5402714014053345,  m_bits (float) = 4.176583290100098\n",
      "Train set: Average loss: 0.6028\n",
      "Test set: Average loss: 0.0311, Accuracy: 0.9928 (99.28%)\n",
      "Layer 0 input e_bits (float) = 0.812356173992157,  m_bits (float) = 2.4748125076293945\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 1.3801988363265991,  m_bits (float) = 2.8772146701812744\n",
      "Layer 1 weight e_bits (float) = 0.9614126682281494,  m_bits (float) = 3.2676165103912354\n",
      "Layer 2 weight e_bits (float) = 1.0596824884414673,  m_bits (float) = 3.3673830032348633\n",
      "Layer 3 weight e_bits (float) = 1.501563549041748,  m_bits (float) = 3.2585697174072266\n",
      "Layer 4 weight e_bits (float) = 1.5016276836395264,  m_bits (float) = 2.977130174636841\n",
      "Layer 5 weight e_bits (float) = 1.5010076761245728,  m_bits (float) = 3.1641852855682373\n",
      "Layer 6 weight e_bits (float) = 1.4639050960540771,  m_bits (float) = 3.4881105422973633\n",
      "Layer 7 weight e_bits (float) = 1.5084668397903442,  m_bits (float) = 3.5100347995758057\n",
      "Layer 8 weight e_bits (float) = 1.5102611780166626,  m_bits (float) = 3.5057907104492188\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.7994014620780945,  m_bits (float) = 1.9966741800308228\n",
      "Layer 7 bias e_bits (float) = 0.7996119856834412,  m_bits (float) = 1.999632716178894\n",
      "Layer 8 bias e_bits (float) = 0.8045944571495056,  m_bits (float) = 2.007079839706421\n",
      "Train set: Average loss: 0.5701\n",
      "Test set: Average loss: 0.0222, Accuracy: 0.9933 (99.33%)\n",
      "Layer 0 input e_bits (float) = 0.504649817943573,  m_bits (float) = 1.9537538290023804\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 1.1703091859817505,  m_bits (float) = 2.4386539459228516\n",
      "Layer 1 weight e_bits (float) = 0.5913171172142029,  m_bits (float) = 2.5107932090759277\n",
      "Layer 2 weight e_bits (float) = 0.7189664244651794,  m_bits (float) = 2.663633346557617\n",
      "Layer 3 weight e_bits (float) = 1.5093181133270264,  m_bits (float) = 2.4999351501464844\n",
      "Layer 4 weight e_bits (float) = 1.5027438402175903,  m_bits (float) = 2.41164493560791\n",
      "Layer 5 weight e_bits (float) = 1.515847086906433,  m_bits (float) = 2.3712003231048584\n",
      "Layer 6 weight e_bits (float) = 1.4251397848129272,  m_bits (float) = 3.286836862564087\n",
      "Layer 7 weight e_bits (float) = 1.498960256576538,  m_bits (float) = 3.4856693744659424\n",
      "Layer 8 weight e_bits (float) = 1.499987244606018,  m_bits (float) = 3.375990390777588\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.471493661403656,  m_bits (float) = 1.0351954698562622\n",
      "Layer 7 bias e_bits (float) = 0.4715631306171417,  m_bits (float) = 1.0390149354934692\n",
      "Layer 8 bias e_bits (float) = 0.4746427834033966,  m_bits (float) = 1.0478142499923706\n",
      "Train set: Average loss: 0.5540\n",
      "Test set: Average loss: 0.0531, Accuracy: 0.9859 (98.59%)\n",
      "Layer 0 input e_bits (float) = 0.4966258406639099,  m_bits (float) = 1.563278317451477\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.949927568435669,  m_bits (float) = 2.4935736656188965\n",
      "Layer 1 weight e_bits (float) = 0.5051606893539429,  m_bits (float) = 2.5788135528564453\n",
      "Layer 2 weight e_bits (float) = 0.5500198006629944,  m_bits (float) = 2.504324197769165\n",
      "Layer 3 weight e_bits (float) = 1.5161120891571045,  m_bits (float) = 2.360344409942627\n",
      "Layer 4 weight e_bits (float) = 1.5248372554779053,  m_bits (float) = 2.316185474395752\n",
      "Layer 5 weight e_bits (float) = 1.5011422634124756,  m_bits (float) = 2.045264959335327\n",
      "Layer 6 weight e_bits (float) = 1.353713035583496,  m_bits (float) = 3.0692079067230225\n",
      "Layer 7 weight e_bits (float) = 1.4786845445632935,  m_bits (float) = 3.326230049133301\n",
      "Layer 8 weight e_bits (float) = 1.4983327388763428,  m_bits (float) = 3.335024118423462\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.3252888023853302,  m_bits (float) = 0.5903645157814026\n",
      "Layer 7 bias e_bits (float) = 0.3253127634525299,  m_bits (float) = 0.592873752117157\n",
      "Layer 8 bias e_bits (float) = 0.32684242725372314,  m_bits (float) = 0.5981950759887695\n",
      "Train set: Average loss: 0.5444\n",
      "Test set: Average loss: 0.0293, Accuracy: 0.9922 (99.22%)\n",
      "Layer 0 input e_bits (float) = 0.4924071133136749,  m_bits (float) = 1.5084489583969116\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.7250522375106812,  m_bits (float) = 2.4840445518493652\n",
      "Layer 1 weight e_bits (float) = 0.5045026540756226,  m_bits (float) = 2.567713737487793\n",
      "Layer 2 weight e_bits (float) = 0.5127000212669373,  m_bits (float) = 2.50544810295105\n",
      "Layer 3 weight e_bits (float) = 1.5108460187911987,  m_bits (float) = 2.0664658546447754\n",
      "Layer 4 weight e_bits (float) = 1.5112698078155518,  m_bits (float) = 1.9929871559143066\n",
      "Layer 5 weight e_bits (float) = 1.5041197538375854,  m_bits (float) = 1.5961371660232544\n",
      "Layer 6 weight e_bits (float) = 1.3081363439559937,  m_bits (float) = 2.8481881618499756\n",
      "Layer 7 weight e_bits (float) = 1.408983588218689,  m_bits (float) = 2.9747917652130127\n",
      "Layer 8 weight e_bits (float) = 1.4986164569854736,  m_bits (float) = 3.393781900405884\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.26748111844062805,  m_bits (float) = 0.38171520829200745\n",
      "Layer 7 bias e_bits (float) = 0.26748812198638916,  m_bits (float) = 0.3830582797527313\n",
      "Layer 8 bias e_bits (float) = 0.2680428922176361,  m_bits (float) = 0.3858286738395691\n",
      "Train set: Average loss: 0.5378\n",
      "Test set: Average loss: 0.0421, Accuracy: 0.9911 (99.11%)\n",
      "Layer 0 input e_bits (float) = 0.4982635974884033,  m_bits (float) = 1.5076079368591309\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5519623160362244,  m_bits (float) = 2.4972033500671387\n",
      "Layer 1 weight e_bits (float) = 0.5040068030357361,  m_bits (float) = 2.5528857707977295\n",
      "Layer 2 weight e_bits (float) = 0.5140997171401978,  m_bits (float) = 2.5528366565704346\n",
      "Layer 3 weight e_bits (float) = 1.517993688583374,  m_bits (float) = 1.65961492061615\n",
      "Layer 4 weight e_bits (float) = 1.4931340217590332,  m_bits (float) = 1.6084049940109253\n",
      "Layer 5 weight e_bits (float) = 1.5428955554962158,  m_bits (float) = 1.5244309902191162\n",
      "Layer 6 weight e_bits (float) = 1.2197452783584595,  m_bits (float) = 2.4985601902008057\n",
      "Layer 7 weight e_bits (float) = 1.3684546947479248,  m_bits (float) = 2.6566877365112305\n",
      "Layer 8 weight e_bits (float) = 1.4991847276687622,  m_bits (float) = 2.9255688190460205\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.2519223093986511,  m_bits (float) = 0.28932809829711914\n",
      "Layer 7 bias e_bits (float) = 0.25192326307296753,  m_bits (float) = 0.28991463780403137\n",
      "Layer 8 bias e_bits (float) = 0.25202450156211853,  m_bits (float) = 0.2911120653152466\n",
      "Train set: Average loss: 0.5351\n",
      "Test set: Average loss: 0.0292, Accuracy: 0.9948 (99.48%)\n",
      "Layer 0 input e_bits (float) = 0.5080716013908386,  m_bits (float) = 1.5321049690246582\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5577844977378845,  m_bits (float) = 2.39638090133667\n",
      "Layer 1 weight e_bits (float) = 0.5025875568389893,  m_bits (float) = 2.5262813568115234\n",
      "Layer 2 weight e_bits (float) = 0.5101233124732971,  m_bits (float) = 2.501305103302002\n",
      "Layer 3 weight e_bits (float) = 1.5476871728897095,  m_bits (float) = 1.543887972831726\n",
      "Layer 4 weight e_bits (float) = 1.5366709232330322,  m_bits (float) = 1.531909465789795\n",
      "Layer 5 weight e_bits (float) = 1.5319095849990845,  m_bits (float) = 1.5065369606018066\n",
      "Layer 6 weight e_bits (float) = 1.1295548677444458,  m_bits (float) = 2.5071780681610107\n",
      "Layer 7 weight e_bits (float) = 1.2463099956512451,  m_bits (float) = 2.5038321018218994\n",
      "Layer 8 weight e_bits (float) = 1.4799232482910156,  m_bits (float) = 2.6327810287475586\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25005605816841125,  m_bits (float) = 0.2569202780723572\n",
      "Layer 7 bias e_bits (float) = 0.25005611777305603,  m_bits (float) = 0.2570834755897522\n",
      "Layer 8 bias e_bits (float) = 0.2500610947608948,  m_bits (float) = 0.25741833448410034\n",
      "Train set: Average loss: 0.5315\n",
      "Test set: Average loss: 0.0232, Accuracy: 0.9952 (99.52%)\n",
      "Layer 0 input e_bits (float) = 0.49309372901916504,  m_bits (float) = 1.5033756494522095\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5514496564865112,  m_bits (float) = 2.352271556854248\n",
      "Layer 1 weight e_bits (float) = 0.5020666122436523,  m_bits (float) = 2.498169183731079\n",
      "Layer 2 weight e_bits (float) = 0.5030384063720703,  m_bits (float) = 2.500196695327759\n",
      "Layer 3 weight e_bits (float) = 1.546187162399292,  m_bits (float) = 1.541388750076294\n",
      "Layer 4 weight e_bits (float) = 1.5185307264328003,  m_bits (float) = 1.4973067045211792\n",
      "Layer 5 weight e_bits (float) = 1.5168462991714478,  m_bits (float) = 1.4863581657409668\n",
      "Layer 6 weight e_bits (float) = 1.0072176456451416,  m_bits (float) = 2.5106420516967773\n",
      "Layer 7 weight e_bits (float) = 1.1463943719863892,  m_bits (float) = 2.4617421627044678\n",
      "Layer 8 weight e_bits (float) = 1.466465711593628,  m_bits (float) = 2.51704740524292\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25000202655792236,  m_bits (float) = 0.2504573464393616\n",
      "Layer 7 bias e_bits (float) = 0.25000202655792236,  m_bits (float) = 0.2504752576351166\n",
      "Layer 8 bias e_bits (float) = 0.25000205636024475,  m_bits (float) = 0.2505125403404236\n",
      "Train set: Average loss: 0.5274\n",
      "Test set: Average loss: 0.0215, Accuracy: 0.9939 (99.39%)\n",
      "Layer 0 input e_bits (float) = 0.4932934045791626,  m_bits (float) = 1.5007838010787964\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5372951626777649,  m_bits (float) = 2.14871883392334\n",
      "Layer 1 weight e_bits (float) = 0.5039894580841064,  m_bits (float) = 2.4920570850372314\n",
      "Layer 2 weight e_bits (float) = 0.5005956888198853,  m_bits (float) = 2.4939870834350586\n",
      "Layer 3 weight e_bits (float) = 1.5434470176696777,  m_bits (float) = 1.537070870399475\n",
      "Layer 4 weight e_bits (float) = 1.5236058235168457,  m_bits (float) = 1.4963511228561401\n",
      "Layer 5 weight e_bits (float) = 1.5021283626556396,  m_bits (float) = 1.4683412313461304\n",
      "Layer 6 weight e_bits (float) = 0.8549691438674927,  m_bits (float) = 2.481257438659668\n",
      "Layer 7 weight e_bits (float) = 1.0210561752319336,  m_bits (float) = 2.381215810775757\n",
      "Layer 8 weight e_bits (float) = 1.4956371784210205,  m_bits (float) = 2.5149331092834473\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25000128149986267,  m_bits (float) = 0.25000590085983276\n",
      "Layer 7 bias e_bits (float) = 0.25000128149986267,  m_bits (float) = 0.25000622868537903\n",
      "Layer 8 bias e_bits (float) = 0.25000128149986267,  m_bits (float) = 0.25000685453414917\n",
      "Train set: Average loss: 0.5238\n",
      "Test set: Average loss: 0.0306, Accuracy: 0.9919 (99.19%)\n",
      "Layer 0 input e_bits (float) = 0.497343510389328,  m_bits (float) = 1.4911152124404907\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5361214876174927,  m_bits (float) = 1.8500690460205078\n",
      "Layer 1 weight e_bits (float) = 0.5032181739807129,  m_bits (float) = 2.479609251022339\n",
      "Layer 2 weight e_bits (float) = 0.5043107867240906,  m_bits (float) = 2.5081844329833984\n",
      "Layer 3 weight e_bits (float) = 1.539760947227478,  m_bits (float) = 1.5308537483215332\n",
      "Layer 4 weight e_bits (float) = 1.539678692817688,  m_bits (float) = 1.5011420249938965\n",
      "Layer 5 weight e_bits (float) = 1.5065975189208984,  m_bits (float) = 1.466576099395752\n",
      "Layer 6 weight e_bits (float) = 0.6867859363555908,  m_bits (float) = 2.3845911026000977\n",
      "Layer 7 weight e_bits (float) = 0.8653300404548645,  m_bits (float) = 2.3264412879943848\n",
      "Layer 8 weight e_bits (float) = 1.4979406595230103,  m_bits (float) = 2.494191884994507\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.2500007748603821,  m_bits (float) = 0.2500016391277313\n",
      "Layer 7 bias e_bits (float) = 0.2500007748603821,  m_bits (float) = 0.2500016689300537\n",
      "Layer 8 bias e_bits (float) = 0.25000080466270447,  m_bits (float) = 0.2500016689300537\n",
      "Train set: Average loss: 0.5227\n",
      "Test set: Average loss: 0.0230, Accuracy: 0.9942 (99.42%)\n",
      "Layer 0 input e_bits (float) = 0.4994819164276123,  m_bits (float) = 1.4983044862747192\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5353333353996277,  m_bits (float) = 1.5417914390563965\n",
      "Layer 1 weight e_bits (float) = 0.5133129358291626,  m_bits (float) = 2.4874637126922607\n",
      "Layer 2 weight e_bits (float) = 0.5015653967857361,  m_bits (float) = 2.4664828777313232\n",
      "Layer 3 weight e_bits (float) = 1.5330091714859009,  m_bits (float) = 1.5194462537765503\n",
      "Layer 4 weight e_bits (float) = 1.5176928043365479,  m_bits (float) = 1.4648265838623047\n",
      "Layer 5 weight e_bits (float) = 1.5001109838485718,  m_bits (float) = 1.4410960674285889\n",
      "Layer 6 weight e_bits (float) = 0.6118713617324829,  m_bits (float) = 2.259997606277466\n",
      "Layer 7 weight e_bits (float) = 0.6936799883842468,  m_bits (float) = 2.217846155166626\n",
      "Layer 8 weight e_bits (float) = 1.4982080459594727,  m_bits (float) = 2.480250120162964\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.2500004768371582,  m_bits (float) = 0.2500010132789612\n",
      "Layer 7 bias e_bits (float) = 0.2500004768371582,  m_bits (float) = 0.25000104308128357\n",
      "Layer 8 bias e_bits (float) = 0.2500004768371582,  m_bits (float) = 0.25000104308128357\n",
      "Train set: Average loss: 0.5198\n",
      "Test set: Average loss: 0.0236, Accuracy: 0.9935 (99.35%)\n",
      "Layer 0 input e_bits (float) = 0.4997226893901825,  m_bits (float) = 1.5127043724060059\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5026071667671204,  m_bits (float) = 1.5000555515289307\n",
      "Layer 1 weight e_bits (float) = 0.5086597800254822,  m_bits (float) = 2.440953493118286\n",
      "Layer 2 weight e_bits (float) = 0.5015016198158264,  m_bits (float) = 2.4349782466888428\n",
      "Layer 3 weight e_bits (float) = 1.524410605430603,  m_bits (float) = 1.5029895305633545\n",
      "Layer 4 weight e_bits (float) = 1.5144346952438354,  m_bits (float) = 1.4554076194763184\n",
      "Layer 5 weight e_bits (float) = 1.5014606714248657,  m_bits (float) = 1.440446138381958\n",
      "Layer 6 weight e_bits (float) = 0.5775376558303833,  m_bits (float) = 2.1585614681243896\n",
      "Layer 7 weight e_bits (float) = 0.5633273720741272,  m_bits (float) = 2.036536455154419\n",
      "Layer 8 weight e_bits (float) = 1.4920605421066284,  m_bits (float) = 2.44708251953125\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.2500002980232239,  m_bits (float) = 0.25000062584877014\n",
      "Layer 7 bias e_bits (float) = 0.2500002980232239,  m_bits (float) = 0.25000062584877014\n",
      "Layer 8 bias e_bits (float) = 0.2500002980232239,  m_bits (float) = 0.25000065565109253\n",
      "Train set: Average loss: 0.5185\n",
      "Test set: Average loss: 0.0198, Accuracy: 0.9956 (99.56%)\n",
      "Layer 0 input e_bits (float) = 0.5017850399017334,  m_bits (float) = 1.5037037134170532\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5157607197761536,  m_bits (float) = 1.5468732118606567\n",
      "Layer 1 weight e_bits (float) = 0.5062902569770813,  m_bits (float) = 2.421861171722412\n",
      "Layer 2 weight e_bits (float) = 0.5111687183380127,  m_bits (float) = 2.4804723262786865\n",
      "Layer 3 weight e_bits (float) = 1.5073436498641968,  m_bits (float) = 1.4804489612579346\n",
      "Layer 4 weight e_bits (float) = 1.5002840757369995,  m_bits (float) = 1.4331557750701904\n",
      "Layer 5 weight e_bits (float) = 1.5005947351455688,  m_bits (float) = 1.4177334308624268\n",
      "Layer 6 weight e_bits (float) = 0.5556908249855042,  m_bits (float) = 2.021470069885254\n",
      "Layer 7 weight e_bits (float) = 0.5252300500869751,  m_bits (float) = 1.8133819103240967\n",
      "Layer 8 weight e_bits (float) = 1.4708541631698608,  m_bits (float) = 2.30995512008667\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.2500001788139343,  m_bits (float) = 0.25000038743019104\n",
      "Layer 7 bias e_bits (float) = 0.2500001788139343,  m_bits (float) = 0.25000038743019104\n",
      "Layer 8 bias e_bits (float) = 0.2500001788139343,  m_bits (float) = 0.25000038743019104\n",
      "Train set: Average loss: 0.5197\n",
      "Test set: Average loss: 0.2025, Accuracy: 0.9835 (98.35%)\n",
      "Layer 0 input e_bits (float) = 0.5016769766807556,  m_bits (float) = 1.499401330947876\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5157612562179565,  m_bits (float) = 1.546871542930603\n",
      "Layer 1 weight e_bits (float) = 0.506374180316925,  m_bits (float) = 2.362436532974243\n",
      "Layer 2 weight e_bits (float) = 0.5101910829544067,  m_bits (float) = 2.468583822250366\n",
      "Layer 3 weight e_bits (float) = 1.5124399662017822,  m_bits (float) = 1.4733165502548218\n",
      "Layer 4 weight e_bits (float) = 1.522212028503418,  m_bits (float) = 1.4394060373306274\n",
      "Layer 5 weight e_bits (float) = 1.4997363090515137,  m_bits (float) = 1.3864012956619263\n",
      "Layer 6 weight e_bits (float) = 0.5070277452468872,  m_bits (float) = 1.8323949575424194\n",
      "Layer 7 weight e_bits (float) = 0.5017918944358826,  m_bits (float) = 1.6633394956588745\n",
      "Layer 8 weight e_bits (float) = 1.3909680843353271,  m_bits (float) = 2.0560765266418457\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25000011920928955,  m_bits (float) = 0.2500002384185791\n",
      "Layer 7 bias e_bits (float) = 0.25000011920928955,  m_bits (float) = 0.2500002384185791\n",
      "Layer 8 bias e_bits (float) = 0.25000011920928955,  m_bits (float) = 0.2500002384185791\n",
      "Train set: Average loss: 0.5138\n",
      "Test set: Average loss: 0.0271, Accuracy: 0.9945 (99.45%)\n",
      "Layer 0 input e_bits (float) = 0.5018865466117859,  m_bits (float) = 1.4996492862701416\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5157613754272461,  m_bits (float) = 1.5468653440475464\n",
      "Layer 1 weight e_bits (float) = 0.5117108821868896,  m_bits (float) = 2.2707886695861816\n",
      "Layer 2 weight e_bits (float) = 0.5077387690544128,  m_bits (float) = 2.4472808837890625\n",
      "Layer 3 weight e_bits (float) = 1.529117226600647,  m_bits (float) = 1.484343409538269\n",
      "Layer 4 weight e_bits (float) = 1.5067555904388428,  m_bits (float) = 1.421042799949646\n",
      "Layer 5 weight e_bits (float) = 1.532664179801941,  m_bits (float) = 1.3141491413116455\n",
      "Layer 6 weight e_bits (float) = 0.5058534741401672,  m_bits (float) = 1.6678125858306885\n",
      "Layer 7 weight e_bits (float) = 0.5004094839096069,  m_bits (float) = 1.5021982192993164\n",
      "Layer 8 weight e_bits (float) = 1.323093295097351,  m_bits (float) = 1.7731976509094238\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.2500000596046448,  m_bits (float) = 0.25000014901161194\n",
      "Layer 7 bias e_bits (float) = 0.2500000596046448,  m_bits (float) = 0.25000014901161194\n",
      "Layer 8 bias e_bits (float) = 0.2500000596046448,  m_bits (float) = 0.25000014901161194\n",
      "Train set: Average loss: 0.5313\n",
      "Test set: Average loss: 0.0405, Accuracy: 0.9939 (99.39%)\n",
      "Layer 0 input e_bits (float) = 0.5005516409873962,  m_bits (float) = 1.5318540334701538\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5157575607299805,  m_bits (float) = 1.5468403100967407\n",
      "Layer 1 weight e_bits (float) = 0.509408175945282,  m_bits (float) = 2.1321933269500732\n",
      "Layer 2 weight e_bits (float) = 0.5054243803024292,  m_bits (float) = 2.439239501953125\n",
      "Layer 3 weight e_bits (float) = 1.5071662664413452,  m_bits (float) = 1.4633077383041382\n",
      "Layer 4 weight e_bits (float) = 1.5181758403778076,  m_bits (float) = 1.4251912832260132\n",
      "Layer 5 weight e_bits (float) = 1.5050252676010132,  m_bits (float) = 1.1501014232635498\n",
      "Layer 6 weight e_bits (float) = 0.5020506978034973,  m_bits (float) = 1.5035576820373535\n",
      "Layer 7 weight e_bits (float) = 0.5031704902648926,  m_bits (float) = 1.5078078508377075\n",
      "Layer 8 weight e_bits (float) = 1.2147341966629028,  m_bits (float) = 1.500905990600586\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.2500000298023224,  m_bits (float) = 0.25000008940696716\n",
      "Layer 7 bias e_bits (float) = 0.2500000298023224,  m_bits (float) = 0.25000008940696716\n",
      "Layer 8 bias e_bits (float) = 0.2500000298023224,  m_bits (float) = 0.25000008940696716\n",
      "Train set: Average loss: 0.5212\n",
      "Test set: Average loss: 0.0208, Accuracy: 0.9944 (99.44%)\n",
      "Layer 0 input e_bits (float) = 0.5003698468208313,  m_bits (float) = 1.5139856338500977\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5157501101493835,  m_bits (float) = 1.5467994213104248\n",
      "Layer 1 weight e_bits (float) = 0.5122817158699036,  m_bits (float) = 1.8577710390090942\n",
      "Layer 2 weight e_bits (float) = 0.505176842212677,  m_bits (float) = 2.3932342529296875\n",
      "Layer 3 weight e_bits (float) = 1.5077970027923584,  m_bits (float) = 1.4564939737319946\n",
      "Layer 4 weight e_bits (float) = 1.5324468612670898,  m_bits (float) = 1.4441642761230469\n",
      "Layer 5 weight e_bits (float) = 1.498159646987915,  m_bits (float) = 0.9555836319923401\n",
      "Layer 6 weight e_bits (float) = 0.5009419918060303,  m_bits (float) = 1.5008960962295532\n",
      "Layer 7 weight e_bits (float) = 0.5033438205718994,  m_bits (float) = 1.5098326206207275\n",
      "Layer 8 weight e_bits (float) = 1.118990421295166,  m_bits (float) = 1.507265567779541\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.2500000298023224,  m_bits (float) = 0.2500000596046448\n",
      "Layer 7 bias e_bits (float) = 0.2500000298023224,  m_bits (float) = 0.2500000596046448\n",
      "Layer 8 bias e_bits (float) = 0.2500000298023224,  m_bits (float) = 0.2500000596046448\n",
      "Train set: Average loss: 0.5243\n",
      "Test set: Average loss: 0.0368, Accuracy: 0.9895 (98.95%)\n",
      "Layer 0 input e_bits (float) = 0.4932403564453125,  m_bits (float) = 1.5002731084823608\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.515740692615509,  m_bits (float) = 1.5467389822006226\n",
      "Layer 1 weight e_bits (float) = 0.5051267743110657,  m_bits (float) = 1.589318037033081\n",
      "Layer 2 weight e_bits (float) = 0.5025540590286255,  m_bits (float) = 2.368867874145508\n",
      "Layer 3 weight e_bits (float) = 1.518500566482544,  m_bits (float) = 1.473625898361206\n",
      "Layer 4 weight e_bits (float) = 1.5021353960037231,  m_bits (float) = 1.4006829261779785\n",
      "Layer 5 weight e_bits (float) = 1.5015618801116943,  m_bits (float) = 0.7478532791137695\n",
      "Layer 6 weight e_bits (float) = 0.5000256896018982,  m_bits (float) = 1.4990856647491455\n",
      "Layer 7 weight e_bits (float) = 0.5037366151809692,  m_bits (float) = 1.5090664625167847\n",
      "Layer 8 weight e_bits (float) = 0.9906148314476013,  m_bits (float) = 1.5052942037582397\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.2500000298023224\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.2500000298023224\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.2500000298023224\n",
      "Train set: Average loss: 0.5141\n",
      "Test set: Average loss: 0.0416, Accuracy: 0.9949 (99.49%)\n",
      "Layer 0 input e_bits (float) = 0.4985564649105072,  m_bits (float) = 1.4988709688186646\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5157522559165955,  m_bits (float) = 1.546720266342163\n",
      "Layer 1 weight e_bits (float) = 0.5154867768287659,  m_bits (float) = 1.5191892385482788\n",
      "Layer 2 weight e_bits (float) = 0.5066339373588562,  m_bits (float) = 2.3340542316436768\n",
      "Layer 3 weight e_bits (float) = 1.528628945350647,  m_bits (float) = 1.448851227760315\n",
      "Layer 4 weight e_bits (float) = 1.50471031665802,  m_bits (float) = 1.3875943422317505\n",
      "Layer 5 weight e_bits (float) = 1.5024231672286987,  m_bits (float) = 0.5591374635696411\n",
      "Layer 6 weight e_bits (float) = 0.5010581016540527,  m_bits (float) = 1.503137230873108\n",
      "Layer 7 weight e_bits (float) = 0.501512348651886,  m_bits (float) = 1.5002750158309937\n",
      "Layer 8 weight e_bits (float) = 0.8334801197052002,  m_bits (float) = 1.5006937980651855\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5106\n",
      "Test set: Average loss: 0.0306, Accuracy: 0.9934 (99.34%)\n",
      "Layer 0 input e_bits (float) = 0.49956199526786804,  m_bits (float) = 1.5059616565704346\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5157769918441772,  m_bits (float) = 1.5467123985290527\n",
      "Layer 1 weight e_bits (float) = 0.5160724520683289,  m_bits (float) = 1.5207120180130005\n",
      "Layer 2 weight e_bits (float) = 0.5076311230659485,  m_bits (float) = 2.152949571609497\n",
      "Layer 3 weight e_bits (float) = 1.5240062475204468,  m_bits (float) = 1.4284098148345947\n",
      "Layer 4 weight e_bits (float) = 1.5011045932769775,  m_bits (float) = 1.3698012828826904\n",
      "Layer 5 weight e_bits (float) = 1.501944899559021,  m_bits (float) = 0.49781692028045654\n",
      "Layer 6 weight e_bits (float) = 0.5003208518028259,  m_bits (float) = 1.5037853717803955\n",
      "Layer 7 weight e_bits (float) = 0.5003994703292847,  m_bits (float) = 1.4999674558639526\n",
      "Layer 8 weight e_bits (float) = 0.6636418104171753,  m_bits (float) = 1.5013784170150757\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5139\n",
      "Test set: Average loss: 0.0668, Accuracy: 0.9914 (99.14%)\n",
      "Layer 0 input e_bits (float) = 0.5038399696350098,  m_bits (float) = 1.51255464553833\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5156883597373962,  m_bits (float) = 1.5463114976882935\n",
      "Layer 1 weight e_bits (float) = 0.5160111784934998,  m_bits (float) = 1.5201990604400635\n",
      "Layer 2 weight e_bits (float) = 0.49306726455688477,  m_bits (float) = 1.8462309837341309\n",
      "Layer 3 weight e_bits (float) = 1.5051263570785522,  m_bits (float) = 1.3927491903305054\n",
      "Layer 4 weight e_bits (float) = 1.5022268295288086,  m_bits (float) = 1.3400242328643799\n",
      "Layer 5 weight e_bits (float) = 1.5473237037658691,  m_bits (float) = 0.49528416991233826\n",
      "Layer 6 weight e_bits (float) = 0.5083312392234802,  m_bits (float) = 1.5245848894119263\n",
      "Layer 7 weight e_bits (float) = 0.506293773651123,  m_bits (float) = 1.514330267906189\n",
      "Layer 8 weight e_bits (float) = 0.5689746141433716,  m_bits (float) = 1.505749225616455\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5122\n",
      "Test set: Average loss: 0.0231, Accuracy: 0.9938 (99.38%)\n",
      "Layer 0 input e_bits (float) = 0.49957039952278137,  m_bits (float) = 1.503709077835083\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5156745314598083,  m_bits (float) = 1.5460559129714966\n",
      "Layer 1 weight e_bits (float) = 0.5158519744873047,  m_bits (float) = 1.5191972255706787\n",
      "Layer 2 weight e_bits (float) = 0.5093350410461426,  m_bits (float) = 1.5411591529846191\n",
      "Layer 3 weight e_bits (float) = 1.5208442211151123,  m_bits (float) = 1.3939123153686523\n",
      "Layer 4 weight e_bits (float) = 1.5026873350143433,  m_bits (float) = 1.2074637413024902\n",
      "Layer 5 weight e_bits (float) = 1.545391321182251,  m_bits (float) = 0.49640485644340515\n",
      "Layer 6 weight e_bits (float) = 0.5045995116233826,  m_bits (float) = 1.5109524726867676\n",
      "Layer 7 weight e_bits (float) = 0.5015861392021179,  m_bits (float) = 1.4990978240966797\n",
      "Layer 8 weight e_bits (float) = 0.5226574540138245,  m_bits (float) = 1.5096080303192139\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5079\n",
      "Test set: Average loss: 0.0459, Accuracy: 0.9941 (99.41%)\n",
      "Layer 0 input e_bits (float) = 0.4963667094707489,  m_bits (float) = 1.5247637033462524\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5155147314071655,  m_bits (float) = 1.5452347993850708\n",
      "Layer 1 weight e_bits (float) = 0.515652596950531,  m_bits (float) = 1.5177597999572754\n",
      "Layer 2 weight e_bits (float) = 0.512973427772522,  m_bits (float) = 1.5216442346572876\n",
      "Layer 3 weight e_bits (float) = 1.5388367176055908,  m_bits (float) = 1.3638548851013184\n",
      "Layer 4 weight e_bits (float) = 1.5170624256134033,  m_bits (float) = 1.0615233182907104\n",
      "Layer 5 weight e_bits (float) = 1.541957974433899,  m_bits (float) = 0.4972040355205536\n",
      "Layer 6 weight e_bits (float) = 0.5108454823493958,  m_bits (float) = 1.531667709350586\n",
      "Layer 7 weight e_bits (float) = 0.5040574669837952,  m_bits (float) = 1.5101747512817383\n",
      "Layer 8 weight e_bits (float) = 0.5135194659233093,  m_bits (float) = 1.537060260772705\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5038\n",
      "Test set: Average loss: 0.0419, Accuracy: 0.9946 (99.46%)\n",
      "Layer 0 input e_bits (float) = 0.4998771846294403,  m_bits (float) = 1.5087403059005737\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5153502821922302,  m_bits (float) = 1.5441944599151611\n",
      "Layer 1 weight e_bits (float) = 0.5156939625740051,  m_bits (float) = 1.5165258646011353\n",
      "Layer 2 weight e_bits (float) = 0.5160666704177856,  m_bits (float) = 1.5460675954818726\n",
      "Layer 3 weight e_bits (float) = 1.5186673402786255,  m_bits (float) = 1.2104928493499756\n",
      "Layer 4 weight e_bits (float) = 1.502777338027954,  m_bits (float) = 0.8846898674964905\n",
      "Layer 5 weight e_bits (float) = 1.537559986114502,  m_bits (float) = 0.4987533390522003\n",
      "Layer 6 weight e_bits (float) = 0.5015944838523865,  m_bits (float) = 1.5002052783966064\n",
      "Layer 7 weight e_bits (float) = 0.5087909698486328,  m_bits (float) = 1.5241634845733643\n",
      "Layer 8 weight e_bits (float) = 0.5096986293792725,  m_bits (float) = 1.5232521295547485\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5057\n",
      "Test set: Average loss: 0.0288, Accuracy: 0.9941 (99.41%)\n",
      "Layer 0 input e_bits (float) = 0.4973444938659668,  m_bits (float) = 1.504271388053894\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5155929923057556,  m_bits (float) = 1.5440481901168823\n",
      "Layer 1 weight e_bits (float) = 0.5154728889465332,  m_bits (float) = 1.513712763786316\n",
      "Layer 2 weight e_bits (float) = 0.5160545706748962,  m_bits (float) = 1.5459707975387573\n",
      "Layer 3 weight e_bits (float) = 1.510462760925293,  m_bits (float) = 1.0517221689224243\n",
      "Layer 4 weight e_bits (float) = 1.4988048076629639,  m_bits (float) = 0.6961380839347839\n",
      "Layer 5 weight e_bits (float) = 1.5290659666061401,  m_bits (float) = 0.49925655126571655\n",
      "Layer 6 weight e_bits (float) = 0.5053473114967346,  m_bits (float) = 1.5127553939819336\n",
      "Layer 7 weight e_bits (float) = 0.5114430785179138,  m_bits (float) = 1.5302211046218872\n",
      "Layer 8 weight e_bits (float) = 0.5043508410453796,  m_bits (float) = 1.5035486221313477\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5020\n",
      "Test set: Average loss: 0.6106, Accuracy: 0.7585 (75.85%)\n",
      "Layer 0 input e_bits (float) = 0.4990992248058319,  m_bits (float) = 1.501869559288025\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5153105854988098,  m_bits (float) = 1.5418099164962769\n",
      "Layer 1 weight e_bits (float) = 0.5146117210388184,  m_bits (float) = 1.50774347782135\n",
      "Layer 2 weight e_bits (float) = 0.5160519480705261,  m_bits (float) = 1.545872688293457\n",
      "Layer 3 weight e_bits (float) = 1.5001029968261719,  m_bits (float) = 0.8641438484191895\n",
      "Layer 4 weight e_bits (float) = 1.5307044982910156,  m_bits (float) = 0.554419219493866\n",
      "Layer 5 weight e_bits (float) = 1.5185049772262573,  m_bits (float) = 0.500257670879364\n",
      "Layer 6 weight e_bits (float) = 0.5078759789466858,  m_bits (float) = 1.522809624671936\n",
      "Layer 7 weight e_bits (float) = 0.49944597482681274,  m_bits (float) = 1.4999816417694092\n",
      "Layer 8 weight e_bits (float) = 0.5151276588439941,  m_bits (float) = 1.5447328090667725\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5030\n",
      "Test set: Average loss: 0.0774, Accuracy: 0.9897 (98.97%)\n",
      "Layer 0 input e_bits (float) = 0.49636122584342957,  m_bits (float) = 1.4989140033721924\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5146054625511169,  m_bits (float) = 1.537483811378479\n",
      "Layer 1 weight e_bits (float) = 0.5133641958236694,  m_bits (float) = 1.500427007675171\n",
      "Layer 2 weight e_bits (float) = 0.516021728515625,  m_bits (float) = 1.545632243156433\n",
      "Layer 3 weight e_bits (float) = 1.5048898458480835,  m_bits (float) = 0.6705405116081238\n",
      "Layer 4 weight e_bits (float) = 1.5416333675384521,  m_bits (float) = 0.5156288146972656\n",
      "Layer 5 weight e_bits (float) = 1.5054198503494263,  m_bits (float) = 0.5006867051124573\n",
      "Layer 6 weight e_bits (float) = 0.5133122801780701,  m_bits (float) = 1.5393400192260742\n",
      "Layer 7 weight e_bits (float) = 0.5058265924453735,  m_bits (float) = 1.5204535722732544\n",
      "Layer 8 weight e_bits (float) = 0.5123618841171265,  m_bits (float) = 1.5350929498672485\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.4972\n",
      "Test set: Average loss: 0.0424, Accuracy: 0.9943 (99.43%)\n",
      "Layer 0 input e_bits (float) = 0.49492785334587097,  m_bits (float) = 1.518221378326416\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5142614245414734,  m_bits (float) = 1.532936692237854\n",
      "Layer 1 weight e_bits (float) = 0.5108082890510559,  m_bits (float) = 1.5015177726745605\n",
      "Layer 2 weight e_bits (float) = 0.5159087777137756,  m_bits (float) = 1.5450520515441895\n",
      "Layer 3 weight e_bits (float) = 1.514849066734314,  m_bits (float) = 0.5331060290336609\n",
      "Layer 4 weight e_bits (float) = 1.5392162799835205,  m_bits (float) = 0.515227198600769\n",
      "Layer 5 weight e_bits (float) = 1.546185851097107,  m_bits (float) = 0.5147729516029358\n",
      "Layer 6 weight e_bits (float) = 0.5097969770431519,  m_bits (float) = 1.5261733531951904\n",
      "Layer 7 weight e_bits (float) = 0.5033614635467529,  m_bits (float) = 1.5054370164871216\n",
      "Layer 8 weight e_bits (float) = 0.5095261335372925,  m_bits (float) = 1.5244660377502441\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5035\n",
      "Test set: Average loss: 0.0435, Accuracy: 0.993 (99.30%)\n",
      "Layer 0 input e_bits (float) = 0.5038092732429504,  m_bits (float) = 1.4994370937347412\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5144680142402649,  m_bits (float) = 1.5279854536056519\n",
      "Layer 1 weight e_bits (float) = 0.5046590566635132,  m_bits (float) = 1.5055593252182007\n",
      "Layer 2 weight e_bits (float) = 0.5157013535499573,  m_bits (float) = 1.5440471172332764\n",
      "Layer 3 weight e_bits (float) = 1.5481427907943726,  m_bits (float) = 0.5160340070724487\n",
      "Layer 4 weight e_bits (float) = 1.53237783908844,  m_bits (float) = 0.51397305727005\n",
      "Layer 5 weight e_bits (float) = 1.542778730392456,  m_bits (float) = 0.5142126679420471\n",
      "Layer 6 weight e_bits (float) = 0.5131499767303467,  m_bits (float) = 1.5391594171524048\n",
      "Layer 7 weight e_bits (float) = 0.5088343024253845,  m_bits (float) = 1.5258584022521973\n",
      "Layer 8 weight e_bits (float) = 0.5051847100257874,  m_bits (float) = 1.5081688165664673\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.4970\n",
      "Test set: Average loss: 0.0528, Accuracy: 0.9939 (99.39%)\n",
      "Layer 0 input e_bits (float) = 0.5034916400909424,  m_bits (float) = 1.5023784637451172\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5120999813079834,  m_bits (float) = 1.5122705698013306\n",
      "Layer 1 weight e_bits (float) = 0.5019587874412537,  m_bits (float) = 1.503779649734497\n",
      "Layer 2 weight e_bits (float) = 0.5155198574066162,  m_bits (float) = 1.5428918600082397\n",
      "Layer 3 weight e_bits (float) = 1.548065185546875,  m_bits (float) = 0.5160155296325684\n",
      "Layer 4 weight e_bits (float) = 1.5259203910827637,  m_bits (float) = 0.5126194357872009\n",
      "Layer 5 weight e_bits (float) = 1.5368242263793945,  m_bits (float) = 0.5130751132965088\n",
      "Layer 6 weight e_bits (float) = 0.507937490940094,  m_bits (float) = 1.5206701755523682\n",
      "Layer 7 weight e_bits (float) = 0.5054857730865479,  m_bits (float) = 1.5063440799713135\n",
      "Layer 8 weight e_bits (float) = 0.5153332352638245,  m_bits (float) = 1.5458359718322754\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.4976\n",
      "Test set: Average loss: 0.0426, Accuracy: 0.9947 (99.47%)\n",
      "Layer 0 input e_bits (float) = 0.5000245571136475,  m_bits (float) = 1.50144624710083\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5104537010192871,  m_bits (float) = 1.5005725622177124\n",
      "Layer 1 weight e_bits (float) = 0.5128827691078186,  m_bits (float) = 1.5356389284133911\n",
      "Layer 2 weight e_bits (float) = 0.5152508616447449,  m_bits (float) = 1.5411103963851929\n",
      "Layer 3 weight e_bits (float) = 1.5479320287704468,  m_bits (float) = 0.5160214304924011\n",
      "Layer 4 weight e_bits (float) = 1.5160009860992432,  m_bits (float) = 0.5104323625564575\n",
      "Layer 5 weight e_bits (float) = 1.5338971614837646,  m_bits (float) = 0.5127995014190674\n",
      "Layer 6 weight e_bits (float) = 0.5009292960166931,  m_bits (float) = 1.5007610321044922\n",
      "Layer 7 weight e_bits (float) = 0.5027263164520264,  m_bits (float) = 1.5019047260284424\n",
      "Layer 8 weight e_bits (float) = 0.5138858556747437,  m_bits (float) = 1.5398131608963013\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.4996\n",
      "Test set: Average loss: 0.0415, Accuracy: 0.9951 (99.51%)\n",
      "Layer 0 input e_bits (float) = 0.49863436818122864,  m_bits (float) = 1.49656081199646\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5140525102615356,  m_bits (float) = 1.5153862237930298\n",
      "Layer 1 weight e_bits (float) = 0.5128452181816101,  m_bits (float) = 1.5351271629333496\n",
      "Layer 2 weight e_bits (float) = 0.5148683190345764,  m_bits (float) = 1.5384074449539185\n",
      "Layer 3 weight e_bits (float) = 1.5476127862930298,  m_bits (float) = 0.5159752368927002\n",
      "Layer 4 weight e_bits (float) = 1.4990441799163818,  m_bits (float) = 0.5065832138061523\n",
      "Layer 5 weight e_bits (float) = 1.5197837352752686,  m_bits (float) = 0.5095409154891968\n",
      "Layer 6 weight e_bits (float) = 0.5102788209915161,  m_bits (float) = 1.5287978649139404\n",
      "Layer 7 weight e_bits (float) = 0.5078557729721069,  m_bits (float) = 1.5203425884246826\n",
      "Layer 8 weight e_bits (float) = 0.5110742449760437,  m_bits (float) = 1.528731346130371\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.4960\n",
      "Test set: Average loss: 0.0316, Accuracy: 0.9929 (99.29%)\n",
      "Layer 0 input e_bits (float) = 0.5009899139404297,  m_bits (float) = 1.499819278717041\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5052156448364258,  m_bits (float) = 1.4993853569030762\n",
      "Layer 1 weight e_bits (float) = 0.5127105116844177,  m_bits (float) = 1.534088373184204\n",
      "Layer 2 weight e_bits (float) = 0.5143653154373169,  m_bits (float) = 1.5344210863113403\n",
      "Layer 3 weight e_bits (float) = 1.547281265258789,  m_bits (float) = 0.5159021615982056\n",
      "Layer 4 weight e_bits (float) = 1.5004205703735352,  m_bits (float) = 0.5013597011566162\n",
      "Layer 5 weight e_bits (float) = 1.5031648874282837,  m_bits (float) = 0.5040448904037476\n",
      "Layer 6 weight e_bits (float) = 0.505906879901886,  m_bits (float) = 1.5119619369506836\n",
      "Layer 7 weight e_bits (float) = 0.5016154050827026,  m_bits (float) = 1.4983799457550049\n",
      "Layer 8 weight e_bits (float) = 0.5075592994689941,  m_bits (float) = 1.5140326023101807\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.4966\n",
      "Test set: Average loss: 0.0164, Accuracy: 0.9956 (99.56%)\n",
      "Layer 0 input e_bits (float) = 0.4982881546020508,  m_bits (float) = 1.5057616233825684\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5004915595054626,  m_bits (float) = 1.5018439292907715\n",
      "Layer 1 weight e_bits (float) = 0.512444257736206,  m_bits (float) = 1.5322763919830322\n",
      "Layer 2 weight e_bits (float) = 0.5139210820198059,  m_bits (float) = 1.5291471481323242\n",
      "Layer 3 weight e_bits (float) = 1.5464718341827393,  m_bits (float) = 0.5157352685928345\n",
      "Layer 4 weight e_bits (float) = 1.5430552959442139,  m_bits (float) = 0.5137197375297546\n",
      "Layer 5 weight e_bits (float) = 1.5004312992095947,  m_bits (float) = 0.4996803402900696\n",
      "Layer 6 weight e_bits (float) = 0.5010907649993896,  m_bits (float) = 1.4997378587722778\n",
      "Layer 7 weight e_bits (float) = 0.5022746324539185,  m_bits (float) = 1.4993577003479004\n",
      "Layer 8 weight e_bits (float) = 0.5032303929328918,  m_bits (float) = 1.5012739896774292\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Train set: Average loss: 0.5022\n",
      "Test set: Average loss: 0.0956, Accuracy: 0.9919 (99.19%)\n",
      "Layer 0 input e_bits (float) = 0.5013169050216675,  m_bits (float) = 1.5189136266708374\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 7 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 8 input e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 0 weight e_bits (float) = 0.5160998702049255,  m_bits (float) = 1.5410715341567993\n",
      "Layer 1 weight e_bits (float) = 0.5124050378799438,  m_bits (float) = 1.5305376052856445\n",
      "Layer 2 weight e_bits (float) = 0.5122863054275513,  m_bits (float) = 1.5180480480194092\n",
      "Layer 3 weight e_bits (float) = 1.5458680391311646,  m_bits (float) = 0.5155211091041565\n",
      "Layer 4 weight e_bits (float) = 1.5302680730819702,  m_bits (float) = 0.5101581811904907\n",
      "Layer 5 weight e_bits (float) = 1.5432478189468384,  m_bits (float) = 0.5143296718597412\n",
      "Layer 6 weight e_bits (float) = 0.505091667175293,  m_bits (float) = 1.5121829509735107\n",
      "Layer 7 weight e_bits (float) = 0.5034248232841492,  m_bits (float) = 1.508664608001709\n",
      "Layer 8 weight e_bits (float) = 0.5110094547271729,  m_bits (float) = 1.5301640033721924\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 2 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 3 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 4 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 5 bias e_bits (float) = 8.0,  m_bits (float) = 23.0\n",
      "Layer 6 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 7 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n",
      "Layer 8 bias e_bits (float) = 0.25,  m_bits (float) = 0.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m test(model, device, test_loader)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(accuracy \u001b[38;5;241m>\u001b[39m best_accuracy):\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, lambda_bw)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#penalty_bw = bitwidth_sum(model) \u001b[39;00m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_ce \u001b[38;5;241m+\u001b[39m lambda_bw\u001b[38;5;241m*\u001b[39mpenalty_bw\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 96\u001b[0m, in \u001b[0;36mFakeFloatFunction.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m#diff_e = (f_plus - f_minus) * grad_output\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m#grad_e_bits = diff_e.sum() / (2.0 * delta)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     diff_e \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mf_plus2 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m8\u001b[39m\u001b[38;5;241m*\u001b[39mf_plus \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m8\u001b[39m\u001b[38;5;241m*\u001b[39mf_minus \u001b[38;5;241m+\u001b[39m f_minus2) \u001b[38;5;241m*\u001b[39m grad_output\n\u001b[0;32m---> 96\u001b[0m     grad_m_bits \u001b[38;5;241m=\u001b[39m \u001b[43mdiff_e\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m12.0\u001b[39m \u001b[38;5;241m*\u001b[39m delta)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_x, grad_e_bits, grad_m_bits\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"using divice \", device)\n",
    "\n",
    "# Create model\n",
    "# model = SimpleQuantizedMLP(e_bits=4.0, m_bits=4.0, num_classes=len(classes)).to(device)\n",
    "model = SimpleQuantizedCNN(num_classes=len(classes)).to(device)\n",
    "#model = SimpleCIFAR10Model(num_classes=len(classes)).to(device)\n",
    "\n",
    "best_model_path = \"train_weights_and_quant_MNIST_best_model.pth\"\n",
    "if(os.path.isfile(best_model_path)):\n",
    "    model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "\n",
    "# Create optimizer (SGD or Adam)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "quant_model_path = \"train_weights_and_quant_MNIST_quant_model.pth\"\n",
    "\n",
    "best_accuracy = 0.0\n",
    "# Train for some epochs\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    accuracy = test(model, device, test_loader)\n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), quant_model_path)\n",
    "        \n",
    "    model.printBitWidths()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
