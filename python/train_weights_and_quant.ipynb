{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "batch_size = 128\n",
    "#dataset = \"MNIST\"\n",
    "dataset = \"CIFAR10\"\n",
    "#dataset = \"CIFAR100\"\n",
    "\n",
    "if(dataset == \"MNIST\"):\n",
    "    # 1) MNIST Dataset & Dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "    input_size = (1, 32, 32)\n",
    "\n",
    "if(dataset == \"CIFAR10\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    input_size = (3, 32, 32)\n",
    "\n",
    "if(dataset == \"CIFAR100\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = [x for x in range(100)]\n",
    "\n",
    "    input_size = (3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9894737..2.1264887].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: a batch of images in shape (B, C, H, W) or a single image in (C, H, W).\n",
    "    \"\"\"\n",
    "    # If it's a batch of images (4D), make a grid first\n",
    "    if len(img_tensor.shape) == 4:\n",
    "        img_tensor = torchvision.utils.make_grid(img_tensor)\n",
    "    # Unnormalize\n",
    "    #img_tensor = unnormalize(img_tensor)\n",
    "    # Convert to numpy\n",
    "    npimg = img_tensor.numpy()\n",
    "    # Transpose from (C, H, W) to (H, W, C)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_bit(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(x)\n",
    "\n",
    "def bit_to_param(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_float_truncate(x: torch.Tensor, e_bits_int: int, m_bits_int: int, scale_int: int) -> torch.Tensor:\n",
    "\n",
    "    sign = x.sign()\n",
    "    abs_x = x.abs().clamp(min=1e-45)\n",
    "\n",
    "    #recover the floatint point representation\n",
    "    #exponent \\in {-2**7,..,2**7-1}\n",
    "    #mantissa \\in {1.0,...,2.0}\n",
    "\n",
    "    exponent = torch.floor(torch.log2(abs_x)).clamp(min=1e-45)\n",
    "    mantissa = abs_x / (2**exponent)\n",
    "\n",
    "    # truncate exponent\n",
    "    # lets parameterize the exponent as a constant value + a variable value\n",
    "    # the constant part is 2**7-1 in standar floating point, but we will learn it\n",
    "    # the variable part \\in {0,..,2**8-1}\n",
    "    # lets say exponent = v_exponent - 2**(bits-1)-1 + c_exponent\n",
    "    # so v_exponent = exponent + 2**(bits-1)-1 - c_exponent\n",
    "    c_exponent = scale_int\n",
    "    v_exponent = exponent + (2**(e_bits_int-1)-1) - c_exponent\n",
    "    \n",
    "    # the valriable part is clamped to the alloted bits\n",
    "    q_min = torch.tensor(float(0)).to(x.device)\n",
    "    q_max = 2**e_bits_int-1\n",
    "    q_exponent = torch.clamp(v_exponent, q_min, q_max) - (2**(e_bits_int-1)-1) + c_exponent\n",
    "\n",
    "    # truncate mantissa\n",
    "    # this just removes the less significant bits\n",
    "    m_scale = 2.0 ** m_bits_int\n",
    "    q_mantissa = torch.floor(mantissa * m_scale) / m_scale\n",
    "\n",
    "    # from quantized floatint point to float\n",
    "    fq_x = sign * (2**q_exponent) * q_mantissa\n",
    "    return fq_x\n",
    "\n",
    "class FakeFloatFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e_bits_param, m_bits_param, scale_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, e_bits_param, m_bits_param, scale_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).item())\n",
    "        s_int = int(torch.round(scale_param).item())\n",
    "\n",
    "        out = fake_float_truncate(x, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "        #print(\"input\")\n",
    "        #print(x)\n",
    "        #print(\"output\")\n",
    "        #print(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, e_bits_param, m_bits_param, scale_param = ctx.saved_tensors\n",
    "        \n",
    "        e_bits = param_to_bit(e_bits_param)\n",
    "        m_bits = param_to_bit(m_bits_param)\n",
    "        scale = scale_param\n",
    "                \n",
    "        e_bits_int = int(torch.round(e_bits).item())\n",
    "        m_bits_int = int(torch.round(m_bits).item())\n",
    "        scale_int = int(torch.round(scale).item())\n",
    "\n",
    "        #print(\"shape x: \", x.shape)\n",
    "        #print(\"shape grad_output: \", grad_output.shape)\n",
    "\n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output\n",
    "        \n",
    "        # 1) Gradient wrt x: approximate with central difference\n",
    "        \"\"\"\n",
    "        grad_x = None\n",
    "        if True:\n",
    "            delta = 0.01            \n",
    "\n",
    "            f_plus2  = fake_float_truncate(x + 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_plus   = fake_float_truncate(x + 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus  = fake_float_truncate(x - 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus2 = fake_float_truncate(x - 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "            grad_x = grad_output * der\n",
    "        \"\"\"\n",
    "                \n",
    "        # 2) Gradient wrt e_bits: approximate with central difference\n",
    "        grad_e_bits = None\n",
    "        if e_bits_param.requires_grad:\n",
    "            \n",
    "            if(e_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int    , m_bits_int, scale_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int + 2, m_bits_int, scale_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int - 1, m_bits_int, scale_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int - 2, m_bits_int, scale_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_e_bits = grad_output * der * e_bits\n",
    "        \n",
    "        # 3) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_m_bits = None\n",
    "        if m_bits_param.requires_grad:\n",
    "            \n",
    "            if(m_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int    , scale_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int + 2, scale_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int - 1, scale_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int - 2, scale_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_m_bits = grad_output * der * m_bits\n",
    "       \n",
    "        # 4) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_scale_bits = None\n",
    "        if scale_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int + 2)\n",
    "            f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int + 1)\n",
    "            f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int - 1)\n",
    "            f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int - 2)\n",
    "            \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0 \n",
    "            grad_scale_bits = grad_output * der\n",
    "             \n",
    "        return grad_x, grad_e_bits, grad_m_bits, grad_scale_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:  tensor([8.1600])  out:  tensor([8.1562])\n",
      "in:  tensor([-39.0895])  out:  tensor([-39.0625])\n",
      "in:  tensor([-5.4981])  out:  tensor([-5.4961])\n",
      "in:  tensor([-29.6607])  out:  tensor([-29.6562])\n",
      "in:  tensor([8.2018])  out:  tensor([8.1953])\n",
      "in:  tensor([35.9290])  out:  tensor([35.9062])\n",
      "in:  tensor([-29.1451])  out:  tensor([-29.1406])\n",
      "in:  tensor([-34.0348])  out:  tensor([-34.0312])\n",
      "in:  tensor([45.6004])  out:  tensor([45.5938])\n",
      "in:  tensor([-26.6778])  out:  tensor([-26.6719])\n",
      "in:  tensor([-49.4872])  out:  tensor([-49.4688])\n",
      "in:  tensor([-4.2289])  out:  tensor([-4.2266])\n",
      "in:  tensor([23.5897])  out:  tensor([23.5781])\n",
      "in:  tensor([45.3265])  out:  tensor([45.3125])\n",
      "in:  tensor([5.1383])  out:  tensor([5.1367])\n",
      "in:  tensor([44.2640])  out:  tensor([44.2500])\n",
      "in:  tensor([45.6646])  out:  tensor([45.6562])\n",
      "in:  tensor([36.9651])  out:  tensor([36.9375])\n",
      "in:  tensor([-18.0205])  out:  tensor([-18.0156])\n",
      "in:  tensor([-22.9767])  out:  tensor([-22.9688])\n",
      "in:  tensor([43.8106])  out:  tensor([43.7812])\n",
      "in:  tensor([-28.9984])  out:  tensor([-28.9844])\n",
      "in:  tensor([-44.0152])  out:  tensor([-44.])\n",
      "in:  tensor([-1.2845])  out:  tensor([-1.2842])\n",
      "in:  tensor([47.8631])  out:  tensor([47.8438])\n",
      "in:  tensor([-24.4228])  out:  tensor([-24.4219])\n",
      "in:  tensor([-20.6315])  out:  tensor([-20.6250])\n",
      "in:  tensor([-47.2920])  out:  tensor([-47.2812])\n",
      "in:  tensor([-18.2280])  out:  tensor([-18.2188])\n",
      "in:  tensor([47.5684])  out:  tensor([47.5625])\n",
      "in:  tensor([-40.3304])  out:  tensor([-40.3125])\n",
      "in:  tensor([-47.3869])  out:  tensor([-47.3750])\n",
      "in:  tensor([33.4106])  out:  tensor([33.4062])\n",
      "in:  tensor([9.5809])  out:  tensor([9.5781])\n",
      "in:  tensor([-36.1539])  out:  tensor([-36.1250])\n",
      "in:  tensor([17.6334])  out:  tensor([17.6250])\n",
      "in:  tensor([-27.6306])  out:  tensor([-27.6250])\n",
      "in:  tensor([-48.2172])  out:  tensor([-48.1875])\n",
      "in:  tensor([0.5849])  out:  tensor([0.5840])\n",
      "in:  tensor([24.6853])  out:  tensor([24.6719])\n",
      "in:  tensor([3.7863])  out:  tensor([3.7852])\n",
      "in:  tensor([7.2707])  out:  tensor([7.2695])\n",
      "in:  tensor([2.3491])  out:  tensor([2.3477])\n",
      "in:  tensor([-7.0078])  out:  tensor([-7.0078])\n",
      "in:  tensor([0.1231])  out:  tensor([0.1230])\n",
      "in:  tensor([-39.6268])  out:  tensor([-39.6250])\n",
      "in:  tensor([22.0204])  out:  tensor([22.0156])\n",
      "in:  tensor([31.4615])  out:  tensor([31.4531])\n",
      "in:  tensor([-47.1607])  out:  tensor([-47.1562])\n",
      "in:  tensor([19.5170])  out:  tensor([19.5156])\n",
      "in:  tensor([-3.5242])  out:  tensor([-3.5234])\n",
      "in:  tensor([-13.3923])  out:  tensor([-13.3906])\n",
      "in:  tensor([13.2115])  out:  tensor([13.2109])\n",
      "in:  tensor([-49.8987])  out:  tensor([-49.8750])\n",
      "in:  tensor([-15.8963])  out:  tensor([-15.8906])\n",
      "in:  tensor([28.7212])  out:  tensor([28.7188])\n",
      "in:  tensor([44.4628])  out:  tensor([44.4375])\n",
      "in:  tensor([44.3360])  out:  tensor([44.3125])\n",
      "in:  tensor([17.5361])  out:  tensor([17.5312])\n",
      "in:  tensor([25.4363])  out:  tensor([25.4219])\n",
      "in:  tensor([-27.3184])  out:  tensor([-27.3125])\n",
      "in:  tensor([-42.7203])  out:  tensor([-42.7188])\n",
      "in:  tensor([-34.7895])  out:  tensor([-34.7812])\n",
      "in:  tensor([-12.3139])  out:  tensor([-12.3125])\n",
      "in:  tensor([33.8479])  out:  tensor([33.8438])\n",
      "in:  tensor([17.0238])  out:  tensor([17.0156])\n",
      "in:  tensor([47.8557])  out:  tensor([47.8438])\n",
      "in:  tensor([-23.4477])  out:  tensor([-23.4375])\n",
      "in:  tensor([31.4713])  out:  tensor([31.4688])\n",
      "in:  tensor([35.2985])  out:  tensor([35.2812])\n",
      "in:  tensor([24.0537])  out:  tensor([24.0469])\n",
      "in:  tensor([8.6042])  out:  tensor([8.6016])\n",
      "in:  tensor([-49.5021])  out:  tensor([-49.5000])\n",
      "in:  tensor([-22.0351])  out:  tensor([-22.0312])\n",
      "in:  tensor([48.1022])  out:  tensor([48.0938])\n",
      "in:  tensor([29.7286])  out:  tensor([29.7188])\n",
      "in:  tensor([34.5601])  out:  tensor([34.5312])\n",
      "in:  tensor([-12.6751])  out:  tensor([-12.6719])\n",
      "in:  tensor([-6.0640])  out:  tensor([-6.0625])\n",
      "in:  tensor([0.1295])  out:  tensor([0.1289])\n",
      "in:  tensor([47.2864])  out:  tensor([47.2812])\n",
      "in:  tensor([19.1952])  out:  tensor([19.1875])\n",
      "in:  tensor([-4.0064])  out:  tensor([-4.0039])\n",
      "in:  tensor([-24.7423])  out:  tensor([-24.7344])\n",
      "in:  tensor([-6.6800])  out:  tensor([-6.6797])\n",
      "in:  tensor([19.7761])  out:  tensor([19.7656])\n",
      "in:  tensor([10.3666])  out:  tensor([10.3594])\n",
      "in:  tensor([44.2380])  out:  tensor([44.2188])\n",
      "in:  tensor([-8.3743])  out:  tensor([-8.3672])\n",
      "in:  tensor([-49.5177])  out:  tensor([-49.5000])\n",
      "in:  tensor([9.6218])  out:  tensor([9.6172])\n",
      "in:  tensor([12.3394])  out:  tensor([12.3359])\n",
      "in:  tensor([24.4463])  out:  tensor([24.4375])\n",
      "in:  tensor([37.1765])  out:  tensor([37.1562])\n",
      "in:  tensor([-14.6596])  out:  tensor([-14.6562])\n",
      "in:  tensor([-35.7685])  out:  tensor([-35.7500])\n",
      "in:  tensor([48.4208])  out:  tensor([48.4062])\n",
      "in:  tensor([-15.6973])  out:  tensor([-15.6953])\n",
      "in:  tensor([-42.7915])  out:  tensor([-42.7812])\n",
      "in:  tensor([-2.0746])  out:  tensor([-2.0742])\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    in_test = (torch.rand(1) - 0.5)*100.0\n",
    "    out_test = fake_float_truncate(in_test, 5, 10, 0)\n",
    "    print(\"in: \", in_test, \" out: \", out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_fixed_truncate(x: torch.Tensor, bits_int: int, scale_int: int, zero_point_int: int) -> torch.Tensor:\n",
    "    \n",
    "    qmin = 0\n",
    "    qmax = 2**bits_int - 1  # e.g. 0..15\n",
    "    \n",
    "    #from float to fixed point, and quantize accordingly\n",
    "    q_x = torch.clamp(torch.round(x * 2**(scale_int + bits_int//2) + 2**(bits_int-1) + zero_point_int), qmin, qmax)\n",
    "\n",
    "    # from quantized fixed point to float\n",
    "    fq_x = (q_x - 2**(bits_int-1) - zero_point_int) / 2**(scale_int + bits_int//2)\n",
    "        \n",
    "    return fq_x\n",
    "\n",
    "class FakeFixedFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, bits_param, scale_param, zero_point_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, bits_param, scale_param, zero_point_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        bits_int = int(torch.round(param_to_bit(bits_param)).item())\n",
    "        scale_int = int(torch.round(scale_param).item())\n",
    "        zero_point_int = int(torch.round(zero_point_param).item())\n",
    "\n",
    "        out = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int)\n",
    "        \n",
    "        #print(\"input\")\n",
    "        #print(x)\n",
    "        #print(\"output\")\n",
    "        #print(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, bits_param, scale_param, zero_point_param = ctx.saved_tensors\n",
    "        \n",
    "        bits = param_to_bit(bits_param)\n",
    "        scale = scale_param\n",
    "        zero_point = zero_point_param\n",
    "                \n",
    "        bits_int = int(torch.round(bits).item())\n",
    "        scale_int = int(torch.round(scale).item())\n",
    "        zero_point_int = int(torch.round(zero_point).item())\n",
    "\n",
    "        #print(\"shape x: \", x.shape)\n",
    "        #print(\"shape grad_output: \", grad_output.shape)\n",
    "\n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output\n",
    "        \n",
    "        # 1) Gradient wrt x: approximate with central difference\n",
    "        \"\"\"\n",
    "        grad_x = None\n",
    "        if True:\n",
    "            delta = 0.01            \n",
    "\n",
    "            f_plus2  = fake_float_truncate(x + 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_plus   = fake_float_truncate(x + 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus  = fake_float_truncate(x - 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus2 = fake_float_truncate(x - 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "            grad_x = grad_output * der\n",
    "        \"\"\"\n",
    "                \n",
    "        # 2) Gradient wrt bits: approximate with central difference\n",
    "        grad_bits = None\n",
    "        if bits_param.requires_grad:\n",
    "            if(bits_int < 2):\n",
    "                f_plus   = fake_fixed_truncate(x, bits_int + 1, scale_int, zero_point_int)\n",
    "                f_minus  = fake_fixed_truncate(x, bits_int    , scale_int, zero_point_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_fixed_truncate(x, bits_int + 2, scale_int, zero_point_int)\n",
    "                f_plus   = fake_fixed_truncate(x, bits_int + 1, scale_int, zero_point_int)\n",
    "                f_minus  = fake_fixed_truncate(x, bits_int - 1, scale_int, zero_point_int)\n",
    "                f_minus2 = fake_fixed_truncate(x, bits_int - 2, scale_int, zero_point_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_bits = grad_output * der * bits\n",
    "        \n",
    "        # 3) Gradient wrt scale: approximate with central difference\n",
    "        grad_scale_bits = None\n",
    "        if scale_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_fixed_truncate(x, bits_int, scale_int + 2, zero_point_int)\n",
    "            f_plus   = fake_fixed_truncate(x, bits_int, scale_int + 1, zero_point_int)\n",
    "            f_minus  = fake_fixed_truncate(x, bits_int, scale_int - 1, zero_point_int)\n",
    "            f_minus2 = fake_fixed_truncate(x, bits_int, scale_int - 2, zero_point_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_scale_bits = grad_output * der\n",
    "       \n",
    "        # 4) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_zero_point_bits = None\n",
    "        if zero_point_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int + 2)\n",
    "            f_plus   = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int + 1)\n",
    "            f_minus  = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int - 1)\n",
    "            f_minus2 = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int - 2)\n",
    "            \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0 \n",
    "            grad_zero_point_bits = grad_output * der\n",
    "             \n",
    "        return grad_x, grad_bits, grad_scale_bits, grad_zero_point_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:  tensor([-32.8451])  out  tensor([-32.8438])\n",
      "in:  tensor([35.3521])  out  tensor([35.3516])\n",
      "in:  tensor([23.6559])  out  tensor([23.6562])\n",
      "in:  tensor([-39.0730])  out  tensor([-39.0742])\n",
      "in:  tensor([17.7516])  out  tensor([17.7500])\n",
      "in:  tensor([31.0086])  out  tensor([31.0078])\n",
      "in:  tensor([-38.6704])  out  tensor([-38.6719])\n",
      "in:  tensor([-10.2348])  out  tensor([-10.2344])\n",
      "in:  tensor([6.7610])  out  tensor([6.7617])\n",
      "in:  tensor([47.8244])  out  tensor([47.8242])\n",
      "in:  tensor([-23.0607])  out  tensor([-23.0625])\n",
      "in:  tensor([3.5757])  out  tensor([3.5742])\n",
      "in:  tensor([14.5921])  out  tensor([14.5938])\n",
      "in:  tensor([8.8425])  out  tensor([8.8438])\n",
      "in:  tensor([21.0010])  out  tensor([21.])\n",
      "in:  tensor([33.9159])  out  tensor([33.9141])\n",
      "in:  tensor([-14.2071])  out  tensor([-14.2070])\n",
      "in:  tensor([12.5587])  out  tensor([12.5586])\n",
      "in:  tensor([8.6999])  out  tensor([8.6992])\n",
      "in:  tensor([-42.6482])  out  tensor([-42.6484])\n",
      "in:  tensor([24.8651])  out  tensor([24.8633])\n",
      "in:  tensor([32.0660])  out  tensor([32.0664])\n",
      "in:  tensor([-24.9735])  out  tensor([-24.9727])\n",
      "in:  tensor([-23.5718])  out  tensor([-23.5703])\n",
      "in:  tensor([3.4946])  out  tensor([3.4961])\n",
      "in:  tensor([45.8632])  out  tensor([45.8633])\n",
      "in:  tensor([40.5538])  out  tensor([40.5547])\n",
      "in:  tensor([-38.0319])  out  tensor([-38.0312])\n",
      "in:  tensor([8.9918])  out  tensor([8.9922])\n",
      "in:  tensor([35.6755])  out  tensor([35.6758])\n",
      "in:  tensor([-44.4305])  out  tensor([-44.4297])\n",
      "in:  tensor([38.8874])  out  tensor([38.8867])\n",
      "in:  tensor([31.4129])  out  tensor([31.4141])\n",
      "in:  tensor([42.6044])  out  tensor([42.6055])\n",
      "in:  tensor([-32.7622])  out  tensor([-32.7617])\n",
      "in:  tensor([22.8634])  out  tensor([22.8633])\n",
      "in:  tensor([-19.0106])  out  tensor([-19.0117])\n",
      "in:  tensor([-22.3184])  out  tensor([-22.3203])\n",
      "in:  tensor([36.8979])  out  tensor([36.8984])\n",
      "in:  tensor([9.9914])  out  tensor([9.9922])\n",
      "in:  tensor([-31.4985])  out  tensor([-31.5000])\n",
      "in:  tensor([-13.8494])  out  tensor([-13.8477])\n",
      "in:  tensor([-8.9161])  out  tensor([-8.9180])\n",
      "in:  tensor([-49.7589])  out  tensor([-49.7578])\n",
      "in:  tensor([46.8476])  out  tensor([46.8477])\n",
      "in:  tensor([-29.6027])  out  tensor([-29.6016])\n",
      "in:  tensor([-48.3209])  out  tensor([-48.3203])\n",
      "in:  tensor([38.1759])  out  tensor([38.1758])\n",
      "in:  tensor([43.4867])  out  tensor([43.4883])\n",
      "in:  tensor([-26.1928])  out  tensor([-26.1914])\n",
      "in:  tensor([31.9106])  out  tensor([31.9102])\n",
      "in:  tensor([-48.8773])  out  tensor([-48.8789])\n",
      "in:  tensor([34.1710])  out  tensor([34.1719])\n",
      "in:  tensor([-22.9764])  out  tensor([-22.9766])\n",
      "in:  tensor([15.8621])  out  tensor([15.8633])\n",
      "in:  tensor([22.0144])  out  tensor([22.0156])\n",
      "in:  tensor([-48.7930])  out  tensor([-48.7930])\n",
      "in:  tensor([43.5208])  out  tensor([43.5195])\n",
      "in:  tensor([-12.0642])  out  tensor([-12.0625])\n",
      "in:  tensor([32.6732])  out  tensor([32.6719])\n",
      "in:  tensor([48.9031])  out  tensor([48.9023])\n",
      "in:  tensor([7.2337])  out  tensor([7.2344])\n",
      "in:  tensor([-27.3486])  out  tensor([-27.3477])\n",
      "in:  tensor([-10.4357])  out  tensor([-10.4375])\n",
      "in:  tensor([-19.8377])  out  tensor([-19.8359])\n",
      "in:  tensor([-30.8261])  out  tensor([-30.8242])\n",
      "in:  tensor([-19.0320])  out  tensor([-19.0312])\n",
      "in:  tensor([30.5076])  out  tensor([30.5078])\n",
      "in:  tensor([-20.1491])  out  tensor([-20.1484])\n",
      "in:  tensor([-42.6162])  out  tensor([-42.6172])\n",
      "in:  tensor([39.7810])  out  tensor([39.7812])\n",
      "in:  tensor([3.7570])  out  tensor([3.7578])\n",
      "in:  tensor([25.5461])  out  tensor([25.5469])\n",
      "in:  tensor([-41.7426])  out  tensor([-41.7422])\n",
      "in:  tensor([-24.3534])  out  tensor([-24.3516])\n",
      "in:  tensor([-43.9288])  out  tensor([-43.9297])\n",
      "in:  tensor([-24.9201])  out  tensor([-24.9219])\n",
      "in:  tensor([17.6080])  out  tensor([17.6094])\n",
      "in:  tensor([39.2461])  out  tensor([39.2461])\n",
      "in:  tensor([-24.5519])  out  tensor([-24.5508])\n",
      "in:  tensor([46.2409])  out  tensor([46.2422])\n",
      "in:  tensor([-39.2198])  out  tensor([-39.2188])\n",
      "in:  tensor([-13.9983])  out  tensor([-14.])\n",
      "in:  tensor([40.2338])  out  tensor([40.2344])\n",
      "in:  tensor([-41.6160])  out  tensor([-41.6172])\n",
      "in:  tensor([31.6741])  out  tensor([31.6758])\n",
      "in:  tensor([-16.1270])  out  tensor([-16.1289])\n",
      "in:  tensor([-18.9299])  out  tensor([-18.9297])\n",
      "in:  tensor([18.9975])  out  tensor([18.9961])\n",
      "in:  tensor([-20.9384])  out  tensor([-20.9375])\n",
      "in:  tensor([30.0154])  out  tensor([30.0156])\n",
      "in:  tensor([49.4210])  out  tensor([49.4219])\n",
      "in:  tensor([22.7723])  out  tensor([22.7734])\n",
      "in:  tensor([24.1116])  out  tensor([24.1133])\n",
      "in:  tensor([-28.6351])  out  tensor([-28.6367])\n",
      "in:  tensor([-15.2622])  out  tensor([-15.2617])\n",
      "in:  tensor([-48.3267])  out  tensor([-48.3281])\n",
      "in:  tensor([20.8118])  out  tensor([20.8125])\n",
      "in:  tensor([-20.9543])  out  tensor([-20.9531])\n",
      "in:  tensor([-15.4365])  out  tensor([-15.4375])\n"
     ]
    }
   ],
   "source": [
    "bits_test = 16\n",
    "for i in range(100):\n",
    "    #in_test = torch.tensor(-15.25)\n",
    "    in_test = (torch.rand(1)-0.5)*100.0\n",
    "    out_test = fake_fixed_truncate(in_test, bits_test, 0, 0)\n",
    "    print(\"in: \", in_test, \" out \", out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### differentiable Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoundSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        return grad_output\n",
    "    \n",
    "class RoundFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.round(input + 2*delta)\n",
    "        f_plus   = torch.round(input + 1*delta)\n",
    "        f_minus  = torch.round(input - 1*delta)\n",
    "        f_minus2 = torch.round(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "class RoundSIG(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function that does a hard round in forward,\n",
    "    but uses a sigmoid-based approximation for the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, alpha=10.0):\n",
    "        \"\"\"\n",
    "        Forward pass: returns torch.round(input).\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.alpha = alpha\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: approximate the gradient of round(x)\n",
    "        with the derivative of a sigmoid centered at the fractional midpoint (0.5).\n",
    "        \"\"\"\n",
    "        (input,) = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "\n",
    "        # Fractional part\n",
    "        frac = input - torch.floor(input)\n",
    "\n",
    "        # Sigmoid of (fractional_part - 0.5), scaled by alpha\n",
    "        s = torch.sigmoid(alpha * (frac - 0.5))\n",
    "\n",
    "        # Derivative of sigmoid = alpha * s * (1 - s)\n",
    "        grad_input = alpha * s * (1 - s) * grad_output\n",
    "        return grad_input, None  # alpha is not a tensor that requires grad\n",
    "    \n",
    "def diff_round(x):\n",
    "    return RoundSTE.apply(x)\n",
    "    #return RoundFDE.apply(x)\n",
    "    #return RoundSIG.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable Floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloorSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass uses standard floor\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-through pass: just return the gradient as-is\n",
    "        return grad_output\n",
    "\n",
    "class FloorFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.floor(input + 2*delta)\n",
    "        f_plus   = torch.floor(input + 1*delta)\n",
    "        f_minus  = torch.floor(input - 1*delta)\n",
    "        f_minus2 = torch.floor(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "def diff_floor(input):\n",
    "    return FloorSTE.apply(input)\n",
    "    #return FloorFDE.apply(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxObserver(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We store running min/max\n",
    "        self.register_buffer(\"min_val\", torch.tensor(float(\"inf\")))\n",
    "        self.register_buffer(\"max_val\", torch.tensor(float(\"-inf\")))\n",
    "        # You could also store averaging stats, etc.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update running min/max\n",
    "        self.min_val = torch.min(self.min_val, x.detach().min())\n",
    "        self.max_val = torch.max(self.max_val, x.detach().max())\n",
    "        return x  # Just pass through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed point quanizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, observer, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.observer = observer\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        \n",
    "        # 1) Get min/max from observer\n",
    "        min_val = self.observer.min_val\n",
    "        max_val = self.observer.max_val\n",
    "\n",
    "        # If they're not valid, skip\n",
    "        #if min_val >= max_val:\n",
    "        #    return x\n",
    "\n",
    "        # 2) Compute scale and zero_point\n",
    "        # For an unsigned 4-bit range, we can hold values 0..15\n",
    "        # qmin, qmax = 0, (1 << b_int) - 1  # e.g. 0..15\n",
    "        qmin, qmax = torch.tensor(float(0)), 2**b_int - 1  # e.g. 0..15\n",
    "        \n",
    "        qmin = qmin.to(x.device)\n",
    "        #qmax = qmax.to(x.device)\n",
    "        max_val = max_val.to(x.device)\n",
    "        min_val = min_val.to(x.device)\n",
    "\n",
    "        # Typical formula for scale/zero-point:\n",
    "        scale = (max_val - min_val) / float(qmax - qmin)\n",
    "        zero_point = qmin - diff_round(min_val / scale)\n",
    "\n",
    "        # 3) Quantize (in floating point)\n",
    "        # clamp to range of [qmin, qmax]\n",
    "        q_x = torch.clamp(diff_round(x / scale + zero_point), qmin, qmax)\n",
    "\n",
    "        # 4) Dequantize back to float\n",
    "        fq_x = (q_x - zero_point) * scale\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"bits: \", self.bits.detach().item())\n",
    "        \n",
    "class FixedPointFakeQuantize2(nn.Module):\n",
    "    def __init__(self, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(bits//2)), requires_grad=requires_grad)\n",
    "        self.zero_point = nn.Parameter(torch.tensor(float(2**(bits//2-1)-1)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        bits_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        zero_point_int = diff_round(self.zero_point)\n",
    "        \n",
    "        qmin = torch.tensor(float(0)).to(x.device)\n",
    "        qmax = 2**bits_int - 1  # e.g. 0..15\n",
    "        \n",
    "        #from float to fixed point, and quantize accordingly\n",
    "        q_x = torch.clamp(diff_round(x * 2**scale_int + zero_point_int), qmin, qmax)\n",
    "\n",
    "        # from quantized fixed point to float\n",
    "        fq_x = (q_x - zero_point_int) / 2**scale_int\n",
    "        \n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"bits: \", self.bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())\n",
    "        print(\"zero point: \", self.zero_point.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatingPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, m_bits=23, e_bits=8, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.e_bits = nn.Parameter(torch.tensor(float(e_bits)), requires_grad=requires_grad)\n",
    "        self.m_bits = nn.Parameter(torch.tensor(float(m_bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(0)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e_bits_int = torch.clamp(diff_round(self.e_bits), 0, 32)\n",
    "        m_bits_int = torch.clamp(diff_round(self.m_bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        \n",
    "        sign = x.sign()\n",
    "        abs_x = x.abs().clamp(min=1e-45)\n",
    "\n",
    "        #recover the floatint point representation\n",
    "        #exponent \\in {-2**7,..,2**7-1}\n",
    "        #mantissa \\in {1.0,...,2.0}\n",
    "\n",
    "        exponent = diff_floor(torch.log2(abs_x)).clamp(min=1e-45)\n",
    "        mantissa = abs_x / (2**exponent)\n",
    "    \n",
    "        # truncate exponent\n",
    "        # lets parameterize the exponent as a constant value + a variable value\n",
    "        # the constant part is 2**7-1 in standar floating point, but we will learn it\n",
    "        # the variable part \\in {0,..,2**8-1}\n",
    "        # lets say exponent = v_exponent - 2**(bits-1)-1 + c_exponent\n",
    "        # so v_exponent = exponent + 2**(bits-1)-1 - c_exponent\n",
    "        c_exponent = scale_int\n",
    "        v_exponent = exponent + (2**(e_bits_int-1)-1) - c_exponent\n",
    "        \n",
    "        # the valriable part is clamped to the alloted bits\n",
    "        q_min = torch.tensor(float(0)).to(x.device)\n",
    "        q_max = 2**e_bits_int-1\n",
    "        q_exponent = torch.clamp(v_exponent, q_min, q_max) - (2**(e_bits_int-1)-1) + c_exponent\n",
    "    \n",
    "        # truncate mantissa\n",
    "        # this just removes the less significant bits\n",
    "        m_scale = 2.0 ** m_bits_int\n",
    "        q_mantissa = diff_floor(mantissa * m_scale) / m_scale\n",
    "    \n",
    "        # from quantized floatint point to float\n",
    "        fq_x = sign * (2**q_exponent) * q_mantissa\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.e_bits, self.m_bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"e_bits: \", self.e_bits.detach().item())\n",
    "        print(\"m_bits: \", self.m_bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float32 example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * input_size[1]//8 * input_size[2]//8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantWrapper(nn.Module):\n",
    "    def __init__(self, module, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.observer = MinMaxObserver()\n",
    "        self.fake_quant_input = FixedPointFakeQuantize(self.observer, requires_grad=optimizeQuant)\n",
    "        self.fake_quant_weight = FixedPointFakeQuantize(self.observer, requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_input = FixedPointFakeQuantize2(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_weight = FixedPointFakeQuantize2(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_input = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_weight = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.observer(x)\n",
    "        x = self.fake_quant_input(x)\n",
    "        w = self.fake_quant_weight(self.module.weight)\n",
    "        b = self.module.bias\n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            return F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            return F.linear(x, w, b)\n",
    "        else:\n",
    "            return self.module(x)\n",
    "        \n",
    "    def getBits(self):\n",
    "        return self.fake_quant_input.getBits() + self.fake_quant_weight.getBits()\n",
    "        #return self.fake_quant_weight.getBits()\n",
    "    \n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        self.fake_quant_input.printParams()\n",
    "        print(\"weight quant params: \")\n",
    "        self.fake_quant_weight.printParams()\n",
    "\n",
    "class QuantWrapperFloatingPoint(nn.Module):\n",
    "    def __init__(self, module, e_bits=5, m_bits=10, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "        self.input_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        self.input_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        self.input_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "        self.weight_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = FakeFloatFunction.apply(x,   self.input_e_bits_param, self.input_m_bits_param, self.input_scale)\n",
    "        w = FakeFloatFunction.apply(self.module.weight, self.weight_e_bits_param, self.weight_m_bits_param, self.weight_scale)\n",
    "        \n",
    "        b = self.module.bias\n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            return F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            return F.linear(x, w, b)\n",
    "        else:\n",
    "            return self.module(x)\n",
    "        \n",
    "    def getBits(self):\n",
    "        return [param_to_bit(self.input_e_bits_param), param_to_bit(self.input_m_bits_param), param_to_bit(self.weight_e_bits_param), param_to_bit(self.weight_m_bits_param)]\n",
    "\n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        print(\"e bits: \", param_to_bit(self.input_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.input_m_bits_param).detach().item(), \" scale \", self.input_scale.detach().item())\n",
    "        print(\"weight quant params: \")\n",
    "        print(\"e bits \", param_to_bit(self.weight_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.weight_m_bits_param).detach().item(), \" scale \", self.weight_scale.detach().item())\n",
    "\n",
    "class QuantWrapperFixedPoint(nn.Module):\n",
    "    def __init__(self, module, bits=32, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "        self.input_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        self.input_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.input_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.weight_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.weight_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = FakeFixedFunction.apply(x, self.input_bits_param, self.input_scale, self.input_zero_point)\n",
    "        w = FakeFixedFunction.apply(self.module.weight, self.weight_bits_param, self.weight_scale, self.weight_zero_point)\n",
    "        \n",
    "        b = self.module.bias\n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            return F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            return F.linear(x, w, b)\n",
    "        else:\n",
    "            return self.module(x)\n",
    "        \n",
    "    def getBits(self):\n",
    "        return [param_to_bit(self.input_bits_param), param_to_bit(self.weight_bits_param)]\n",
    "\n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.input_bits_param).detach().item(), \" scale \", self.input_scale.detach().item(), \" zero point \", self.input_zero_point.detach().item())\n",
    "        print(\"weight quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.weight_bits_param).detach().item(), \" scale \", self.weight_scale.detach().item(), \" zero point \", self.weight_zero_point.detach().item())\n",
    "\n",
    "class QuantSimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = QuantWrapperFloatingPoint(nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = QuantWrapperFloatingPoint(nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        #self.conv3 = QuantWrapperFixedPoint(nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        #self.bn3 = nn.BatchNorm2d(64)\n",
    "        #self.conv4 = QuantWrapperFixedPoint(nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        #self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        #self.conv5 = QuantWrapperFixedPoint(nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        #self.bn5 = nn.BatchNorm2d(128)\n",
    "        #self.conv6 = QuantWrapperFixedPoint(nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        #self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = QuantWrapperFloatingPoint(nn.Linear(64 * input_size[1]//4 * input_size[2]//4, 128), optimizeQuant=optimizeQuant)\n",
    "        #self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = QuantWrapperFloatingPoint(nn.Linear(128, num_classes), optimizeQuant=optimizeQuant)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #x = self.conv3(x)\n",
    "        #x = self.bn3(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.conv4(x)\n",
    "        #x = self.bn4(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        #x = self.conv5(x)\n",
    "        #x = self.bn5(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.conv6(x)\n",
    "        #x = self.bn6(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwidth_squared(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += bit ** 2\n",
    "                c += 1\n",
    "    return s/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBitWidths(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            print(\"module: \", name)\n",
    "            module.printQuantParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, lambda_bw=1e-3):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_ce = F.cross_entropy(output, target)\n",
    "        penalty_bw = bitwidth_squared(model) \n",
    "        #penalty_bw = bitwidth_sum(model) \n",
    "        loss = loss_ce + lambda_bw*penalty_bw\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #if batch_idx % 200 == 0:\n",
    "        #    print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "        #          f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Train set: Average loss: {train_loss:.4f}\")\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy} ({100.0*accuracy:.2f}%)\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using divice  cpu\n",
      "Train set: Average loss: 0.5863\n",
      "Test set: Average loss: 0.7386, Accuracy: 0.7533 (75.33%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  3.5783753395080566  m bits  7.1226959228515625  scale  0.0\n",
      "weight quant params: \n",
      "e bits  3.5783753395080566  m bits  8.381473541259766  scale  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.616895914077759  m bits  7.1163177490234375  scale  0.0\n",
      "weight quant params: \n",
      "e bits  3.5783753395080566  m bits  7.519367694854736  scale  0.0\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  3.651777982711792  m bits  7.070778846740723  scale  0.0\n",
      "weight quant params: \n",
      "e bits  3.5783753395080566  m bits  8.143450736999512  scale  0.0\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.721526861190796  m bits  7.178051471710205  scale  0.0\n",
      "weight quant params: \n",
      "e bits  3.5783753395080566  m bits  8.15917682647705  scale  0.0\n",
      "Train set: Average loss: 0.5407\n",
      "Test set: Average loss: 0.7627, Accuracy: 0.7508 (75.08%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  2.782980442047119  m bits  5.696863174438477  scale  0.0\n",
      "weight quant params: \n",
      "e bits  2.782980442047119  m bits  7.752338886260986  scale  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.500622034072876  m bits  5.498461723327637  scale  0.02099478803575039\n",
      "weight quant params: \n",
      "e bits  2.782980442047119  m bits  7.1617536544799805  scale  0.0\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  3.019407272338867  m bits  5.193502902984619  scale  0.060207754373550415\n",
      "weight quant params: \n",
      "e bits  2.782980442047119  m bits  7.451269626617432  scale  0.0\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.508538007736206  m bits  5.6700119972229  scale  -0.011971081607043743\n",
      "weight quant params: \n",
      "e bits  2.782980442047119  m bits  7.499476432800293  scale  0.0\n",
      "Train set: Average loss: 0.5113\n",
      "Test set: Average loss: 0.7467, Accuracy: 0.7481 (74.81%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  2.3433403968811035  m bits  5.307369232177734  scale  0.0020068036392331123\n",
      "weight quant params: \n",
      "e bits  2.2691729068756104  m bits  7.494980812072754  scale  0.021381976082921028\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.4957070350646973  m bits  4.422703266143799  scale  0.04455141723155975\n",
      "weight quant params: \n",
      "e bits  2.2691822052001953  m bits  7.00660514831543  scale  0.062390897423028946\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.761471748352051  m bits  3.9038543701171875  scale  0.09847133606672287\n",
      "weight quant params: \n",
      "e bits  2.4683144092559814  m bits  7.411961555480957  scale  -0.07550785690546036\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.51096773147583  m bits  4.924757957458496  scale  -0.001215342665091157\n",
      "weight quant params: \n",
      "e bits  2.5023584365844727  m bits  7.589186191558838  scale  -0.0986051931977272\n",
      "Train set: Average loss: 0.4865\n",
      "Test set: Average loss: 0.7419, Accuracy: 0.7592 (75.92%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  2.1680922508239746  m bits  4.946786403656006  scale  0.008884217590093613\n",
      "weight quant params: \n",
      "e bits  1.9062528610229492  m bits  7.5326080322265625  scale  0.029383745044469833\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.47098708152771  m bits  3.8874731063842773  scale  0.12074294686317444\n",
      "weight quant params: \n",
      "e bits  1.9062626361846924  m bits  6.655515670776367  scale  0.10318901389837265\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5129635334014893  m bits  3.1428234577178955  scale  0.16716234385967255\n",
      "weight quant params: \n",
      "e bits  2.502046585083008  m bits  7.484059810638428  scale  -0.2074013650417328\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.501955509185791  m bits  4.5214033126831055  scale  -0.002454096218571067\n",
      "weight quant params: \n",
      "e bits  2.514183759689331  m bits  7.459266662597656  scale  -0.24195849895477295\n",
      "Train set: Average loss: 0.4617\n",
      "Test set: Average loss: 0.7464, Accuracy: 0.7585 (75.85%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.9793674945831299  m bits  4.782282829284668  scale  0.053388215601444244\n",
      "weight quant params: \n",
      "e bits  1.6339930295944214  m bits  7.493808746337891  scale  0.038523126393556595\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.4445552825927734  m bits  3.492856979370117  scale  0.17824478447437286\n",
      "weight quant params: \n",
      "e bits  1.6340097188949585  m bits  6.5060882568359375  scale  0.13251888751983643\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.39170503616333  m bits  2.656322479248047  scale  0.05372597649693489\n",
      "weight quant params: \n",
      "e bits  2.4946134090423584  m bits  7.469558238983154  scale  -0.3093067407608032\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.4998390674591064  m bits  4.4392523765563965  scale  -0.009434591047465801\n",
      "weight quant params: \n",
      "e bits  2.4905030727386475  m bits  7.364793300628662  scale  -0.33914297819137573\n",
      "Train set: Average loss: 0.4391\n",
      "Test set: Average loss: 0.7934, Accuracy: 0.7491 (74.91%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.8611239194869995  m bits  4.601219654083252  scale  0.07533814013004303\n",
      "weight quant params: \n",
      "e bits  1.4208049774169922  m bits  7.665063858032227  scale  0.03214927762746811\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.485175371170044  m bits  3.495338201522827  scale  0.1941944658756256\n",
      "weight quant params: \n",
      "e bits  1.4208099842071533  m bits  6.500088691711426  scale  0.1557527333498001\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.3210980892181396  m bits  2.421013355255127  scale  -0.06507180631160736\n",
      "weight quant params: \n",
      "e bits  2.5145721435546875  m bits  7.504878044128418  scale  -0.4095475673675537\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.4917349815368652  m bits  4.437317848205566  scale  0.009892582893371582\n",
      "weight quant params: \n",
      "e bits  2.510519504547119  m bits  7.464175224304199  scale  -0.4421273171901703\n",
      "Train set: Average loss: 0.4213\n",
      "Test set: Average loss: 0.8120, Accuracy: 0.7496 (74.96%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.7953592538833618  m bits  4.591312885284424  scale  0.07489832490682602\n",
      "weight quant params: \n",
      "e bits  1.2484393119812012  m bits  7.647919178009033  scale  0.045710742473602295\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.493975877761841  m bits  3.4445993900299072  scale  0.22250622510910034\n",
      "weight quant params: \n",
      "e bits  1.2484418153762817  m bits  6.514358043670654  scale  0.16416765749454498\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.2062859535217285  m bits  2.310814380645752  scale  -0.18410438299179077\n",
      "weight quant params: \n",
      "e bits  2.495706796646118  m bits  7.5026631355285645  scale  -0.49119558930397034\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5014607906341553  m bits  4.492702007293701  scale  0.017351293936371803\n",
      "weight quant params: \n",
      "e bits  2.314476728439331  m bits  7.5024003982543945  scale  -0.5375685095787048\n",
      "Train set: Average loss: 0.4029\n",
      "Test set: Average loss: 0.7881, Accuracy: 0.7581 (75.81%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.712503433227539  m bits  4.59296989440918  scale  0.09064573794603348\n",
      "weight quant params: \n",
      "e bits  1.1056530475616455  m bits  7.6380934715271  scale  0.04961468651890755\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5007638931274414  m bits  3.486142635345459  scale  0.23344968259334564\n",
      "weight quant params: \n",
      "e bits  1.1056538820266724  m bits  6.497149467468262  scale  0.14090260863304138\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.096320629119873  m bits  2.244209051132202  scale  -0.29374468326568604\n",
      "weight quant params: \n",
      "e bits  2.0332071781158447  m bits  7.495166778564453  scale  -0.6204651594161987\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.501399517059326  m bits  4.429141998291016  scale  0.024027816951274872\n",
      "weight quant params: \n",
      "e bits  1.8489207029342651  m bits  7.2702860832214355  scale  -0.6862571239471436\n",
      "Train set: Average loss: 0.3858\n",
      "Test set: Average loss: 0.7991, Accuracy: 0.755 (75.50%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.6844960451126099  m bits  4.577587127685547  scale  0.07324276119470596\n",
      "weight quant params: \n",
      "e bits  0.9851117730140686  m bits  8.069796562194824  scale  0.033795326948165894\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.4855754375457764  m bits  3.453874349594116  scale  0.28276997804641724\n",
      "weight quant params: \n",
      "e bits  0.9851120114326477  m bits  6.597024440765381  scale  0.12988896667957306\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.025655508041382  m bits  2.1579031944274902  scale  -0.3662548363208771\n",
      "weight quant params: \n",
      "e bits  1.69584059715271  m bits  7.472992897033691  scale  -0.7593762278556824\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.506913185119629  m bits  4.4037861824035645  scale  0.05047913268208504\n",
      "weight quant params: \n",
      "e bits  1.5582523345947266  m bits  7.260105133056641  scale  -0.8322377800941467\n",
      "Train set: Average loss: 0.3708\n",
      "Test set: Average loss: 0.8198, Accuracy: 0.7524 (75.24%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.6307371854782104  m bits  4.505466938018799  scale  0.07508181035518646\n",
      "weight quant params: \n",
      "e bits  0.88182133436203  m bits  7.875635623931885  scale  0.031877364963293076\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5004196166992188  m bits  3.3997833728790283  scale  0.30910614132881165\n",
      "weight quant params: \n",
      "e bits  0.8818210959434509  m bits  6.4450483322143555  scale  0.12698008120059967\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  1.9850115776062012  m bits  2.08600115776062  scale  -0.4403799772262573\n",
      "weight quant params: \n",
      "e bits  1.4781701564788818  m bits  7.387012958526611  scale  -0.7735818028450012\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5000836849212646  m bits  4.427122592926025  scale  0.05533413216471672\n",
      "weight quant params: \n",
      "e bits  1.4472965002059937  m bits  7.392037391662598  scale  -0.573725163936615\n",
      "Train set: Average loss: 0.3504\n",
      "Test set: Average loss: 0.8492, Accuracy: 0.7495 (74.95%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.580706238746643  m bits  4.5124664306640625  scale  0.07846511900424957\n",
      "weight quant params: \n",
      "e bits  0.7922558784484863  m bits  7.511691570281982  scale  0.045601844787597656\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.4727261066436768  m bits  3.4003829956054688  scale  0.36801621317863464\n",
      "weight quant params: \n",
      "e bits  0.7922554016113281  m bits  6.527313232421875  scale  0.11855689436197281\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  1.979598879814148  m bits  2.070629596710205  scale  -0.49979373812675476\n",
      "weight quant params: \n",
      "e bits  1.4241265058517456  m bits  7.295027732849121  scale  -0.5024678111076355\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.502462863922119  m bits  4.43641996383667  scale  0.07536757737398148\n",
      "weight quant params: \n",
      "e bits  1.388518214225769  m bits  7.497526168823242  scale  -0.2755712568759918\n",
      "Train set: Average loss: 0.3375\n",
      "Test set: Average loss: 0.8579, Accuracy: 0.7579 (75.79%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5370410680770874  m bits  4.51975679397583  scale  0.0795983374118805\n",
      "weight quant params: \n",
      "e bits  0.7138493657112122  m bits  7.448877334594727  scale  0.0351455956697464\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.485692262649536  m bits  3.3900530338287354  scale  0.37591493129730225\n",
      "weight quant params: \n",
      "e bits  0.7138486504554749  m bits  6.525387763977051  scale  0.10004673898220062\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.0908477306365967  m bits  2.0243184566497803  scale  -0.49685290455818176\n",
      "weight quant params: \n",
      "e bits  1.3657152652740479  m bits  7.026270389556885  scale  -0.2035430371761322\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5123813152313232  m bits  4.439198970794678  scale  0.03994336724281311\n",
      "weight quant params: \n",
      "e bits  1.3258247375488281  m bits  7.520872116088867  scale  -0.05718129873275757\n",
      "Train set: Average loss: 0.3203\n",
      "Test set: Average loss: 0.8401, Accuracy: 0.7593 (75.93%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5025620460510254  m bits  4.539928436279297  scale  0.07491527497768402\n",
      "weight quant params: \n",
      "e bits  0.644683301448822  m bits  7.505032062530518  scale  0.04518655315041542\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.487044095993042  m bits  3.432109832763672  scale  0.35194164514541626\n",
      "weight quant params: \n",
      "e bits  0.6446825861930847  m bits  6.502109050750732  scale  0.12411973625421524\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.205190896987915  m bits  1.9968948364257812  scale  -0.49992018938064575\n",
      "weight quant params: \n",
      "e bits  1.3035458326339722  m bits  6.923034191131592  scale  0.027326436713337898\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5024313926696777  m bits  4.497443199157715  scale  0.06716108322143555\n",
      "weight quant params: \n",
      "e bits  1.2599931955337524  m bits  7.227410793304443  scale  0.1459987610578537\n",
      "Train set: Average loss: 0.3061\n",
      "Test set: Average loss: 0.8496, Accuracy: 0.7577 (75.77%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.4626798629760742  m bits  4.52810525894165  scale  0.12171918898820877\n",
      "weight quant params: \n",
      "e bits  0.5832896828651428  m bits  7.488163471221924  scale  0.035701051354408264\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.473726511001587  m bits  3.4167897701263428  scale  0.36479079723358154\n",
      "weight quant params: \n",
      "e bits  0.5832889080047607  m bits  6.436477184295654  scale  0.11425192654132843\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.3110949993133545  m bits  1.9633055925369263  scale  -0.499880850315094\n",
      "weight quant params: \n",
      "e bits  1.2383787631988525  m bits  6.90004301071167  scale  0.23493799567222595\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.500234603881836  m bits  4.500524997711182  scale  0.08680631220340729\n",
      "weight quant params: \n",
      "e bits  1.1919113397598267  m bits  7.038735389709473  scale  0.33812445402145386\n",
      "Train set: Average loss: 0.2958\n",
      "Test set: Average loss: 0.8596, Accuracy: 0.7633 (76.33%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.418305516242981  m bits  4.492369174957275  scale  0.08020275086164474\n",
      "weight quant params: \n",
      "e bits  0.528520941734314  m bits  7.396792411804199  scale  0.03628377988934517\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.50093412399292  m bits  3.3961713314056396  scale  0.3515334725379944\n",
      "weight quant params: \n",
      "e bits  0.5285202264785767  m bits  6.5226616859436035  scale  0.11118614673614502\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.4179470539093018  m bits  1.9396591186523438  scale  -0.49878260493278503\n",
      "weight quant params: \n",
      "e bits  1.1710822582244873  m bits  6.793728351593018  scale  0.41760072112083435\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.4552419185638428  m bits  4.484732151031494  scale  0.1072920635342598\n",
      "weight quant params: \n",
      "e bits  1.222719430923462  m bits  6.99940824508667  scale  0.49657201766967773\n",
      "Train set: Average loss: 0.2933\n",
      "Test set: Average loss: 0.8604, Accuracy: 0.7594 (75.94%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.3694531917572021  m bits  4.520508766174316  scale  0.05451062694191933\n",
      "weight quant params: \n",
      "e bits  0.47946417331695557  m bits  7.569879055023193  scale  0.03261670470237732\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.37953782081604  m bits  3.381181240081787  scale  0.42268791794776917\n",
      "weight quant params: \n",
      "e bits  0.47946426272392273  m bits  6.521264553070068  scale  0.07406343519687653\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.4806833267211914  m bits  1.9245859384536743  scale  -0.49345695972442627\n",
      "weight quant params: \n",
      "e bits  1.1894232034683228  m bits  6.535330295562744  scale  0.48516276478767395\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.4899182319641113  m bits  4.429427623748779  scale  0.12229233235120773\n",
      "weight quant params: \n",
      "e bits  1.5088611841201782  m bits  6.8405280113220215  scale  0.49485716223716736\n",
      "Train set: Average loss: 0.2730\n",
      "Test set: Average loss: 1.4464, Accuracy: 0.7587 (75.87%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.3163996934890747  m bits  4.511603355407715  scale  0.07542581111192703\n",
      "weight quant params: \n",
      "e bits  0.4353589713573456  m bits  7.563528537750244  scale  0.03443920612335205\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.467289447784424  m bits  3.4133598804473877  scale  0.44132786989212036\n",
      "weight quant params: \n",
      "e bits  0.43537867069244385  m bits  6.5283331871032715  scale  0.05837157741189003\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.486225128173828  m bits  1.9340882301330566  scale  -0.49228206276893616\n",
      "weight quant params: \n",
      "e bits  1.489031434059143  m bits  6.500816822052002  scale  0.5000588297843933\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5294735431671143  m bits  4.366722583770752  scale  0.12758967280387878\n",
      "weight quant params: \n",
      "e bits  1.5245790481567383  m bits  6.523200511932373  scale  0.47651907801628113\n",
      "Train set: Average loss: 0.2481\n",
      "Test set: Average loss: 0.9032, Accuracy: 0.7646 (76.46%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.2595921754837036  m bits  4.50671911239624  scale  0.06758054345846176\n",
      "weight quant params: \n",
      "e bits  0.39566582441329956  m bits  7.66347599029541  scale  0.02749600261449814\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.4455718994140625  m bits  3.4910356998443604  scale  0.49661514163017273\n",
      "weight quant params: \n",
      "e bits  0.3956419825553894  m bits  6.5545172691345215  scale  0.08566407859325409\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5085091590881348  m bits  2.056241989135742  scale  -0.49749019742012024\n",
      "weight quant params: \n",
      "e bits  1.5483684539794922  m bits  6.705158710479736  scale  0.4656706750392914\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5181822776794434  m bits  4.487109661102295  scale  0.12942224740982056\n",
      "weight quant params: \n",
      "e bits  1.5741603374481201  m bits  6.584113121032715  scale  0.43171730637550354\n",
      "Train set: Average loss: 0.2385\n",
      "Test set: Average loss: 0.9592, Accuracy: 0.7557 (75.57%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.1996411085128784  m bits  4.4826860427856445  scale  0.06545791029930115\n",
      "weight quant params: \n",
      "e bits  0.3598673939704895  m bits  8.034276008605957  scale  0.012139799073338509\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.503904342651367  m bits  3.467222213745117  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.35976648330688477  m bits  6.5119500160217285  scale  0.08149053156375885\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5206167697906494  m bits  2.173487901687622  scale  -0.4937552213668823\n",
      "weight quant params: \n",
      "e bits  1.5865354537963867  m bits  6.554396152496338  scale  0.4318934381008148\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.50563645362854  m bits  4.496795177459717  scale  0.12690754234790802\n",
      "weight quant params: \n",
      "e bits  1.6267602443695068  m bits  6.599107265472412  scale  0.38539034128189087\n",
      "Train set: Average loss: 0.2292\n",
      "Test set: Average loss: 0.9540, Accuracy: 0.7523 (75.23%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.1372864246368408  m bits  4.527052402496338  scale  0.09772574156522751\n",
      "weight quant params: \n",
      "e bits  0.3273853659629822  m bits  8.450983047485352  scale  0.013244381174445152\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5135228633880615  m bits  3.5046091079711914  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.3273070752620697  m bits  6.49062442779541  scale  0.08312457799911499\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.4980061054229736  m bits  2.2115976810455322  scale  -0.48002710938453674\n",
      "weight quant params: \n",
      "e bits  1.634645700454712  m bits  6.57078218460083  scale  0.39131784439086914\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5145111083984375  m bits  4.435075759887695  scale  0.1713520884513855\n",
      "weight quant params: \n",
      "e bits  1.693379282951355  m bits  6.56480598449707  scale  0.3303532898426056\n",
      "Train set: Average loss: 0.2301\n",
      "Test set: Average loss: 0.9721, Accuracy: 0.7511 (75.11%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.073351263999939  m bits  4.508086204528809  scale  0.11117014288902283\n",
      "weight quant params: \n",
      "e bits  0.2979952096939087  m bits  8.518315315246582  scale  0.00704253651201725\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.503584861755371  m bits  3.5007174015045166  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.29790356755256653  m bits  6.487468719482422  scale  0.06761464476585388\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.50189471244812  m bits  2.2603299617767334  scale  -0.4892127215862274\n",
      "weight quant params: \n",
      "e bits  1.6792778968811035  m bits  6.516293525695801  scale  0.3529297113418579\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.510124444961548  m bits  4.4557414054870605  scale  0.2136375606060028\n",
      "weight quant params: \n",
      "e bits  1.754055142402649  m bits  6.498753547668457  scale  0.27939581871032715\n",
      "Train set: Average loss: 0.2126\n",
      "Test set: Average loss: 1.0004, Accuracy: 0.7588 (75.88%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.008695125579834  m bits  4.507572650909424  scale  0.11743034422397614\n",
      "weight quant params: \n",
      "e bits  0.271322637796402  m bits  8.130221366882324  scale  0.00243760016746819\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5092885494232178  m bits  3.527240753173828  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.2712160646915436  m bits  6.506293773651123  scale  0.1086755022406578\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.514655113220215  m bits  2.398841381072998  scale  -0.49665185809135437\n",
      "weight quant params: \n",
      "e bits  1.75334894657135  m bits  6.535059452056885  scale  0.29657694697380066\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5001978874206543  m bits  4.465841293334961  scale  0.21027106046676636\n",
      "weight quant params: \n",
      "e bits  1.8568905591964722  m bits  6.543128967285156  scale  0.20506171882152557\n",
      "Train set: Average loss: 0.2147\n",
      "Test set: Average loss: 1.0308, Accuracy: 0.7525 (75.25%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.9441589117050171  m bits  4.508438587188721  scale  0.12941458821296692\n",
      "weight quant params: \n",
      "e bits  0.24704287946224213  m bits  8.524467468261719  scale  0.009633068926632404\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.50589656829834  m bits  3.502902030944824  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.24699947237968445  m bits  6.504920959472656  scale  0.10482894629240036\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5151467323303223  m bits  2.489488124847412  scale  -0.49050772190093994\n",
      "weight quant params: \n",
      "e bits  1.8048770427703857  m bits  6.540094375610352  scale  0.2528667151927948\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5091679096221924  m bits  4.4112701416015625  scale  0.20721010863780975\n",
      "weight quant params: \n",
      "e bits  1.9242193698883057  m bits  6.541483402252197  scale  0.14922896027565002\n",
      "Train set: Average loss: 0.1991\n",
      "Test set: Average loss: 1.0205, Accuracy: 0.7537 (75.37%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.8805202841758728  m bits  4.52269172668457  scale  0.1454077959060669\n",
      "weight quant params: \n",
      "e bits  0.2250765711069107  m bits  8.416292190551758  scale  0.00024492392549291253\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.503340005874634  m bits  3.4976806640625  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.22500163316726685  m bits  6.561856269836426  scale  0.07439491152763367\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.4871132373809814  m bits  2.4835479259490967  scale  -0.4924374222755432\n",
      "weight quant params: \n",
      "e bits  1.89263117313385  m bits  6.5319952964782715  scale  0.18917958438396454\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5040125846862793  m bits  4.469688415527344  scale  0.19725266098976135\n",
      "weight quant params: \n",
      "e bits  2.0426671504974365  m bits  6.641285419464111  scale  0.06814098358154297\n",
      "Train set: Average loss: 0.1957\n",
      "Test set: Average loss: 1.1165, Accuracy: 0.747 (74.70%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.8184589147567749  m bits  4.5044074058532715  scale  0.16706378757953644\n",
      "weight quant params: \n",
      "e bits  0.2050812691450119  m bits  7.813087463378906  scale  -0.003974024206399918\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.507927894592285  m bits  3.5077919960021973  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.20498709380626678  m bits  6.5508928298950195  scale  0.08887096494436264\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.514488458633423  m bits  2.5012197494506836  scale  -0.47900158166885376\n",
      "weight quant params: \n",
      "e bits  1.9814425706863403  m bits  6.545489311218262  scale  0.12449240684509277\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5062201023101807  m bits  4.493751525878906  scale  0.22102726995944977\n",
      "weight quant params: \n",
      "e bits  2.158233404159546  m bits  6.504514694213867  scale  -0.012397297658026218\n",
      "Train set: Average loss: 0.1853\n",
      "Test set: Average loss: 1.0820, Accuracy: 0.7567 (75.67%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.7585363388061523  m bits  4.518003463745117  scale  0.17608939111232758\n",
      "weight quant params: \n",
      "e bits  0.1869535595178604  m bits  7.7671709060668945  scale  -0.021950798109173775\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5018177032470703  m bits  3.500074863433838  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.18678918480873108  m bits  6.523907661437988  scale  0.07398419082164764\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.4624524116516113  m bits  2.5021231174468994  scale  -0.49350956082344055\n",
      "weight quant params: \n",
      "e bits  2.1028709411621094  m bits  6.506791591644287  scale  0.04450321942567825\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.50041127204895  m bits  4.387969493865967  scale  0.24144604802131653\n",
      "weight quant params: \n",
      "e bits  2.310485363006592  m bits  6.552205562591553  scale  -0.10887064039707184\n",
      "Train set: Average loss: 0.1702\n",
      "Test set: Average loss: 1.1227, Accuracy: 0.7566 (75.66%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.7011877298355103  m bits  4.510879039764404  scale  0.1793830394744873\n",
      "weight quant params: \n",
      "e bits  0.17034287750720978  m bits  7.4932122230529785  scale  -0.019655121490359306\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.504220962524414  m bits  3.503026008605957  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.17022134363651276  m bits  6.503335475921631  scale  0.08200311660766602\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5065853595733643  m bits  2.5015268325805664  scale  -0.4918297827243805\n",
      "weight quant params: \n",
      "e bits  2.3208844661712646  m bits  6.652048110961914  scale  -0.07125244289636612\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.506722927093506  m bits  4.422755241394043  scale  0.2784760594367981\n",
      "weight quant params: \n",
      "e bits  2.4985897541046143  m bits  6.59156608581543  scale  -0.22010409832000732\n",
      "Train set: Average loss: 0.1696\n",
      "Test set: Average loss: 1.1095, Accuracy: 0.7568 (75.68%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.6467249989509583  m bits  4.525148868560791  scale  0.17502303421497345\n",
      "weight quant params: \n",
      "e bits  0.15528209507465363  m bits  7.735687255859375  scale  -0.02772824466228485\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.504258155822754  m bits  3.5163228511810303  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.15514621138572693  m bits  6.506905555725098  scale  0.06383249908685684\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5298001766204834  m bits  2.5044119358062744  scale  -0.4869568943977356\n",
      "weight quant params: \n",
      "e bits  2.5053517818450928  m bits  6.480955600738525  scale  -0.17429767549037933\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.476270914077759  m bits  4.436159133911133  scale  0.3218141794204712\n",
      "weight quant params: \n",
      "e bits  2.5044548511505127  m bits  6.529374122619629  scale  -0.2812192440032959\n",
      "Train set: Average loss: 0.1600\n",
      "Test set: Average loss: 1.1205, Accuracy: 0.7542 (75.42%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5953471660614014  m bits  4.515040874481201  scale  0.1643415093421936\n",
      "weight quant params: \n",
      "e bits  0.14160673320293427  m bits  7.462245941162109  scale  -0.04494650289416313\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.50608491897583  m bits  3.5102121829986572  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.14140662550926208  m bits  6.502235412597656  scale  0.08823561668395996\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5012717247009277  m bits  2.500363826751709  scale  -0.4825519025325775\n",
      "weight quant params: \n",
      "e bits  2.500518321990967  m bits  6.490769863128662  scale  -0.2269265353679657\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.4894680976867676  m bits  4.45996618270874  scale  0.37799251079559326\n",
      "weight quant params: \n",
      "e bits  2.5003855228424072  m bits  6.532144546508789  scale  -0.3480408787727356\n",
      "Train set: Average loss: 0.1597\n",
      "Test set: Average loss: 1.1534, Accuracy: 0.7581 (75.81%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5471557378768921  m bits  4.511319160461426  scale  0.17274066805839539\n",
      "weight quant params: \n",
      "e bits  0.12905949354171753  m bits  7.502477169036865  scale  -0.043727897107601166\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.501530647277832  m bits  3.509866952896118  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.12890134751796722  m bits  6.52634859085083  scale  0.08305203914642334\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5193850994110107  m bits  2.501917600631714  scale  -0.4926307797431946\n",
      "weight quant params: \n",
      "e bits  2.5003483295440674  m bits  6.498929500579834  scale  -0.2897983491420746\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.501187801361084  m bits  4.4270548820495605  scale  0.398271769285202\n",
      "weight quant params: \n",
      "e bits  2.5009307861328125  m bits  6.51990270614624  scale  -0.425996333360672\n",
      "Train set: Average loss: 0.1542\n",
      "Test set: Average loss: 1.2102, Accuracy: 0.7531 (75.31%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5021719336509705  m bits  4.504087924957275  scale  0.1549585610628128\n",
      "weight quant params: \n",
      "e bits  0.11766742914915085  m bits  7.519132614135742  scale  -0.05189698562026024\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.502000570297241  m bits  3.532716751098633  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.11750683933496475  m bits  6.501726150512695  scale  0.08633677661418915\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.508394479751587  m bits  2.5013463497161865  scale  -0.4942798912525177\n",
      "weight quant params: \n",
      "e bits  2.5005993843078613  m bits  6.5675554275512695  scale  -0.3620016872882843\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.4936065673828125  m bits  4.39286994934082  scale  0.4138224124908447\n",
      "weight quant params: \n",
      "e bits  2.4630982875823975  m bits  6.433886528015137  scale  -0.509117841720581\n",
      "Train set: Average loss: 0.1557\n",
      "Test set: Average loss: 1.1750, Accuracy: 0.753 (75.30%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5114670395851135  m bits  4.510617733001709  scale  0.1596239060163498\n",
      "weight quant params: \n",
      "e bits  0.10724684596061707  m bits  7.481496334075928  scale  -0.05028415843844414\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.506115198135376  m bits  3.5046098232269287  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.10713879019021988  m bits  6.462889194488525  scale  0.036159489303827286\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.4793312549591064  m bits  2.5046072006225586  scale  -0.4911439120769501\n",
      "weight quant params: \n",
      "e bits  2.4797208309173584  m bits  6.51889181137085  scale  -0.435623437166214\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.510366201400757  m bits  4.470502853393555  scale  0.43007391691207886\n",
      "weight quant params: \n",
      "e bits  2.0226709842681885  m bits  6.476400852203369  scale  -0.5907777547836304\n",
      "Train set: Average loss: 0.1443\n",
      "Test set: Average loss: 1.1586, Accuracy: 0.7517 (75.17%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.50580233335495  m bits  4.509312629699707  scale  0.16645391285419464\n",
      "weight quant params: \n",
      "e bits  0.09786640107631683  m bits  7.780628681182861  scale  -0.07304461300373077\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5024540424346924  m bits  3.515839099884033  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.09767381101846695  m bits  6.503089427947998  scale  0.049166396260261536\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.508070707321167  m bits  2.540128707885742  scale  -0.4949977993965149\n",
      "weight quant params: \n",
      "e bits  2.340632915496826  m bits  6.495157718658447  scale  -0.5250807404518127\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5275356769561768  m bits  4.500319957733154  scale  0.4573642611503601\n",
      "weight quant params: \n",
      "e bits  1.671143651008606  m bits  6.268287658691406  scale  -0.7111231088638306\n",
      "Train set: Average loss: 0.1299\n",
      "Test set: Average loss: 1.2941, Accuracy: 0.7425 (74.25%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.511072039604187  m bits  4.5123467445373535  scale  0.17769783735275269\n",
      "weight quant params: \n",
      "e bits  0.08923518657684326  m bits  7.697864055633545  scale  -0.07738951593637466\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5039896965026855  m bits  3.502865791320801  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.08905623108148575  m bits  6.383476257324219  scale  0.03458521515130997\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.508222818374634  m bits  2.603031635284424  scale  -0.49365440011024475\n",
      "weight quant params: \n",
      "e bits  1.8122098445892334  m bits  6.5428667068481445  scale  -0.6703104376792908\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5145413875579834  m bits  4.389233589172363  scale  0.47035402059555054\n",
      "weight quant params: \n",
      "e bits  1.456176996231079  m bits  6.518397331237793  scale  -0.5866819620132446\n",
      "Train set: Average loss: 0.1320\n",
      "Test set: Average loss: 1.2628, Accuracy: 0.7527 (75.27%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5054709315299988  m bits  4.479796886444092  scale  0.21191027760505676\n",
      "weight quant params: \n",
      "e bits  0.08133794367313385  m bits  7.707547664642334  scale  -0.0729687437415123\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5041043758392334  m bits  3.5070314407348633  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.08119673281908035  m bits  6.4959211349487305  scale  0.046362847089767456\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.513200521469116  m bits  2.616014003753662  scale  -0.48902538418769836\n",
      "weight quant params: \n",
      "e bits  1.5101121664047241  m bits  6.52952241897583  scale  -0.805708646774292\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.5212759971618652  m bits  4.393045425415039  scale  0.4772055745124817\n",
      "weight quant params: \n",
      "e bits  1.3877122402191162  m bits  6.462283611297607  scale  -0.14167551696300507\n",
      "Train set: Average loss: 0.1278\n",
      "Test set: Average loss: 1.3108, Accuracy: 0.7506 (75.06%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5137291550636292  m bits  4.504974842071533  scale  0.21884377300739288\n",
      "weight quant params: \n",
      "e bits  0.07419715821743011  m bits  7.52029275894165  scale  -0.08205440640449524\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.506094217300415  m bits  3.5113368034362793  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.07404009997844696  m bits  6.510221004486084  scale  0.0284395981580019\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.547560453414917  m bits  2.694197654724121  scale  -0.4784148931503296\n",
      "weight quant params: \n",
      "e bits  1.4286830425262451  m bits  6.504315376281738  scale  -0.3543807566165924\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.4939064979553223  m bits  4.407192707061768  scale  0.5042043924331665\n",
      "weight quant params: \n",
      "e bits  1.3163743019104004  m bits  6.482903003692627  scale  0.1543227732181549\n",
      "Train set: Average loss: 0.1264\n",
      "Test set: Average loss: 1.2940, Accuracy: 0.7466 (74.66%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.509816586971283  m bits  4.510320663452148  scale  0.23895448446273804\n",
      "weight quant params: \n",
      "e bits  0.06767972558736801  m bits  7.5330119132995605  scale  -0.08849464356899261\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.467393159866333  m bits  3.5091044902801514  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.06751032918691635  m bits  6.505873203277588  scale  0.04159170389175415\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.505711793899536  m bits  2.6723265647888184  scale  -0.4776878356933594\n",
      "weight quant params: \n",
      "e bits  1.3573179244995117  m bits  6.429022312164307  scale  0.008104521781206131\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.4247498512268066  m bits  4.30494499206543  scale  0.5007696747779846\n",
      "weight quant params: \n",
      "e bits  1.2430981397628784  m bits  6.384550094604492  scale  0.38758522272109985\n",
      "Train set: Average loss: 0.1345\n",
      "Test set: Average loss: 1.3581, Accuracy: 0.7516 (75.16%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5051606893539429  m bits  4.502376079559326  scale  0.2462797909975052\n",
      "weight quant params: \n",
      "e bits  0.0617431104183197  m bits  7.5425944328308105  scale  -0.09792954474687576\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.53680419921875  m bits  3.501746416091919  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.061567991971969604  m bits  6.533729553222656  scale  0.021376101300120354\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.3916831016540527  m bits  2.6490931510925293  scale  -0.49187198281288147\n",
      "weight quant params: \n",
      "e bits  1.2836658954620361  m bits  6.500777244567871  scale  0.14655809104442596\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.3213050365448  m bits  4.259923934936523  scale  0.5010619759559631\n",
      "weight quant params: \n",
      "e bits  1.254105567932129  m bits  6.46666955947876  scale  0.4895908534526825\n",
      "Train set: Average loss: 0.1372\n",
      "Test set: Average loss: 1.2638, Accuracy: 0.7514 (75.14%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.512450635433197  m bits  4.505710124969482  scale  0.2702125310897827\n",
      "weight quant params: \n",
      "e bits  0.05635271593928337  m bits  7.511929512023926  scale  -0.10967440903186798\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5024991035461426  m bits  3.5246012210845947  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.05615474283695221  m bits  6.499026298522949  scale  -0.0128407571464777\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.4599273204803467  m bits  2.6863248348236084  scale  -0.49511414766311646\n",
      "weight quant params: \n",
      "e bits  1.2087125778198242  m bits  6.463529586791992  scale  0.21218036115169525\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.3817825317382812  m bits  4.1549272537231445  scale  0.505291759967804\n",
      "weight quant params: \n",
      "e bits  1.4937320947647095  m bits  6.440683364868164  scale  0.49693241715431213\n",
      "Train set: Average loss: 0.1196\n",
      "Test set: Average loss: 1.3706, Accuracy: 0.7484 (74.84%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5107044577598572  m bits  4.506499767303467  scale  0.27559563517570496\n",
      "weight quant params: \n",
      "e bits  0.05140139162540436  m bits  7.53505802154541  scale  -0.11251482367515564\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.5123143196105957  m bits  3.511809825897217  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.051210347563028336  m bits  6.530757904052734  scale  -0.008505781181156635\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5001108646392822  m bits  2.772935152053833  scale  -0.49313005805015564\n",
      "weight quant params: \n",
      "e bits  1.1334506273269653  m bits  6.520415782928467  scale  0.38143274188041687\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.4223411083221436  m bits  4.070414066314697  scale  0.502440869808197\n",
      "weight quant params: \n",
      "e bits  1.5549308061599731  m bits  6.492847442626953  scale  0.4569077491760254\n",
      "Train set: Average loss: 0.1166\n",
      "Test set: Average loss: 1.3645, Accuracy: 0.7442 (74.42%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5067307949066162  m bits  4.498873233795166  scale  0.30443304777145386\n",
      "weight quant params: \n",
      "e bits  0.046864304691553116  m bits  7.577800750732422  scale  -0.10910116136074066\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.508387804031372  m bits  3.51426100730896  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.04671730473637581  m bits  6.507293224334717  scale  -0.05126206949353218\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.5032362937927246  m bits  2.8739137649536133  scale  -0.4912300109863281\n",
      "weight quant params: \n",
      "e bits  1.2463160753250122  m bits  6.497905254364014  scale  0.49803537130355835\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.469472646713257  m bits  3.9628360271453857  scale  0.5242793560028076\n",
      "weight quant params: \n",
      "e bits  1.5785026550292969  m bits  6.403328895568848  scale  0.42703762650489807\n",
      "Train set: Average loss: 0.1217\n",
      "Test set: Average loss: 1.3369, Accuracy: 0.7505 (75.05%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5020214319229126  m bits  4.487844944000244  scale  0.3040013015270233\n",
      "weight quant params: \n",
      "e bits  0.04281913489103317  m bits  7.78610897064209  scale  -0.12392422556877136\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.502843141555786  m bits  3.5551655292510986  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.04261583834886551  m bits  6.4985032081604  scale  -0.07066744565963745\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.479268789291382  m bits  2.862746238708496  scale  -0.4984564483165741\n",
      "weight quant params: \n",
      "e bits  1.4943991899490356  m bits  6.504095077514648  scale  0.49639227986335754\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.2984273433685303  m bits  3.7825493812561035  scale  0.5220836400985718\n",
      "weight quant params: \n",
      "e bits  1.5800524950027466  m bits  6.502940654754639  scale  0.4092622399330139\n",
      "Train set: Average loss: 0.1069\n",
      "Test set: Average loss: 1.3966, Accuracy: 0.7569 (75.69%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5138177871704102  m bits  4.505702495574951  scale  0.3082423210144043\n",
      "weight quant params: \n",
      "e bits  0.03906836733222008  m bits  7.332287311553955  scale  -0.12366586923599243\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.4991796016693115  m bits  3.547114133834839  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.03888088837265968  m bits  6.493171691894531  scale  -0.10341311246156693\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.491483449935913  m bits  2.9629790782928467  scale  -0.49850553274154663\n",
      "weight quant params: \n",
      "e bits  1.5495495796203613  m bits  6.505378246307373  scale  0.462316632270813\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.2902796268463135  m bits  3.8378403186798096  scale  0.50566565990448\n",
      "weight quant params: \n",
      "e bits  1.6393259763717651  m bits  6.502442359924316  scale  0.35149216651916504\n",
      "Train set: Average loss: 0.0977\n",
      "Test set: Average loss: 1.3630, Accuracy: 0.7539 (75.39%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  0.5113105177879333  m bits  4.499848365783691  scale  0.3357251286506653\n",
      "weight quant params: \n",
      "e bits  0.03568593040108681  m bits  7.411954879760742  scale  -0.1321895271539688\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.505319833755493  m bits  3.5015342235565186  scale  0.500745415687561\n",
      "weight quant params: \n",
      "e bits  0.035465486347675323  m bits  6.512733459472656  scale  -0.08590669929981232\n",
      "module:  fc1\n",
      "input quant params: \n",
      "e bits:  2.504108428955078  m bits  3.110689163208008  scale  -0.49206623435020447\n",
      "weight quant params: \n",
      "e bits  1.5908732414245605  m bits  6.465236663818359  scale  0.42148375511169434\n",
      "module:  fc3\n",
      "input quant params: \n",
      "e bits:  3.251497983932495  m bits  3.7781922817230225  scale  0.500529408454895\n",
      "weight quant params: \n",
      "e bits  1.7350283861160278  m bits  6.476749420166016  scale  0.2710525393486023\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"using divice \", device)\n",
    "\n",
    "best_model_path = f\"train_weights_and_quant_{dataset}_best_model.pth\"\n",
    "quant_model_path = f\"train_weights_and_quant_{dataset}_quant_model.pth\"\n",
    "\n",
    "load_model_path = None\n",
    "save_model_path = best_model_path\n",
    "\n",
    "if(os.path.isfile(quant_model_path)):\n",
    "    load_model_path = quant_model_path\n",
    "    save_model_path =quant_model_path\n",
    "elif(os.path.isfile(best_model_path)):\n",
    "    load_model_path = best_model_path   \n",
    "    save_model_path = quant_model_path\n",
    "    \n",
    "if(load_model_path):\n",
    "    # Create model\n",
    "    # model = SimpleQuantizedMLP(e_bits=4.0, m_bits=4.0, num_classes=len(classes)).to(device)\n",
    "    model = QuantSimpleCIFAR10Model(num_classes=len(classes), optimizeQuant=True).to(device)\n",
    "    #model = SimpleCIFAR10Model(num_classes=len(classes)).to(device)\n",
    "    model.load_state_dict(torch.load(load_model_path, weights_only=True))\n",
    "else:\n",
    "    model = QuantSimpleCIFAR10Model(num_classes=len(classes), optimizeQuant=False).to(device)\n",
    "\n",
    "# Create optimizer (SGD or Adam)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "# Train for some epochs\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    accuracy = test(model, device, test_loader)\n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "        \n",
    "    printBitWidths(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
