{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "batch_size = 1024\n",
    "#dataset = \"MNIST\"\n",
    "dataset = \"CIFAR10\"\n",
    "#dataset = \"CIFAR100\"\n",
    "#dataset = \"FMNIST\"\n",
    "\n",
    "if(dataset == \"MNIST\"):\n",
    "    # 1) MNIST Dataset & Dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "    input_size = (1, 32, 32)\n",
    "\n",
    "if(dataset == \"FMNIST\"):\n",
    "    # 1) MNIST Dataset & Dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset  = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "    input_size = (1, 32, 32)\n",
    "    \n",
    "if(dataset == \"CIFAR10\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    input_size = (3, 32, 32)\n",
    "\n",
    "if(dataset == \"CIFAR100\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = [x for x in range(100)]\n",
    "\n",
    "    input_size = (3, 32, 32)\n",
    "    \n",
    "if(dataset == \"IMAGENET\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageNet(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.ImageNet(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = [x for x in range(100)]\n",
    "\n",
    "    input_size = (3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: a batch of images in shape (B, C, H, W) or a single image in (C, H, W).\n",
    "    \"\"\"\n",
    "    # If it's a batch of images (4D), make a grid first\n",
    "    if len(img_tensor.shape) == 4:\n",
    "        img_tensor = torchvision.utils.make_grid(img_tensor)\n",
    "    # Unnormalize\n",
    "    #img_tensor = unnormalize(img_tensor)\n",
    "    # Convert to numpy\n",
    "    npimg = img_tensor.numpy()\n",
    "    # Transpose from (C, H, W) to (H, W, C)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_bit(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(x)\n",
    "\n",
    "def bit_to_param(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_float_truncate(x: torch.Tensor, e_bits_int: int, m_bits_int: int, scale_int: int) -> torch.Tensor:\n",
    "\n",
    "    sign = x.sign()\n",
    "    abs_x = x.abs().clamp(min=1e-45) / 2**scale_int\n",
    "\n",
    "    #recover the floatint point representation\n",
    "    #exponent \\in {-2**7,..,2**7-1}\n",
    "    #mantissa \\in {1.0,...,2.0}\n",
    "\n",
    "    exponent = torch.floor(torch.log2(abs_x)).clamp(min=1e-45)\n",
    "    mantissa = abs_x / (2**exponent)\n",
    "    \n",
    "    #print(\"exp: \", exponent)\n",
    "    #print(\"mantissa: \", mantissa)\n",
    "\n",
    "    # truncate exponent\n",
    "    # lets parameterize the exponent as a constant value + a variable value\n",
    "    # the constant part is 2**7-1 in standar floating point, but we will learn it\n",
    "    # the variable part \\in {0,..,2**8-1}\n",
    "    # lets say exponent = v_exponent - 2**(bits-1)-1 + c_exponent\n",
    "    # so v_exponent = exponent + 2**(bits-1)-1 - c_exponent\n",
    "    c_exponent = 0 #scale_int\n",
    "    z_exponent = (2**(e_bits_int-1)-1)\n",
    "    if e_bits_int == 0:\n",
    "        z_exponent = 0\n",
    "    v_exponent = exponent + z_exponent - c_exponent\n",
    "    \n",
    "    #print(\"v exponent: \", v_exponent)\n",
    "    \n",
    "    # the valriable part is clamped to the alloted bits\n",
    "    q_min = torch.tensor(float(0)).to(x.device)\n",
    "    q_max = 2**e_bits_int-1\n",
    "    q_v_exponent = torch.clamp(v_exponent, q_min, q_max)\n",
    "    q_exponent = q_v_exponent - z_exponent + c_exponent\n",
    "\n",
    "    #print(\"q v exponent: \", q_v_exponent)\n",
    "    #print(\"q exponent: \", q_exponent)\n",
    "\n",
    "    # truncate mantissa\n",
    "    # this just removes the less significant bits\n",
    "    m_scale = 2.0 ** m_bits_int\n",
    "    q_mantissa = torch.floor(mantissa * m_scale) / m_scale\n",
    "\n",
    "    #print(\"q mantissa \", q_mantissa)\n",
    "\n",
    "    # from quantized floatint point to float\n",
    "    fq_x = sign * (2**q_exponent) * q_mantissa * 2**scale_int\n",
    "    return fq_x\n",
    "\n",
    "class FakeFloatFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e_bits_param, m_bits_param, scale_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, e_bits_param, m_bits_param, scale_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).item())\n",
    "        s_int = int(torch.round(scale_param).item())\n",
    "\n",
    "        out = fake_float_truncate(x, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "        #print(\"input\")\n",
    "        #print(x)\n",
    "        #print(\"output\")\n",
    "        #print(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, e_bits_param, m_bits_param, scale_param = ctx.saved_tensors\n",
    "        \n",
    "        e_bits = param_to_bit(e_bits_param)\n",
    "        m_bits = param_to_bit(m_bits_param)\n",
    "        scale = scale_param\n",
    "                \n",
    "        e_bits_int = int(torch.round(e_bits).item())\n",
    "        m_bits_int = int(torch.round(m_bits).item())\n",
    "        scale_int = int(torch.round(scale).item())\n",
    "\n",
    "        #print(\"shape x: \", x.shape)\n",
    "        #print(\"shape grad_output: \", grad_output.shape)\n",
    "\n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output\n",
    "        \n",
    "        # 1) Gradient wrt x: approximate with central difference\n",
    "        \"\"\"\n",
    "        grad_x = None\n",
    "        if True:\n",
    "            delta = 0.01            \n",
    "\n",
    "            f_plus2  = fake_float_truncate(x + 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_plus   = fake_float_truncate(x + 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus  = fake_float_truncate(x - 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus2 = fake_float_truncate(x - 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "            grad_x = grad_output * der\n",
    "        \"\"\"\n",
    "                \n",
    "        # 2) Gradient wrt e_bits: approximate with central difference\n",
    "        grad_e_bits = None\n",
    "        if e_bits_param.requires_grad:\n",
    "            \n",
    "            if(e_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int    , m_bits_int, scale_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int + 2, m_bits_int, scale_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int - 1, m_bits_int, scale_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int - 2, m_bits_int, scale_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_e_bits = grad_output * der * e_bits\n",
    "        \n",
    "        # 3) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_m_bits = None\n",
    "        if m_bits_param.requires_grad:\n",
    "            \n",
    "            if(m_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int    , scale_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int + 2, scale_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int - 1, scale_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int - 2, scale_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_m_bits = grad_output * der * m_bits\n",
    "       \n",
    "        # 4) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_scale_bits = None\n",
    "        if scale_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int + 2)\n",
    "            f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int + 1)\n",
    "            f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int - 1)\n",
    "            f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int - 2)\n",
    "            \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0 \n",
    "            grad_scale_bits = grad_output * der\n",
    "             \n",
    "        return grad_x, grad_e_bits, grad_m_bits, grad_scale_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_bits  5  m_bits  14  scale  1\n",
      "in:  tensor([-14.8656])  out:  tensor([-14.8652])\n",
      "e_bits  5  m_bits  11  scale  2\n",
      "in:  tensor([0.8147])  out:  tensor([0.8145])\n",
      "e_bits  6  m_bits  12  scale  2\n",
      "in:  tensor([21.1087])  out:  tensor([21.1055])\n",
      "e_bits  8  m_bits  17  scale  1\n",
      "in:  tensor([27.1807])  out:  tensor([27.1807])\n",
      "e_bits  1  m_bits  16  scale  -2\n",
      "in:  tensor([4.2998])  out:  tensor([0.5375])\n",
      "e_bits  6  m_bits  2  scale  2\n",
      "in:  tensor([-10.6638])  out:  tensor([-10.])\n",
      "e_bits  3  m_bits  9  scale  0\n",
      "in:  tensor([26.1001])  out:  tensor([26.0938])\n",
      "e_bits  8  m_bits  18  scale  -2\n",
      "in:  tensor([49.0925])  out:  tensor([49.0924])\n",
      "e_bits  6  m_bits  16  scale  0\n",
      "in:  tensor([-40.1979])  out:  tensor([-40.1978])\n",
      "e_bits  7  m_bits  17  scale  2\n",
      "in:  tensor([8.4961])  out:  tensor([8.4961])\n",
      "e_bits  8  m_bits  13  scale  -1\n",
      "in:  tensor([-43.9975])  out:  tensor([-43.9961])\n",
      "e_bits  6  m_bits  4  scale  1\n",
      "in:  tensor([-23.1570])  out:  tensor([-23.])\n",
      "e_bits  9  m_bits  1  scale  1\n",
      "in:  tensor([-41.6095])  out:  tensor([-32.])\n",
      "e_bits  6  m_bits  18  scale  -2\n",
      "in:  tensor([-29.6556])  out:  tensor([-29.6556])\n",
      "e_bits  9  m_bits  23  scale  -1\n",
      "in:  tensor([-32.3547])  out:  tensor([-32.3547])\n",
      "e_bits  0  m_bits  5  scale  -2\n",
      "in:  tensor([-44.3685])  out:  tensor([-0.3438])\n",
      "e_bits  8  m_bits  14  scale  -1\n",
      "in:  tensor([7.5115])  out:  tensor([7.5115])\n",
      "e_bits  6  m_bits  25  scale  1\n",
      "in:  tensor([4.9128])  out:  tensor([4.9128])\n",
      "e_bits  4  m_bits  17  scale  -2\n",
      "in:  tensor([-20.6209])  out:  tensor([-20.6208])\n",
      "e_bits  6  m_bits  1  scale  0\n",
      "in:  tensor([-6.7037])  out:  tensor([-6.])\n",
      "e_bits  8  m_bits  8  scale  1\n",
      "in:  tensor([18.3125])  out:  tensor([18.2500])\n",
      "e_bits  10  m_bits  12  scale  2\n",
      "in:  tensor([-11.7751])  out:  tensor([-11.7734])\n",
      "e_bits  2  m_bits  25  scale  1\n",
      "in:  tensor([17.5807])  out:  tensor([8.7903])\n",
      "e_bits  0  m_bits  6  scale  0\n",
      "in:  tensor([-27.2172])  out:  tensor([-1.6875])\n",
      "e_bits  5  m_bits  29  scale  -2\n",
      "in:  tensor([-25.4463])  out:  tensor([-25.4463])\n",
      "e_bits  1  m_bits  29  scale  2\n",
      "in:  tensor([34.5434])  out:  tensor([8.6359])\n",
      "e_bits  3  m_bits  27  scale  -2\n",
      "in:  tensor([-19.8689])  out:  tensor([-4.9672])\n",
      "e_bits  1  m_bits  28  scale  -2\n",
      "in:  tensor([-31.9061])  out:  tensor([-0.9971])\n",
      "e_bits  8  m_bits  20  scale  -1\n",
      "in:  tensor([21.0631])  out:  tensor([21.0631])\n",
      "e_bits  6  m_bits  2  scale  1\n",
      "in:  tensor([15.2192])  out:  tensor([14.])\n",
      "e_bits  5  m_bits  12  scale  2\n",
      "in:  tensor([-0.4997])  out:  tensor([-0.4990])\n",
      "e_bits  4  m_bits  11  scale  -1\n",
      "in:  tensor([-6.6381])  out:  tensor([-6.6367])\n",
      "e_bits  6  m_bits  28  scale  -1\n",
      "in:  tensor([-15.2831])  out:  tensor([-15.2831])\n",
      "e_bits  5  m_bits  6  scale  1\n",
      "in:  tensor([-33.6051])  out:  tensor([-33.5000])\n",
      "e_bits  3  m_bits  6  scale  -1\n",
      "in:  tensor([-49.0978])  out:  tensor([-12.2500])\n",
      "e_bits  5  m_bits  2  scale  -1\n",
      "in:  tensor([30.8325])  out:  tensor([28.])\n",
      "e_bits  10  m_bits  22  scale  1\n",
      "in:  tensor([-10.2197])  out:  tensor([-10.2197])\n",
      "e_bits  7  m_bits  9  scale  -1\n",
      "in:  tensor([44.9159])  out:  tensor([44.8750])\n",
      "e_bits  6  m_bits  20  scale  0\n",
      "in:  tensor([37.0891])  out:  tensor([37.0891])\n",
      "e_bits  3  m_bits  1  scale  2\n",
      "in:  tensor([26.6948])  out:  tensor([24.])\n",
      "e_bits  5  m_bits  15  scale  0\n",
      "in:  tensor([33.6217])  out:  tensor([33.6211])\n",
      "e_bits  7  m_bits  27  scale  0\n",
      "in:  tensor([4.2845])  out:  tensor([4.2845])\n",
      "e_bits  7  m_bits  14  scale  -2\n",
      "in:  tensor([39.7612])  out:  tensor([39.7598])\n",
      "e_bits  7  m_bits  18  scale  -1\n",
      "in:  tensor([-45.3879])  out:  tensor([-45.3878])\n",
      "e_bits  5  m_bits  28  scale  2\n",
      "in:  tensor([-36.0219])  out:  tensor([-36.0219])\n",
      "e_bits  9  m_bits  0  scale  -2\n",
      "in:  tensor([21.4286])  out:  tensor([16.])\n",
      "e_bits  6  m_bits  4  scale  -2\n",
      "in:  tensor([31.7783])  out:  tensor([31.])\n",
      "e_bits  4  m_bits  8  scale  0\n",
      "in:  tensor([40.3691])  out:  tensor([40.2500])\n",
      "e_bits  8  m_bits  12  scale  0\n",
      "in:  tensor([-34.2508])  out:  tensor([-34.2500])\n",
      "e_bits  2  m_bits  9  scale  -2\n",
      "in:  tensor([-2.4171])  out:  tensor([-1.2070])\n",
      "e_bits  6  m_bits  19  scale  2\n",
      "in:  tensor([-3.5648])  out:  tensor([-3.5648])\n",
      "e_bits  10  m_bits  21  scale  -2\n",
      "in:  tensor([45.4731])  out:  tensor([45.4731])\n",
      "e_bits  5  m_bits  2  scale  0\n",
      "in:  tensor([32.3686])  out:  tensor([32.])\n",
      "e_bits  1  m_bits  6  scale  -2\n",
      "in:  tensor([-44.3657])  out:  tensor([-0.6875])\n",
      "e_bits  9  m_bits  4  scale  0\n",
      "in:  tensor([-49.8431])  out:  tensor([-48.])\n",
      "e_bits  10  m_bits  14  scale  -2\n",
      "in:  tensor([26.4108])  out:  tensor([26.4102])\n",
      "e_bits  9  m_bits  16  scale  2\n",
      "in:  tensor([45.0090])  out:  tensor([45.0088])\n",
      "e_bits  1  m_bits  13  scale  1\n",
      "in:  tensor([32.5151])  out:  tensor([4.0640])\n",
      "e_bits  10  m_bits  1  scale  2\n",
      "in:  tensor([-25.9537])  out:  tensor([-24.])\n",
      "e_bits  4  m_bits  23  scale  2\n",
      "in:  tensor([15.4587])  out:  tensor([15.4587])\n",
      "e_bits  4  m_bits  7  scale  1\n",
      "in:  tensor([-40.9788])  out:  tensor([-40.7500])\n",
      "e_bits  2  m_bits  13  scale  -1\n",
      "in:  tensor([40.3935])  out:  tensor([2.5244])\n",
      "e_bits  2  m_bits  10  scale  -2\n",
      "in:  tensor([34.4060])  out:  tensor([1.0742])\n",
      "e_bits  4  m_bits  0  scale  -1\n",
      "in:  tensor([-47.1606])  out:  tensor([-32.])\n",
      "e_bits  5  m_bits  10  scale  -1\n",
      "in:  tensor([26.5384])  out:  tensor([26.5312])\n",
      "e_bits  7  m_bits  13  scale  -2\n",
      "in:  tensor([47.8655])  out:  tensor([47.8633])\n",
      "e_bits  5  m_bits  30  scale  2\n",
      "in:  tensor([16.4895])  out:  tensor([16.4895])\n",
      "e_bits  6  m_bits  12  scale  -2\n",
      "in:  tensor([-5.4827])  out:  tensor([-5.4824])\n",
      "e_bits  5  m_bits  13  scale  2\n",
      "in:  tensor([38.9580])  out:  tensor([38.9570])\n",
      "e_bits  3  m_bits  4  scale  -1\n",
      "in:  tensor([1.5012])  out:  tensor([1.5000])\n",
      "e_bits  3  m_bits  5  scale  -2\n",
      "in:  tensor([-8.1205])  out:  tensor([-4.])\n",
      "e_bits  8  m_bits  18  scale  -2\n",
      "in:  tensor([2.6169])  out:  tensor([2.6169])\n",
      "e_bits  4  m_bits  6  scale  0\n",
      "in:  tensor([-35.4267])  out:  tensor([-35.])\n",
      "e_bits  2  m_bits  20  scale  0\n",
      "in:  tensor([44.8653])  out:  tensor([5.6082])\n",
      "e_bits  3  m_bits  2  scale  -2\n",
      "in:  tensor([16.6504])  out:  tensor([4.])\n",
      "e_bits  7  m_bits  12  scale  1\n",
      "in:  tensor([13.1006])  out:  tensor([13.0996])\n",
      "e_bits  2  m_bits  8  scale  -2\n",
      "in:  tensor([8.9916])  out:  tensor([1.1211])\n",
      "e_bits  4  m_bits  4  scale  1\n",
      "in:  tensor([41.3244])  out:  tensor([40.])\n",
      "e_bits  8  m_bits  14  scale  0\n",
      "in:  tensor([-43.0253])  out:  tensor([-43.0234])\n",
      "e_bits  3  m_bits  30  scale  -1\n",
      "in:  tensor([-36.7450])  out:  tensor([-9.1862])\n",
      "e_bits  10  m_bits  1  scale  -1\n",
      "in:  tensor([25.9948])  out:  tensor([24.])\n",
      "e_bits  0  m_bits  7  scale  -1\n",
      "in:  tensor([-18.0673])  out:  tensor([-0.5625])\n",
      "e_bits  2  m_bits  15  scale  -2\n",
      "in:  tensor([2.4044])  out:  tensor([1.2022])\n",
      "e_bits  2  m_bits  20  scale  1\n",
      "in:  tensor([-16.3810])  out:  tensor([-8.1905])\n",
      "e_bits  6  m_bits  1  scale  -1\n",
      "in:  tensor([24.4153])  out:  tensor([24.])\n",
      "e_bits  3  m_bits  15  scale  -2\n",
      "in:  tensor([-43.4648])  out:  tensor([-5.4331])\n",
      "e_bits  0  m_bits  12  scale  1\n",
      "in:  tensor([15.8957])  out:  tensor([3.9736])\n",
      "e_bits  2  m_bits  7  scale  -1\n",
      "in:  tensor([-4.5239])  out:  tensor([-2.2500])\n",
      "e_bits  3  m_bits  14  scale  1\n",
      "in:  tensor([27.9512])  out:  tensor([27.9512])\n",
      "e_bits  2  m_bits  11  scale  -2\n",
      "in:  tensor([-40.0535])  out:  tensor([-1.2515])\n",
      "e_bits  6  m_bits  27  scale  -1\n",
      "in:  tensor([-36.5748])  out:  tensor([-36.5748])\n",
      "e_bits  7  m_bits  28  scale  2\n",
      "in:  tensor([-5.7797])  out:  tensor([-5.7797])\n",
      "e_bits  1  m_bits  7  scale  2\n",
      "in:  tensor([-2.1329])  out:  tensor([-2.1250])\n",
      "e_bits  8  m_bits  28  scale  1\n",
      "in:  tensor([29.8763])  out:  tensor([29.8763])\n",
      "e_bits  10  m_bits  25  scale  0\n",
      "in:  tensor([29.9491])  out:  tensor([29.9491])\n",
      "e_bits  4  m_bits  8  scale  1\n",
      "in:  tensor([7.2393])  out:  tensor([7.2344])\n",
      "e_bits  5  m_bits  7  scale  0\n",
      "in:  tensor([-17.6124])  out:  tensor([-17.5000])\n",
      "e_bits  2  m_bits  17  scale  1\n",
      "in:  tensor([12.9362])  out:  tensor([12.9362])\n",
      "e_bits  7  m_bits  24  scale  -1\n",
      "in:  tensor([6.1757])  out:  tensor([6.1757])\n",
      "e_bits  8  m_bits  17  scale  -2\n",
      "in:  tensor([-46.1531])  out:  tensor([-46.1531])\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    in_test = (torch.rand(1) - 0.5)*100.0\n",
    "    e_bits = int(torch.round(torch.rand(1)*10).item())\n",
    "    m_bits = int(torch.round(torch.rand(1)*30).item())\n",
    "    scale = int(torch.round((torch.rand(1) - 0.5)*5.0).item())\n",
    "    out_test = fake_float_truncate(in_test, e_bits, m_bits, scale)\n",
    "    print(\"e_bits \", e_bits, \" m_bits \", m_bits, \" scale \", scale)\n",
    "    print(\"in: \", in_test, \" out: \", out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_fixed_truncate2(x: torch.Tensor, bits_int: int, scale_int: int, zero_point_int: int) -> torch.Tensor:\n",
    "    \n",
    "    qmin = 0\n",
    "    qmax = 2**bits_int - 1\n",
    "    \n",
    "    #from float to fixed point, and quantize accordingly\n",
    "    q_x = torch.clamp(torch.round(x * 2**(scale_int + bits_int//2) + 2**(bits_int-1) + zero_point_int), qmin, qmax)\n",
    "\n",
    "    # from quantized fixed point to float\n",
    "    fq_x = (q_x - 2**(bits_int-1) - zero_point_int) / 2**(scale_int + bits_int//2)\n",
    "        \n",
    "    return fq_x\n",
    "\n",
    "def fake_fixed_truncate(x: torch.Tensor, bits_int: int, scale_int: int, zero_point_int: int) -> torch.Tensor:\n",
    "    \n",
    "    qmin = 0\n",
    "    qmax = 2**bits_int - 1\n",
    "    \n",
    "    mantissa = x * 2**(scale_int + bits_int//2) + zero_point_int + 2**(bits_int-1)\n",
    "    \n",
    "    #from float to fixed point, and quantize accordingly\n",
    "    q_x = torch.clamp(torch.round(mantissa), qmin, qmax)\n",
    "\n",
    "    # from quantized fixed point to float\n",
    "    fq_x = (q_x - 2**(bits_int-1) - zero_point_int) / 2**(scale_int + bits_int//2)\n",
    "        \n",
    "    return fq_x\n",
    "\n",
    "class FakeFixedFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, bits_param, scale_param, zero_point_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, bits_param, scale_param, zero_point_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        bits_int = int(torch.round(param_to_bit(bits_param)).item())\n",
    "        scale_int = int(torch.round(scale_param).item())\n",
    "        zero_point_int = int(torch.round(zero_point_param).item())\n",
    "\n",
    "        out = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int)\n",
    "        \n",
    "        #print(\"input\")\n",
    "        #print(x)\n",
    "        #print(\"output\")\n",
    "        #print(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, bits_param, scale_param, zero_point_param = ctx.saved_tensors\n",
    "        \n",
    "        bits = param_to_bit(bits_param)\n",
    "        scale = scale_param\n",
    "        zero_point = zero_point_param\n",
    "                \n",
    "        bits_int = int(torch.round(bits).item())\n",
    "        scale_int = int(torch.round(scale).item())\n",
    "        zero_point_int = int(torch.round(zero_point).item())\n",
    "\n",
    "        #print(\"shape x: \", x.shape)\n",
    "        #print(\"shape grad_output: \", grad_output.shape)\n",
    "\n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output\n",
    "        \n",
    "        # 1) Gradient wrt x: approximate with central difference\n",
    "        \"\"\"\n",
    "        grad_x = None\n",
    "        if True:\n",
    "            delta = 0.01            \n",
    "\n",
    "            f_plus2  = fake_float_truncate(x + 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_plus   = fake_float_truncate(x + 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus  = fake_float_truncate(x - 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus2 = fake_float_truncate(x - 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "            grad_x = grad_output * der\n",
    "        \"\"\"\n",
    "                \n",
    "        # 2) Gradient wrt bits: approximate with central difference\n",
    "        grad_bits = None\n",
    "        if bits_param.requires_grad:\n",
    "            if(bits_int < 2):\n",
    "                f_plus   = fake_fixed_truncate(x, bits_int + 1, scale_int, zero_point_int)\n",
    "                f_minus  = fake_fixed_truncate(x, bits_int    , scale_int, zero_point_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_fixed_truncate(x, bits_int + 2, scale_int, zero_point_int)\n",
    "                f_plus   = fake_fixed_truncate(x, bits_int + 1, scale_int, zero_point_int)\n",
    "                f_minus  = fake_fixed_truncate(x, bits_int - 1, scale_int, zero_point_int)\n",
    "                f_minus2 = fake_fixed_truncate(x, bits_int - 2, scale_int, zero_point_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_bits = grad_output * der * bits\n",
    "        \n",
    "        # 3) Gradient wrt scale: approximate with central difference\n",
    "        grad_scale_bits = None\n",
    "        if scale_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_fixed_truncate(x, bits_int, scale_int + 2, zero_point_int)\n",
    "            f_plus   = fake_fixed_truncate(x, bits_int, scale_int + 1, zero_point_int)\n",
    "            f_minus  = fake_fixed_truncate(x, bits_int, scale_int - 1, zero_point_int)\n",
    "            f_minus2 = fake_fixed_truncate(x, bits_int, scale_int - 2, zero_point_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_scale_bits = grad_output * der\n",
    "       \n",
    "        # 4) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_zero_point_bits = None\n",
    "        if zero_point_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int + 2)\n",
    "            f_plus   = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int + 1)\n",
    "            f_minus  = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int - 1)\n",
    "            f_minus2 = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int - 2)\n",
    "            \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0 \n",
    "            grad_zero_point_bits = grad_output * der\n",
    "             \n",
    "        return grad_x, grad_bits, grad_scale_bits, grad_zero_point_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits  1  scale  -2  zero_point  0\n",
      "in:  tensor([-13.1717])  out  tensor([-4.])\n",
      "bits  20  scale  0  zero_point  0\n",
      "in:  tensor([4.5344])  out  tensor([4.5342])\n",
      "bits  4  scale  -1  zero_point  0\n",
      "in:  tensor([-29.9198])  out  tensor([-4.])\n",
      "bits  23  scale  2  zero_point  0\n",
      "in:  tensor([45.0447])  out  tensor([45.0447])\n",
      "bits  27  scale  2  zero_point  0\n",
      "in:  tensor([-2.5901])  out  tensor([-2.5901])\n",
      "bits  7  scale  2  zero_point  0\n",
      "in:  tensor([-17.0951])  out  tensor([-2.])\n",
      "bits  24  scale  0  zero_point  0\n",
      "in:  tensor([13.8588])  out  tensor([13.8589])\n",
      "bits  5  scale  -2  zero_point  0\n",
      "in:  tensor([-33.2408])  out  tensor([-16.])\n",
      "bits  17  scale  -2  zero_point  0\n",
      "in:  tensor([5.1955])  out  tensor([5.2031])\n",
      "bits  32  scale  2  zero_point  0\n",
      "in:  tensor([-17.7370])  out  tensor([-17.7368])\n",
      "bits  10  scale  2  zero_point  0\n",
      "in:  tensor([-14.5775])  out  tensor([-4.])\n",
      "bits  24  scale  1  zero_point  0\n",
      "in:  tensor([44.3839])  out  tensor([44.3839])\n",
      "bits  8  scale  0  zero_point  0\n",
      "in:  tensor([44.8660])  out  tensor([7.9375])\n",
      "bits  6  scale  -1  zero_point  0\n",
      "in:  tensor([-16.6040])  out  tensor([-8.])\n",
      "bits  25  scale  -2  zero_point  0\n",
      "in:  tensor([4.6757])  out  tensor([4.6758])\n",
      "bits  9  scale  -1  zero_point  0\n",
      "in:  tensor([-36.4201])  out  tensor([-32.])\n",
      "bits  14  scale  -1  zero_point  0\n",
      "in:  tensor([-16.7281])  out  tensor([-16.7344])\n",
      "bits  8  scale  -2  zero_point  0\n",
      "in:  tensor([32.6901])  out  tensor([31.7500])\n",
      "bits  19  scale  -1  zero_point  0\n",
      "in:  tensor([48.1521])  out  tensor([48.1523])\n",
      "bits  29  scale  0  zero_point  0\n",
      "in:  tensor([29.5912])  out  tensor([29.5918])\n",
      "bits  24  scale  -2  zero_point  0\n",
      "in:  tensor([40.3220])  out  tensor([40.3223])\n",
      "bits  11  scale  0  zero_point  0\n",
      "in:  tensor([-45.6163])  out  tensor([-32.])\n",
      "bits  26  scale  0  zero_point  0\n",
      "in:  tensor([46.0319])  out  tensor([46.0317])\n",
      "bits  25  scale  0  zero_point  0\n",
      "in:  tensor([-38.0999])  out  tensor([-38.0999])\n",
      "bits  26  scale  0  zero_point  0\n",
      "in:  tensor([-1.2968])  out  tensor([-1.2969])\n",
      "bits  15  scale  -2  zero_point  0\n",
      "in:  tensor([-45.5634])  out  tensor([-45.5625])\n",
      "bits  24  scale  0  zero_point  0\n",
      "in:  tensor([36.0538])  out  tensor([36.0537])\n",
      "bits  19  scale  -1  zero_point  0\n",
      "in:  tensor([-26.1241])  out  tensor([-26.1250])\n",
      "bits  5  scale  -1  zero_point  0\n",
      "in:  tensor([2.4574])  out  tensor([2.5000])\n",
      "bits  20  scale  -1  zero_point  0\n",
      "in:  tensor([-24.6177])  out  tensor([-24.6172])\n",
      "bits  23  scale  1  zero_point  0\n",
      "in:  tensor([-0.8558])  out  tensor([-0.8557])\n",
      "bits  31  scale  0  zero_point  0\n",
      "in:  tensor([-42.1877])  out  tensor([-42.1875])\n",
      "bits  10  scale  1  zero_point  0\n",
      "in:  tensor([-26.9977])  out  tensor([-8.])\n",
      "bits  4  scale  -1  zero_point  0\n",
      "in:  tensor([40.4975])  out  tensor([3.5000])\n",
      "bits  11  scale  1  zero_point  0\n",
      "in:  tensor([-36.7459])  out  tensor([-16.])\n",
      "bits  1  scale  2  zero_point  0\n",
      "in:  tensor([-7.8336])  out  tensor([-0.2500])\n",
      "bits  22  scale  -2  zero_point  0\n",
      "in:  tensor([-10.0922])  out  tensor([-10.0918])\n",
      "bits  11  scale  0  zero_point  0\n",
      "in:  tensor([-6.5633])  out  tensor([-6.5625])\n",
      "bits  26  scale  0  zero_point  0\n",
      "in:  tensor([43.0837])  out  tensor([43.0835])\n",
      "bits  15  scale  0  zero_point  0\n",
      "in:  tensor([6.6646])  out  tensor([6.6641])\n",
      "bits  8  scale  -2  zero_point  0\n",
      "in:  tensor([-3.9156])  out  tensor([-4.])\n",
      "bits  3  scale  -2  zero_point  0\n",
      "in:  tensor([37.6302])  out  tensor([6.])\n",
      "bits  24  scale  2  zero_point  0\n",
      "in:  tensor([-46.1515])  out  tensor([-46.1515])\n",
      "bits  26  scale  -1  zero_point  0\n",
      "in:  tensor([30.0467])  out  tensor([30.0469])\n",
      "bits  0  scale  -1  zero_point  0\n",
      "in:  tensor([13.9603])  out  tensor([-1.])\n",
      "bits  22  scale  2  zero_point  0\n",
      "in:  tensor([-28.2123])  out  tensor([-28.2123])\n",
      "bits  25  scale  -2  zero_point  0\n",
      "in:  tensor([-21.6422])  out  tensor([-21.6426])\n",
      "bits  8  scale  2  zero_point  0\n",
      "in:  tensor([28.4844])  out  tensor([1.9844])\n",
      "bits  30  scale  -2  zero_point  0\n",
      "in:  tensor([18.9832])  out  tensor([18.9844])\n",
      "bits  10  scale  0  zero_point  0\n",
      "in:  tensor([-34.6586])  out  tensor([-16.])\n",
      "bits  9  scale  -1  zero_point  0\n",
      "in:  tensor([-13.8251])  out  tensor([-13.8750])\n",
      "bits  5  scale  -1  zero_point  0\n",
      "in:  tensor([-21.4558])  out  tensor([-8.])\n",
      "bits  30  scale  0  zero_point  0\n",
      "in:  tensor([48.1123])  out  tensor([48.1133])\n",
      "bits  3  scale  -2  zero_point  0\n",
      "in:  tensor([49.2095])  out  tensor([6.])\n",
      "bits  11  scale  -2  zero_point  0\n",
      "in:  tensor([-37.6825])  out  tensor([-37.6250])\n",
      "bits  3  scale  -1  zero_point  0\n",
      "in:  tensor([45.5234])  out  tensor([3.])\n",
      "bits  9  scale  2  zero_point  0\n",
      "in:  tensor([42.1993])  out  tensor([3.9844])\n",
      "bits  26  scale  -1  zero_point  0\n",
      "in:  tensor([-4.3409])  out  tensor([-4.3408])\n",
      "bits  10  scale  0  zero_point  0\n",
      "in:  tensor([9.4478])  out  tensor([9.4375])\n",
      "bits  19  scale  2  zero_point  0\n",
      "in:  tensor([-14.1939])  out  tensor([-14.1938])\n",
      "bits  31  scale  -1  zero_point  0\n",
      "in:  tensor([-43.5410])  out  tensor([-43.5430])\n",
      "bits  19  scale  2  zero_point  0\n",
      "in:  tensor([-36.8191])  out  tensor([-36.8193])\n",
      "bits  15  scale  2  zero_point  0\n",
      "in:  tensor([-19.4204])  out  tensor([-19.4199])\n",
      "bits  13  scale  -1  zero_point  0\n",
      "in:  tensor([-43.5411])  out  tensor([-43.5312])\n",
      "bits  15  scale  -1  zero_point  0\n",
      "in:  tensor([-7.8552])  out  tensor([-7.8594])\n",
      "bits  8  scale  2  zero_point  0\n",
      "in:  tensor([5.7034])  out  tensor([1.9844])\n",
      "bits  27  scale  -1  zero_point  0\n",
      "in:  tensor([44.2500])  out  tensor([44.2500])\n",
      "bits  21  scale  0  zero_point  0\n",
      "in:  tensor([47.1300])  out  tensor([47.1299])\n",
      "bits  26  scale  -2  zero_point  0\n",
      "in:  tensor([34.7111])  out  tensor([34.7109])\n",
      "bits  4  scale  2  zero_point  0\n",
      "in:  tensor([-27.1653])  out  tensor([-0.5000])\n",
      "bits  4  scale  -1  zero_point  0\n",
      "in:  tensor([18.1004])  out  tensor([3.5000])\n",
      "bits  28  scale  0  zero_point  0\n",
      "in:  tensor([33.8943])  out  tensor([33.8945])\n",
      "bits  4  scale  2  zero_point  0\n",
      "in:  tensor([8.8394])  out  tensor([0.4375])\n",
      "bits  28  scale  -2  zero_point  0\n",
      "in:  tensor([24.7940])  out  tensor([24.7930])\n",
      "bits  11  scale  -2  zero_point  0\n",
      "in:  tensor([47.8304])  out  tensor([47.8750])\n",
      "bits  3  scale  2  zero_point  0\n",
      "in:  tensor([1.7475])  out  tensor([0.3750])\n",
      "bits  25  scale  2  zero_point  0\n",
      "in:  tensor([-33.2664])  out  tensor([-33.2664])\n",
      "bits  1  scale  -2  zero_point  0\n",
      "in:  tensor([-10.2519])  out  tensor([-4.])\n",
      "bits  21  scale  0  zero_point  0\n",
      "in:  tensor([41.2299])  out  tensor([41.2295])\n",
      "bits  23  scale  -2  zero_point  0\n",
      "in:  tensor([-46.7377])  out  tensor([-46.7383])\n",
      "bits  30  scale  0  zero_point  0\n",
      "in:  tensor([-27.5456])  out  tensor([-27.5459])\n",
      "bits  7  scale  -1  zero_point  0\n",
      "in:  tensor([-39.5362])  out  tensor([-16.])\n",
      "bits  5  scale  -2  zero_point  0\n",
      "in:  tensor([20.2547])  out  tensor([15.])\n",
      "bits  7  scale  -2  zero_point  0\n",
      "in:  tensor([5.2064])  out  tensor([5.])\n",
      "bits  30  scale  -1  zero_point  0\n",
      "in:  tensor([-48.8326])  out  tensor([-48.8320])\n",
      "bits  6  scale  2  zero_point  0\n",
      "in:  tensor([-32.7332])  out  tensor([-1.])\n",
      "bits  17  scale  0  zero_point  0\n",
      "in:  tensor([-18.1420])  out  tensor([-18.1406])\n",
      "bits  21  scale  1  zero_point  0\n",
      "in:  tensor([-13.8294])  out  tensor([-13.8296])\n",
      "bits  16  scale  1  zero_point  0\n",
      "in:  tensor([-9.5732])  out  tensor([-9.5723])\n",
      "bits  22  scale  -1  zero_point  0\n",
      "in:  tensor([19.1954])  out  tensor([19.1953])\n",
      "bits  12  scale  -1  zero_point  0\n",
      "in:  tensor([22.1121])  out  tensor([22.1250])\n",
      "bits  7  scale  2  zero_point  0\n",
      "in:  tensor([-17.3454])  out  tensor([-2.])\n",
      "bits  27  scale  1  zero_point  0\n",
      "in:  tensor([3.1605])  out  tensor([3.1606])\n",
      "bits  25  scale  2  zero_point  0\n",
      "in:  tensor([12.1028])  out  tensor([12.1029])\n",
      "bits  26  scale  0  zero_point  0\n",
      "in:  tensor([32.8159])  out  tensor([32.8159])\n",
      "bits  5  scale  2  zero_point  0\n",
      "in:  tensor([-43.5187])  out  tensor([-1.])\n",
      "bits  18  scale  0  zero_point  0\n",
      "in:  tensor([-0.0208])  out  tensor([-0.0215])\n",
      "bits  20  scale  -2  zero_point  0\n",
      "in:  tensor([18.0644])  out  tensor([18.0625])\n",
      "bits  29  scale  2  zero_point  0\n",
      "in:  tensor([48.7176])  out  tensor([48.7178])\n",
      "bits  6  scale  -1  zero_point  0\n",
      "in:  tensor([-25.3229])  out  tensor([-8.])\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    bits = int(torch.round(torch.rand(1)*32).item())\n",
    "    scale = int(torch.round((torch.rand(1) - 0.5)*5.0).item())\n",
    "    zero_point = int(torch.round((torch.rand(1) - 0.5)*0.0).item())\n",
    "    in_test = (torch.rand(1)-0.5)*100.0\n",
    "    out_test = fake_fixed_truncate(in_test, bits, scale, zero_point)\n",
    "    print(\"bits \", bits, \" scale \", scale, \" zero_point \", zero_point)\n",
    "    print(\"in: \", in_test, \" out \", out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### differentiable Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoundSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        return grad_output\n",
    "    \n",
    "class RoundFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.round(input + 2*delta)\n",
    "        f_plus   = torch.round(input + 1*delta)\n",
    "        f_minus  = torch.round(input - 1*delta)\n",
    "        f_minus2 = torch.round(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "class RoundSIG(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function that does a hard round in forward,\n",
    "    but uses a sigmoid-based approximation for the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, alpha=10.0):\n",
    "        \"\"\"\n",
    "        Forward pass: returns torch.round(input).\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.alpha = alpha\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: approximate the gradient of round(x)\n",
    "        with the derivative of a sigmoid centered at the fractional midpoint (0.5).\n",
    "        \"\"\"\n",
    "        (input,) = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "\n",
    "        # Fractional part\n",
    "        frac = input - torch.floor(input)\n",
    "\n",
    "        # Sigmoid of (fractional_part - 0.5), scaled by alpha\n",
    "        s = torch.sigmoid(alpha * (frac - 0.5))\n",
    "\n",
    "        # Derivative of sigmoid = alpha * s * (1 - s)\n",
    "        grad_input = alpha * s * (1 - s) * grad_output\n",
    "        return grad_input, None  # alpha is not a tensor that requires grad\n",
    "    \n",
    "def diff_round(x):\n",
    "    return RoundSTE.apply(x)\n",
    "    #return RoundFDE.apply(x)\n",
    "    #return RoundSIG.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable Floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloorSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass uses standard floor\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-through pass: just return the gradient as-is\n",
    "        return grad_output\n",
    "\n",
    "class FloorFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.floor(input + 2*delta)\n",
    "        f_plus   = torch.floor(input + 1*delta)\n",
    "        f_minus  = torch.floor(input - 1*delta)\n",
    "        f_minus2 = torch.floor(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "def diff_floor(input):\n",
    "    return FloorSTE.apply(input)\n",
    "    #return FloorFDE.apply(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxObserver(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We store running min/max\n",
    "        self.register_buffer(\"min_val\", torch.tensor(float(\"inf\")))\n",
    "        self.register_buffer(\"max_val\", torch.tensor(float(\"-inf\")))\n",
    "        # You could also store averaging stats, etc.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update running min/max\n",
    "        self.min_val = torch.min(self.min_val, x.detach().min())\n",
    "        self.max_val = torch.max(self.max_val, x.detach().max())\n",
    "        return x  # Just pass through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed point quanizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, observer, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.observer = observer\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        \n",
    "        # 1) Get min/max from observer\n",
    "        min_val = self.observer.min_val\n",
    "        max_val = self.observer.max_val\n",
    "\n",
    "        # If they're not valid, skip\n",
    "        #if min_val >= max_val:\n",
    "        #    return x\n",
    "\n",
    "        # 2) Compute scale and zero_point\n",
    "        # For an unsigned 4-bit range, we can hold values 0..15\n",
    "        # qmin, qmax = 0, (1 << b_int) - 1  # e.g. 0..15\n",
    "        qmin, qmax = torch.tensor(float(0)), 2**b_int - 1  # e.g. 0..15\n",
    "        \n",
    "        qmin = qmin.to(x.device)\n",
    "        #qmax = qmax.to(x.device)\n",
    "        max_val = max_val.to(x.device)\n",
    "        min_val = min_val.to(x.device)\n",
    "\n",
    "        # Typical formula for scale/zero-point:\n",
    "        scale = (max_val - min_val) / float(qmax - qmin)\n",
    "        zero_point = qmin - diff_round(min_val / scale)\n",
    "\n",
    "        # 3) Quantize (in floating point)\n",
    "        # clamp to range of [qmin, qmax]\n",
    "        q_x = torch.clamp(diff_round(x / scale + zero_point), qmin, qmax)\n",
    "\n",
    "        # 4) Dequantize back to float\n",
    "        fq_x = (q_x - zero_point) * scale\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"bits: \", self.bits.detach().item())\n",
    "        \n",
    "class FixedPointFakeQuantize2(nn.Module):\n",
    "    def __init__(self, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(bits//2)), requires_grad=requires_grad)\n",
    "        self.zero_point = nn.Parameter(torch.tensor(float(2**(bits//2-1)-1)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        bits_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        zero_point_int = diff_round(self.zero_point)\n",
    "        \n",
    "        qmin = torch.tensor(float(0)).to(x.device)\n",
    "        qmax = 2**bits_int - 1  # e.g. 0..15\n",
    "        \n",
    "        #from float to fixed point, and quantize accordingly\n",
    "        q_x = torch.clamp(diff_round(x * 2**scale_int + zero_point_int), qmin, qmax)\n",
    "\n",
    "        # from quantized fixed point to float\n",
    "        fq_x = (q_x - zero_point_int) / 2**scale_int\n",
    "        \n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"bits: \", self.bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())\n",
    "        print(\"zero point: \", self.zero_point.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatingPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, m_bits=23, e_bits=8, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.e_bits = nn.Parameter(torch.tensor(float(e_bits)), requires_grad=requires_grad)\n",
    "        self.m_bits = nn.Parameter(torch.tensor(float(m_bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(0)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e_bits_int = torch.clamp(diff_round(self.e_bits), 0, 32)\n",
    "        m_bits_int = torch.clamp(diff_round(self.m_bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        \n",
    "        sign = x.sign()\n",
    "        abs_x = x.abs().clamp(min=1e-45)\n",
    "\n",
    "        #recover the floatint point representation\n",
    "        #exponent \\in {-2**7,..,2**7-1}\n",
    "        #mantissa \\in {1.0,...,2.0}\n",
    "\n",
    "        exponent = diff_floor(torch.log2(abs_x)).clamp(min=1e-45)\n",
    "        mantissa = abs_x / (2**exponent)\n",
    "    \n",
    "        # truncate exponent\n",
    "        # lets parameterize the exponent as a constant value + a variable value\n",
    "        # the constant part is 2**7-1 in standar floating point, but we will learn it\n",
    "        # the variable part \\in {0,..,2**8-1}\n",
    "        # lets say exponent = v_exponent - 2**(bits-1)-1 + c_exponent\n",
    "        # so v_exponent = exponent + 2**(bits-1)-1 - c_exponent\n",
    "        c_exponent = scale_int\n",
    "        v_exponent = exponent + (2**(e_bits_int-1)-1) - c_exponent\n",
    "        \n",
    "        # the valriable part is clamped to the alloted bits\n",
    "        q_min = torch.tensor(float(0)).to(x.device)\n",
    "        q_max = 2**e_bits_int-1\n",
    "        q_exponent = torch.clamp(v_exponent, q_min, q_max) - (2**(e_bits_int-1)-1) + c_exponent\n",
    "    \n",
    "        # truncate mantissa\n",
    "        # this just removes the less significant bits\n",
    "        m_scale = 2.0 ** m_bits_int\n",
    "        q_mantissa = diff_floor(mantissa * m_scale) / m_scale\n",
    "    \n",
    "        # from quantized floatint point to float\n",
    "        fq_x = sign * (2**q_exponent) * q_mantissa\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.e_bits, self.m_bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"e_bits: \", self.e_bits.detach().item())\n",
    "        print(\"m_bits: \", self.m_bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float32 example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * input_size[1]//8 * input_size[2]//8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantWrapper(nn.Module):\n",
    "    def __init__(self, module, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.observer = MinMaxObserver()\n",
    "        self.fake_quant_input = FixedPointFakeQuantize(self.observer, requires_grad=optimizeQuant)\n",
    "        self.fake_quant_weight = FixedPointFakeQuantize(self.observer, requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_input = FixedPointFakeQuantize2(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_weight = FixedPointFakeQuantize2(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_input = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_weight = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.observer(x)\n",
    "        x = self.fake_quant_input(x)\n",
    "        w = self.fake_quant_weight(self.module.weight)\n",
    "        b = self.module.bias\n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            return F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            return F.linear(x, w, b)\n",
    "        else:\n",
    "            return self.module(x)\n",
    "        \n",
    "    def getBits(self):\n",
    "        return self.fake_quant_input.getBits() + self.fake_quant_weight.getBits()\n",
    "        #return self.fake_quant_weight.getBits()\n",
    "    \n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        self.fake_quant_input.printParams()\n",
    "        print(\"weight quant params: \")\n",
    "        self.fake_quant_weight.printParams()\n",
    "\n",
    "class QuantWrapperFloatingPoint(nn.Module):\n",
    "    def __init__(self, module, e_bits=5, m_bits=10, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "        self.input_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        self.input_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        self.input_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "        self.weight_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = FakeFloatFunction.apply(x,   self.input_e_bits_param, self.input_m_bits_param, self.input_scale)\n",
    "        w = FakeFloatFunction.apply(self.module.weight, self.weight_e_bits_param, self.weight_m_bits_param, self.weight_scale)\n",
    "        \n",
    "        b = self.module.bias\n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            return F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            return F.linear(x, w, b)\n",
    "        else:\n",
    "            return self.module(x)\n",
    "        \n",
    "    def getBits(self):\n",
    "        return [param_to_bit(self.input_e_bits_param) + param_to_bit(self.input_m_bits_param) + 1, param_to_bit(self.weight_e_bits_param) + param_to_bit(self.weight_m_bits_param) + 1]\n",
    "\n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        print(\"e bits: \", param_to_bit(self.input_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.input_m_bits_param).detach().item(), \" scale \", self.input_scale.detach().item())\n",
    "        print(\"weight quant params: \")\n",
    "        print(\"e bits \", param_to_bit(self.weight_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.weight_m_bits_param).detach().item(), \" scale \", self.weight_scale.detach().item())\n",
    "\n",
    "class QuantWrapperFixedPoint(nn.Module):\n",
    "    def __init__(self, module, bits=32, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "        self.input_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        self.input_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.input_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.weight_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.weight_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = FakeFixedFunction.apply(x, self.input_bits_param, self.input_scale, self.input_zero_point)\n",
    "        w = FakeFixedFunction.apply(self.module.weight, self.weight_bits_param, self.weight_scale, self.weight_zero_point)\n",
    "        \n",
    "        b = self.module.bias\n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            return F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            return F.linear(x, w, b)\n",
    "        else:\n",
    "            return self.module(x)\n",
    "        \n",
    "    def getBits(self):\n",
    "        return [param_to_bit(self.input_bits_param), param_to_bit(self.weight_bits_param)]\n",
    "\n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.input_bits_param).detach().item(), \" scale \", self.input_scale.detach().item(), \" zero point \", self.input_zero_point.detach().item())\n",
    "        print(\"weight quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.weight_bits_param).detach().item(), \" scale \", self.weight_scale.detach().item(), \" zero point \", self.weight_zero_point.detach().item())\n",
    "\n",
    "class QuantSimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, QuantClass, num_classes=10, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = QuantClass(nn.Conv2d(input_size[0], 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = QuantClass(nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = QuantClass(nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = QuantClass(nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5 = QuantClass(nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = QuantClass(nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.fc7 = QuantClass(nn.Linear(256 * input_size[1]//4 * input_size[2]//4, 512), optimizeQuant=optimizeQuant)\n",
    "        self.bn7 = nn.BatchNorm1d(512)\n",
    "        self.fc8 = QuantClass(nn.Linear(512, 512), optimizeQuant=optimizeQuant)\n",
    "        self.bn8 = nn.BatchNorm1d(512)\n",
    "        self.fc9 = QuantClass(nn.Linear(512, num_classes), optimizeQuant=optimizeQuant)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = 2.0 * x - torch.tensor([1.0], device=x.device)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class squared_hinge_loss(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, predictions, targets):\n",
    "        ctx.save_for_backward(predictions, targets)\n",
    "        output = 1. - predictions.mul(targets)\n",
    "        output[output.le(0.)] = 0.\n",
    "        loss = torch.mean(output.mul(output))\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        predictions, targets = ctx.saved_tensors\n",
    "        output = 1. - predictions.mul(targets)\n",
    "        output[output.le(0.)] = 0.\n",
    "        grad_output.resize_as_(predictions).copy_(targets).mul_(-2.).mul_(output)\n",
    "        grad_output.mul_(output.ne(0).float())\n",
    "        grad_output.div_(predictions.numel())\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "class SqrHingeLoss(nn.Module):\n",
    "    # Squared Hinge Loss\n",
    "    def __init__(self):\n",
    "        super(SqrHingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return squared_hinge_loss.apply(input, target)\n",
    "    \n",
    "def label_smoothing_loss(pred, target, smoothing=0.1):\n",
    "    confidence = 1.0 - smoothing\n",
    "    log_probs = F.log_softmax(pred, dim=-1)\n",
    "    nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "    nll_loss = nll_loss.squeeze(1)\n",
    "    smooth_loss = -log_probs.mean(dim=-1)\n",
    "    loss = confidence * nll_loss + smoothing * smooth_loss\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwidth_squared(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += bit ** 2\n",
    "                c += 1\n",
    "    if c == 0:\n",
    "        return 0\n",
    "    return s/c\n",
    "\n",
    "def bitwidth_sum(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += bit\n",
    "                c += 1\n",
    "    if c==0:\n",
    "        return 0\n",
    "    return s/c\n",
    "\n",
    "def bitwidth_round_sum(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += torch.round(bit)\n",
    "                c += 1\n",
    "    if c==0:\n",
    "        return 0\n",
    "    return s/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBitWidths(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            print(\"module: \", name)\n",
    "            module.printQuantParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, bit_width_criterion, scheduler=None, lambda_bw=1e-1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        if isinstance(criterion, SqrHingeLoss):\n",
    "            target = target.unsqueeze(1)\n",
    "            target_onehot = torch.Tensor(target.size(0), len(classes)).to(device, non_blocking=True)\n",
    "            target_onehot.fill_(-1)\n",
    "            target_onehot.scatter_(1, target, 1)\n",
    "            target = target.squeeze()\n",
    "            target = target_onehot\n",
    "                    \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_ce = criterion(output, target)\n",
    "        penalty_bw = bit_width_criterion(model) \n",
    "        loss = loss_ce + lambda_bw*penalty_bw\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #if batch_idx % 200 == 0:\n",
    "        #    print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "        #          f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Train set: Average loss: {train_loss:.4f}\")\n",
    "\n",
    "def test(model, device, test_loader, criterion, bit_width_criterion, lambda_bw=1e-1):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    penalty_bw = bit_width_criterion(model) \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            if isinstance(criterion, SqrHingeLoss):\n",
    "                target = target.unsqueeze(1)\n",
    "                target_onehot = torch.Tensor(target.size(0), len(classes)).to(device, non_blocking=True)\n",
    "                target_onehot.fill_(-1)\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                target = target.squeeze()\n",
    "                target = target_onehot\n",
    "\n",
    "            loss_ce = criterion(output, target)\n",
    "            loss = loss_ce + lambda_bw*penalty_bw\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy} ({100.0*accuracy:.2f}%) bit penalty {penalty_bw}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using divice  cuda\n",
      "Train set: Average loss: 1.4337\n",
      "Test set: Average loss: 1.5796, Accuracy: 0.8695 (86.95%) bit penalty 10.448759078979492\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  3.150547742843628  m bits  6.294532775878906  scale  0.010214137844741344\n",
      "weight quant params: \n",
      "e bits  3.150547742843628  m bits  6.2869062423706055  scale  0.064816415309906\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  3.1669836044311523  m bits  6.3007402420043945  scale  0.047850511968135834\n",
      "weight quant params: \n",
      "e bits  3.150547742843628  m bits  6.295043468475342  scale  0.2037103772163391\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  3.153286933898926  m bits  6.301995277404785  scale  0.023515572771430016\n",
      "weight quant params: \n",
      "e bits  3.150547742843628  m bits  6.301348686218262  scale  0.17917558550834656\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  3.1507532596588135  m bits  6.298923015594482  scale  -0.006321337074041367\n",
      "weight quant params: \n",
      "e bits  3.150547742843628  m bits  6.298120498657227  scale  0.1814655363559723\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  3.1514759063720703  m bits  6.301714897155762  scale  -0.06267822533845901\n",
      "weight quant params: \n",
      "e bits  3.150547742843628  m bits  6.300666809082031  scale  0.062782883644104\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  3.1531105041503906  m bits  6.300343036651611  scale  0.1577119529247284\n",
      "weight quant params: \n",
      "e bits  3.150547742843628  m bits  6.300531387329102  scale  0.07565266638994217\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  3.1546359062194824  m bits  6.302143096923828  scale  0.19827452301979065\n",
      "weight quant params: \n",
      "e bits  3.150547742843628  m bits  6.292784214019775  scale  0.3068755269050598\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  3.1507716178894043  m bits  6.299971103668213  scale  0.23821692168712616\n",
      "weight quant params: \n",
      "e bits  3.150547742843628  m bits  6.29951810836792  scale  0.1238163486123085\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  3.150622844696045  m bits  6.294936656951904  scale  0.4423297345638275\n",
      "weight quant params: \n",
      "e bits  3.150547742843628  m bits  6.270313262939453  scale  0.44742536544799805\n",
      "Train set: Average loss: 0.9672\n",
      "Test set: Average loss: 1.2417, Accuracy: 0.8656 (86.56%) bit penalty 7.553471088409424\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  2.1651957035064697  m bits  4.734099864959717  scale  -0.5056944489479065\n",
      "weight quant params: \n",
      "e bits  2.1651957035064697  m bits  4.541627883911133  scale  -0.16791798174381256\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.327319383621216  m bits  4.3332905769348145  scale  0.33800309896469116\n",
      "weight quant params: \n",
      "e bits  2.1651957035064697  m bits  4.333481311798096  scale  0.2886268198490143\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  2.196599245071411  m bits  4.331116199493408  scale  0.061198022216558456\n",
      "weight quant params: \n",
      "e bits  2.165156126022339  m bits  4.333738803863525  scale  0.3132571876049042\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  2.176590919494629  m bits  4.321890354156494  scale  0.2103360891342163\n",
      "weight quant params: \n",
      "e bits  2.1651957035064697  m bits  4.378771781921387  scale  -0.09417697787284851\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  2.1598761081695557  m bits  4.333999156951904  scale  -0.008864674717187881\n",
      "weight quant params: \n",
      "e bits  2.1651957035064697  m bits  4.343625545501709  scale  -0.025867942720651627\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  2.1725375652313232  m bits  4.3360276222229  scale  0.2504105865955353\n",
      "weight quant params: \n",
      "e bits  2.1651957035064697  m bits  4.3416523933410645  scale  0.10490868240594864\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  2.159172296524048  m bits  4.334719657897949  scale  0.24648255109786987\n",
      "weight quant params: \n",
      "e bits  2.1651973724365234  m bits  4.317821979522705  scale  0.540233314037323\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  2.166975736618042  m bits  4.347937107086182  scale  0.10852031409740448\n",
      "weight quant params: \n",
      "e bits  2.1651957035064697  m bits  4.3359527587890625  scale  0.028907479718327522\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  2.164973497390747  m bits  4.351500988006592  scale  0.2680489420890808\n",
      "weight quant params: \n",
      "e bits  2.1651957035064697  m bits  4.435254096984863  scale  0.2418471723794937\n",
      "Train set: Average loss: 0.7769\n",
      "Test set: Average loss: 1.2789, Accuracy: 0.824 (82.40%) bit penalty 5.934443950653076\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.6028023958206177  m bits  3.7291464805603027  scale  -0.7820444703102112\n",
      "weight quant params: \n",
      "e bits  1.5925617218017578  m bits  3.833061695098877  scale  -0.46307656168937683\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.9068858623504639  m bits  3.3242313861846924  scale  0.4798285961151123\n",
      "weight quant params: \n",
      "e bits  1.5925617218017578  m bits  3.314567804336548  scale  -0.01800207979977131\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5680299997329712  m bits  3.2004356384277344  scale  -0.07454602420330048\n",
      "weight quant params: \n",
      "e bits  1.5922942161560059  m bits  3.284971237182617  scale  0.06596296280622482\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.5965369939804077  m bits  3.2335493564605713  scale  0.045274198055267334\n",
      "weight quant params: \n",
      "e bits  1.592682957649231  m bits  3.376966953277588  scale  -0.5911228060722351\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  1.5898075103759766  m bits  3.240561008453369  scale  -0.22380296885967255\n",
      "weight quant params: \n",
      "e bits  1.5925617218017578  m bits  3.258261203765869  scale  -0.18879686295986176\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  1.594454288482666  m bits  3.229487419128418  scale  0.1373821645975113\n",
      "weight quant params: \n",
      "e bits  1.5925617218017578  m bits  3.2837512493133545  scale  -0.2986893653869629\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  1.5948388576507568  m bits  3.2121713161468506  scale  0.20145387947559357\n",
      "weight quant params: \n",
      "e bits  1.5925626754760742  m bits  3.276890277862549  scale  0.5806424021720886\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  1.5917500257492065  m bits  3.196735382080078  scale  0.10080022364854813\n",
      "weight quant params: \n",
      "e bits  1.5925617218017578  m bits  3.216554641723633  scale  -0.11938004195690155\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  1.592394232749939  m bits  3.208785057067871  scale  0.174601212143898\n",
      "weight quant params: \n",
      "e bits  1.5925617218017578  m bits  3.4294567108154297  scale  -0.1092144250869751\n",
      "Train set: Average loss: 0.6872\n",
      "Test set: Average loss: 1.2197, Accuracy: 0.8153 (81.53%) bit penalty 4.9732842445373535\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.2495472431182861  m bits  3.5618667602539062  scale  -1.2484018802642822\n",
      "weight quant params: \n",
      "e bits  1.2281572818756104  m bits  3.4683241844177246  scale  -0.7086146473884583\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.587888240814209  m bits  2.7316477298736572  scale  0.4983040690422058\n",
      "weight quant params: \n",
      "e bits  1.2281572818756104  m bits  2.5216031074523926  scale  -0.0012080823071300983\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.4990992546081543  m bits  2.4908649921417236  scale  0.15944504737854004\n",
      "weight quant params: \n",
      "e bits  1.2279009819030762  m bits  2.678539276123047  scale  -0.2383297234773636\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.3582230806350708  m bits  2.523421049118042  scale  0.2937587797641754\n",
      "weight quant params: \n",
      "e bits  1.2277929782867432  m bits  2.6384968757629395  scale  -0.8196071982383728\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  1.3623427152633667  m bits  2.5216352939605713  scale  0.15337586402893066\n",
      "weight quant params: \n",
      "e bits  1.2281572818756104  m bits  2.6965200901031494  scale  -0.5780353546142578\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  1.3595064878463745  m bits  2.5259926319122314  scale  0.5090819001197815\n",
      "weight quant params: \n",
      "e bits  1.2281572818756104  m bits  2.641775131225586  scale  -0.6470853090286255\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  1.3216965198516846  m bits  2.523496150970459  scale  0.5159256458282471\n",
      "weight quant params: \n",
      "e bits  1.2281575202941895  m bits  2.5837790966033936  scale  0.5216611623764038\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  1.2938591241836548  m bits  2.4825475215911865  scale  0.45032715797424316\n",
      "weight quant params: \n",
      "e bits  1.2281572818756104  m bits  2.535637855529785  scale  -0.40437954664230347\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  1.2298617362976074  m bits  2.4408302307128906  scale  0.2818650007247925\n",
      "weight quant params: \n",
      "e bits  1.2281572818756104  m bits  2.6373302936553955  scale  -0.15226677060127258\n",
      "Train set: Average loss: 0.6541\n",
      "Test set: Average loss: 1.0267, Accuracy: 0.8285 (82.85%) bit penalty 4.600342273712158\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.3526039123535156  m bits  3.9436357021331787  scale  -1.4975695610046387\n",
      "weight quant params: \n",
      "e bits  0.9806333184242249  m bits  3.6036109924316406  scale  -1.0363680124282837\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.536570429801941  m bits  2.5101571083068848  scale  0.4835578501224518\n",
      "weight quant params: \n",
      "e bits  0.9806333184242249  m bits  2.5038111209869385  scale  -0.3243740499019623\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.50652277469635  m bits  2.3026602268218994  scale  0.2857944667339325\n",
      "weight quant params: \n",
      "e bits  0.9930508136749268  m bits  2.4397928714752197  scale  -0.5037078857421875\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.3428925275802612  m bits  2.3375675678253174  scale  0.2977590262889862\n",
      "weight quant params: \n",
      "e bits  0.9804214239120483  m bits  2.373483657836914  scale  -1.3151077032089233\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  1.2898690700531006  m bits  2.190709352493286  scale  0.326858788728714\n",
      "weight quant params: \n",
      "e bits  0.9806333184242249  m bits  2.292877674102783  scale  -0.7891097068786621\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  1.2522386312484741  m bits  2.5040440559387207  scale  0.48941510915756226\n",
      "weight quant params: \n",
      "e bits  0.9806339740753174  m bits  2.18921160697937  scale  -0.8465593457221985\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  1.1582039594650269  m bits  2.3603460788726807  scale  0.49044883251190186\n",
      "weight quant params: \n",
      "e bits  0.9806333780288696  m bits  2.1923604011535645  scale  0.41494420170783997\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  1.1073716878890991  m bits  2.1385579109191895  scale  0.5077856779098511\n",
      "weight quant params: \n",
      "e bits  0.9806333184242249  m bits  2.091536045074463  scale  -0.7330374121665955\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.9833962321281433  m bits  1.9643374681472778  scale  0.12902866303920746\n",
      "weight quant params: \n",
      "e bits  0.9806333184242249  m bits  2.4998886585235596  scale  -0.3920131027698517\n",
      "Train set: Average loss: 0.6178\n",
      "Test set: Average loss: 1.0882, Accuracy: 0.8368 (83.68%) bit penalty 4.318634033203125\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5342496633529663  m bits  4.100637912750244  scale  -1.4873749017715454\n",
      "weight quant params: \n",
      "e bits  0.8040985465049744  m bits  3.4215474128723145  scale  -1.1986056566238403\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.575622797012329  m bits  2.5079941749572754  scale  0.48985525965690613\n",
      "weight quant params: \n",
      "e bits  0.8043484687805176  m bits  2.43449330329895  scale  -0.4974733293056488\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5081595182418823  m bits  2.057969331741333  scale  0.4326117932796478\n",
      "weight quant params: \n",
      "e bits  0.8362604975700378  m bits  2.291215419769287  scale  -0.49931854009628296\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.4359855651855469  m bits  2.0137178897857666  scale  0.48854315280914307\n",
      "weight quant params: \n",
      "e bits  0.8297985792160034  m bits  2.123814582824707  scale  -1.5011162757873535\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  1.2513139247894287  m bits  2.139414072036743  scale  0.4862808585166931\n",
      "weight quant params: \n",
      "e bits  0.8040985465049744  m bits  1.9969762563705444  scale  -1.0119564533233643\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  1.1471269130706787  m bits  2.4978113174438477  scale  0.49219539761543274\n",
      "weight quant params: \n",
      "e bits  0.8040734529495239  m bits  1.8534029722213745  scale  -1.042541742324829\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  1.0611605644226074  m bits  2.332960367202759  scale  0.5000364184379578\n",
      "weight quant params: \n",
      "e bits  0.8040984272956848  m bits  1.7829314470291138  scale  0.4606497883796692\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.9288748502731323  m bits  1.9841457605361938  scale  0.49872028827667236\n",
      "weight quant params: \n",
      "e bits  0.8040985465049744  m bits  1.7017813920974731  scale  -0.7681699991226196\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.8070037961006165  m bits  1.5589547157287598  scale  0.25899961590766907\n",
      "weight quant params: \n",
      "e bits  0.8040985465049744  m bits  2.3911759853363037  scale  -0.5017744898796082\n",
      "Train set: Average loss: 0.5830\n",
      "Test set: Average loss: 1.1994, Accuracy: 0.7989 (79.89%) bit penalty 4.1103692054748535\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.553541898727417  m bits  3.837801218032837  scale  -1.4703948497772217\n",
      "weight quant params: \n",
      "e bits  0.6733339428901672  m bits  3.305065155029297  scale  -1.297822117805481\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.6442979574203491  m bits  2.5425493717193604  scale  0.5028618574142456\n",
      "weight quant params: \n",
      "e bits  0.676325798034668  m bits  2.1647722721099854  scale  -0.4878084361553192\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5035724639892578  m bits  1.95287024974823  scale  0.4768235981464386\n",
      "weight quant params: \n",
      "e bits  0.7202593684196472  m bits  2.2428252696990967  scale  -0.5088797211647034\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.4925038814544678  m bits  2.18194842338562  scale  0.4997207522392273\n",
      "weight quant params: \n",
      "e bits  0.7167818546295166  m bits  1.8125001192092896  scale  -1.5069857835769653\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  1.2477142810821533  m bits  2.442451000213623  scale  0.48749539256095886\n",
      "weight quant params: \n",
      "e bits  0.6733596920967102  m bits  1.7576899528503418  scale  -1.137334942817688\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  1.0618538856506348  m bits  2.5269007682800293  scale  0.4999179542064667\n",
      "weight quant params: \n",
      "e bits  0.6734021902084351  m bits  1.6342720985412598  scale  -1.2948366403579712\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.9250156283378601  m bits  2.2047033309936523  scale  0.502275288105011\n",
      "weight quant params: \n",
      "e bits  0.6733338832855225  m bits  1.506681203842163  scale  0.48167139291763306\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.7888897061347961  m bits  1.756689429283142  scale  0.5048531889915466\n",
      "weight quant params: \n",
      "e bits  0.6733339428901672  m bits  1.4316076040267944  scale  -0.8575922846794128\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.675688624382019  m bits  1.4981708526611328  scale  -0.14199461042881012\n",
      "weight quant params: \n",
      "e bits  0.6733339428901672  m bits  2.1406073570251465  scale  -0.5331135392189026\n",
      "Train set: Average loss: 0.5529\n",
      "Test set: Average loss: 1.1184, Accuracy: 0.8205 (82.05%) bit penalty 3.980198860168457\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5283794403076172  m bits  3.8800129890441895  scale  -1.4788331985473633\n",
      "weight quant params: \n",
      "e bits  0.573497474193573  m bits  3.591069221496582  scale  -1.4654771089553833\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.6213845014572144  m bits  2.4343066215515137  scale  0.4926186203956604\n",
      "weight quant params: \n",
      "e bits  0.5766028761863708  m bits  1.910514235496521  scale  -0.49581536650657654\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5159056186676025  m bits  1.9131110906600952  scale  0.48247048258781433\n",
      "weight quant params: \n",
      "e bits  0.6291148066520691  m bits  2.2596371173858643  scale  -0.5044006109237671\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.4963438510894775  m bits  2.527536153793335  scale  0.5025674700737\n",
      "weight quant params: \n",
      "e bits  0.6615757346153259  m bits  1.6166640520095825  scale  -1.5044941902160645\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  1.1099932193756104  m bits  2.5123655796051025  scale  0.4972037076950073\n",
      "weight quant params: \n",
      "e bits  0.5732378959655762  m bits  1.5662142038345337  scale  -1.3427400588989258\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.9379547238349915  m bits  2.504124879837036  scale  0.49833744764328003\n",
      "weight quant params: \n",
      "e bits  0.578422486782074  m bits  1.4998323917388916  scale  -1.5212786197662354\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.7912893891334534  m bits  1.979103922843933  scale  0.5029817819595337\n",
      "weight quant params: \n",
      "e bits  0.5734974145889282  m bits  1.5063356161117554  scale  0.31850481033325195\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.6895424127578735  m bits  1.5890125036239624  scale  0.49822258949279785\n",
      "weight quant params: \n",
      "e bits  0.573497474193573  m bits  1.3489387035369873  scale  -1.3723844289779663\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.5751291513442993  m bits  1.4968483448028564  scale  -0.43005019426345825\n",
      "weight quant params: \n",
      "e bits  0.573497474193573  m bits  1.9290833473205566  scale  -0.5690193176269531\n",
      "Train set: Average loss: 0.5348\n",
      "Test set: Average loss: 0.8954, Accuracy: 0.8577 (85.77%) bit penalty 3.840113401412964\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5431575775146484  m bits  3.758491039276123  scale  -1.4853672981262207\n",
      "weight quant params: \n",
      "e bits  0.5558468103408813  m bits  3.547605037689209  scale  -1.4875149726867676\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.7482000589370728  m bits  2.506725788116455  scale  0.5022839903831482\n",
      "weight quant params: \n",
      "e bits  0.5046995878219604  m bits  1.7815699577331543  scale  -0.4917595684528351\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.521977424621582  m bits  1.9323227405548096  scale  0.46799910068511963\n",
      "weight quant params: \n",
      "e bits  0.5451348423957825  m bits  2.1845521926879883  scale  -0.49924492835998535\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.3947296142578125  m bits  2.5192177295684814  scale  0.4958389103412628\n",
      "weight quant params: \n",
      "e bits  0.6034091711044312  m bits  1.472726821899414  scale  -1.5104130506515503\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  1.0204986333847046  m bits  2.4414827823638916  scale  0.5035558938980103\n",
      "weight quant params: \n",
      "e bits  0.51120924949646  m bits  1.50088632106781  scale  -1.5189259052276611\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.8374797105789185  m bits  2.480311870574951  scale  0.5048733353614807\n",
      "weight quant params: \n",
      "e bits  0.5289815068244934  m bits  1.4392553567886353  scale  -1.5281620025634766\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.6733507513999939  m bits  1.723758339881897  scale  0.501787543296814\n",
      "weight quant params: \n",
      "e bits  0.49536967277526855  m bits  1.5034356117248535  scale  0.16022612154483795\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.6074689626693726  m bits  1.5300472974777222  scale  0.50444495677948\n",
      "weight quant params: \n",
      "e bits  0.49641260504722595  m bits  1.1923967599868774  scale  -1.550762414932251\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.5059568881988525  m bits  1.3524924516677856  scale  -0.5198575854301453\n",
      "weight quant params: \n",
      "e bits  0.49536678194999695  m bits  1.6655092239379883  scale  -0.527341365814209\n",
      "Train set: Average loss: 0.5178\n",
      "Test set: Average loss: 1.0103, Accuracy: 0.8457 (84.57%) bit penalty 3.8059446811676025\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5306081771850586  m bits  4.214463233947754  scale  -1.5272679328918457\n",
      "weight quant params: \n",
      "e bits  0.5452993512153625  m bits  3.5446510314941406  scale  -1.4964298009872437\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.7699145078659058  m bits  2.5264952182769775  scale  0.501439094543457\n",
      "weight quant params: \n",
      "e bits  0.5022731423377991  m bits  1.786747694015503  scale  -0.49181216955184937\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5063295364379883  m bits  1.90375816822052  scale  0.4838293194770813\n",
      "weight quant params: \n",
      "e bits  0.5031903982162476  m bits  2.2791237831115723  scale  -0.4958846867084503\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.3437317609786987  m bits  2.526200294494629  scale  0.5002731084823608\n",
      "weight quant params: \n",
      "e bits  0.5888174772262573  m bits  1.5099226236343384  scale  -1.5009135007858276\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.9278960227966309  m bits  2.4903781414031982  scale  0.5114337801933289\n",
      "weight quant params: \n",
      "e bits  0.5036205053329468  m bits  1.4820541143417358  scale  -1.50386381149292\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.7321585416793823  m bits  2.40193247795105  scale  0.49870312213897705\n",
      "weight quant params: \n",
      "e bits  0.5027067065238953  m bits  1.4165581464767456  scale  -1.5166161060333252\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5978634357452393  m bits  1.5598094463348389  scale  0.503207802772522\n",
      "weight quant params: \n",
      "e bits  0.4330112636089325  m bits  1.5119225978851318  scale  0.016158653423190117\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.5430143475532532  m bits  1.5402458906173706  scale  0.5043688416481018\n",
      "weight quant params: \n",
      "e bits  0.45696374773979187  m bits  1.059563398361206  scale  -1.4921550750732422\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.48803433775901794  m bits  1.310190200805664  scale  -0.480803519487381\n",
      "weight quant params: \n",
      "e bits  0.4330390393733978  m bits  1.5345135927200317  scale  -0.5760549306869507\n",
      "Train set: Average loss: 0.5311\n",
      "Test set: Average loss: 1.0466, Accuracy: 0.8326 (83.26%) bit penalty 3.7603366374969482\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.522307276725769  m bits  3.7963907718658447  scale  -1.5490283966064453\n",
      "weight quant params: \n",
      "e bits  0.5217884182929993  m bits  3.4849648475646973  scale  -1.4947813749313354\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.7609639167785645  m bits  2.5788841247558594  scale  0.4943957030773163\n",
      "weight quant params: \n",
      "e bits  0.5050039291381836  m bits  1.832442283630371  scale  -0.5047852396965027\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5055373907089233  m bits  1.9169102907180786  scale  0.4926723837852478\n",
      "weight quant params: \n",
      "e bits  0.5172029733657837  m bits  2.5333828926086426  scale  -0.48101383447647095\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.2766685485839844  m bits  2.505345582962036  scale  0.4961014688014984\n",
      "weight quant params: \n",
      "e bits  0.5976623892784119  m bits  1.5104968547821045  scale  -1.4936853647232056\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.8437149524688721  m bits  2.4910213947296143  scale  0.5192047357559204\n",
      "weight quant params: \n",
      "e bits  0.5077105760574341  m bits  1.5033429861068726  scale  -1.4986193180084229\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.6781690716743469  m bits  2.389970541000366  scale  0.5040745139122009\n",
      "weight quant params: \n",
      "e bits  0.4986141324043274  m bits  1.4508517980575562  scale  -1.4921395778656006\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5507158637046814  m bits  1.5135246515274048  scale  0.5143855214118958\n",
      "weight quant params: \n",
      "e bits  0.38239845633506775  m bits  1.505617618560791  scale  -0.08062253147363663\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.49668562412261963  m bits  1.5102258920669556  scale  0.5481887459754944\n",
      "weight quant params: \n",
      "e bits  0.4148285984992981  m bits  0.9659016728401184  scale  -1.45564866065979\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.46335259079933167  m bits  1.2687183618545532  scale  -0.42646512389183044\n",
      "weight quant params: \n",
      "e bits  0.3821271061897278  m bits  1.5026148557662964  scale  -0.772036075592041\n",
      "Train set: Average loss: 0.5204\n",
      "Test set: Average loss: 1.1103, Accuracy: 0.8257 (82.57%) bit penalty 3.727834463119507\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5003165006637573  m bits  3.4887356758117676  scale  -1.5516713857650757\n",
      "weight quant params: \n",
      "e bits  0.5295143723487854  m bits  3.6142818927764893  scale  -1.458436131477356\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.8264647722244263  m bits  2.5561630725860596  scale  0.5019617676734924\n",
      "weight quant params: \n",
      "e bits  0.5022672414779663  m bits  1.8366516828536987  scale  -0.500892162322998\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.511239767074585  m bits  1.9650306701660156  scale  0.4936337172985077\n",
      "weight quant params: \n",
      "e bits  0.4991777539253235  m bits  2.505249261856079  scale  -0.4914698898792267\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.3141601085662842  m bits  2.5117716789245605  scale  0.49910157918930054\n",
      "weight quant params: \n",
      "e bits  0.6192610859870911  m bits  1.5009233951568604  scale  -1.4996615648269653\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.7831013798713684  m bits  2.4868593215942383  scale  0.5207850933074951\n",
      "weight quant params: \n",
      "e bits  0.5021808743476868  m bits  1.5030105113983154  scale  -1.47622811794281\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.6198461055755615  m bits  2.3235645294189453  scale  0.5187978744506836\n",
      "weight quant params: \n",
      "e bits  0.5019781589508057  m bits  1.4967824220657349  scale  -1.500507116317749\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5094541907310486  m bits  1.5176165103912354  scale  0.5164029598236084\n",
      "weight quant params: \n",
      "e bits  0.3405211269855499  m bits  1.5068731307983398  scale  -0.2026112675666809\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.46732327342033386  m bits  1.497513771057129  scale  0.5652056336402893\n",
      "weight quant params: \n",
      "e bits  0.38471487164497375  m bits  0.8742898106575012  scale  -1.4865063428878784\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.42498111724853516  m bits  1.2371068000793457  scale  -0.4834233522415161\n",
      "weight quant params: \n",
      "e bits  0.3403767943382263  m bits  1.5017277002334595  scale  -0.976916491985321\n",
      "Train set: Average loss: 0.5136\n",
      "Test set: Average loss: 1.0996, Accuracy: 0.7937 (79.37%) bit penalty 3.704357147216797\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.4997572898864746  m bits  3.5595550537109375  scale  -1.5699427127838135\n",
      "weight quant params: \n",
      "e bits  0.5096523761749268  m bits  3.613412857055664  scale  -1.4969241619110107\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.8722994327545166  m bits  2.5135576725006104  scale  0.49815991520881653\n",
      "weight quant params: \n",
      "e bits  0.5371326804161072  m bits  1.9182411432266235  scale  -0.5108898878097534\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.536258578300476  m bits  2.1339757442474365  scale  0.4928770661354065\n",
      "weight quant params: \n",
      "e bits  0.5168594121932983  m bits  2.50870943069458  scale  -0.47637858986854553\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.3542135953903198  m bits  2.459310293197632  scale  0.499684602022171\n",
      "weight quant params: \n",
      "e bits  0.6370111703872681  m bits  1.5045958757400513  scale  -1.496161937713623\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.7067451477050781  m bits  2.497276544570923  scale  0.5056565999984741\n",
      "weight quant params: \n",
      "e bits  0.5152008533477783  m bits  1.504708170890808  scale  -1.4811203479766846\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5254021883010864  m bits  2.0081334114074707  scale  0.5310895442962646\n",
      "weight quant params: \n",
      "e bits  0.5201547145843506  m bits  1.483780026435852  scale  -1.499908685684204\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5003109574317932  m bits  1.4925036430358887  scale  0.6130480766296387\n",
      "weight quant params: \n",
      "e bits  0.30543825030326843  m bits  1.49871027469635  scale  -0.28199300169944763\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.44363903999328613  m bits  1.505586862564087  scale  0.6245906949043274\n",
      "weight quant params: \n",
      "e bits  0.3488156497478485  m bits  0.7860657572746277  scale  -1.4687429666519165\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.3877524733543396  m bits  1.1680086851119995  scale  -0.4506586790084839\n",
      "weight quant params: \n",
      "e bits  0.3052178621292114  m bits  1.5004446506500244  scale  -1.171050786972046\n",
      "Train set: Average loss: 0.5057\n",
      "Test set: Average loss: 0.9166, Accuracy: 0.8646 (86.46%) bit penalty 3.6805050373077393\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5102514028549194  m bits  3.49220871925354  scale  -1.5527421236038208\n",
      "weight quant params: \n",
      "e bits  0.5264581441879272  m bits  3.4827816486358643  scale  -1.4545485973358154\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.923336386680603  m bits  2.5012142658233643  scale  0.5006043314933777\n",
      "weight quant params: \n",
      "e bits  0.5001770257949829  m bits  1.8078516721725464  scale  -0.5011619925498962\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5141654014587402  m bits  2.2773406505584717  scale  0.4922620356082916\n",
      "weight quant params: \n",
      "e bits  0.4973074495792389  m bits  2.4972143173217773  scale  -0.4917048513889313\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.3799394369125366  m bits  2.5187418460845947  scale  0.49566254019737244\n",
      "weight quant params: \n",
      "e bits  0.6779960989952087  m bits  1.4607744216918945  scale  -1.5107648372650146\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.663649320602417  m bits  2.4882302284240723  scale  0.5033232569694519\n",
      "weight quant params: \n",
      "e bits  0.5018410682678223  m bits  1.515798568725586  scale  -1.4870802164077759\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5067646503448486  m bits  2.0298733711242676  scale  0.5369186997413635\n",
      "weight quant params: \n",
      "e bits  0.5241299271583557  m bits  1.4217145442962646  scale  -1.5015770196914673\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.4996805191040039  m bits  1.5090769529342651  scale  0.6726915836334229\n",
      "weight quant params: \n",
      "e bits  0.27596014738082886  m bits  1.5068720579147339  scale  -0.39169469475746155\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.4207771420478821  m bits  1.511474370956421  scale  0.6645721197128296\n",
      "weight quant params: \n",
      "e bits  0.3163486421108246  m bits  0.7100552320480347  scale  -1.4913957118988037\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.35706719756126404  m bits  1.1456469297409058  scale  -0.4562221169471741\n",
      "weight quant params: \n",
      "e bits  0.27577507495880127  m bits  1.500601887702942  scale  -1.340136170387268\n",
      "Train set: Average loss: 0.4801\n",
      "Test set: Average loss: 0.9755, Accuracy: 0.861 (86.10%) bit penalty 3.642594575881958\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.516526699066162  m bits  3.535569429397583  scale  -1.5472387075424194\n",
      "weight quant params: \n",
      "e bits  0.5000685453414917  m bits  3.4397003650665283  scale  -1.4224821329116821\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  1.9593920707702637  m bits  2.5088024139404297  scale  0.49842917919158936\n",
      "weight quant params: \n",
      "e bits  0.5016348958015442  m bits  1.839928150177002  scale  -0.497659832239151\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5067291259765625  m bits  2.210893392562866  scale  0.49975821375846863\n",
      "weight quant params: \n",
      "e bits  0.5016787648200989  m bits  2.5058321952819824  scale  -0.49156227707862854\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.3027489185333252  m bits  2.498464584350586  scale  0.5002772212028503\n",
      "weight quant params: \n",
      "e bits  0.764848530292511  m bits  1.502921462059021  scale  -1.5079443454742432\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5817158818244934  m bits  2.3405628204345703  scale  0.5009698271751404\n",
      "weight quant params: \n",
      "e bits  0.5014976263046265  m bits  1.508467674255371  scale  -1.5012048482894897\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.49905332922935486  m bits  1.8993031978607178  scale  0.6235957741737366\n",
      "weight quant params: \n",
      "e bits  0.5096079707145691  m bits  1.3381083011627197  scale  -1.4957982301712036\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5024682879447937  m bits  1.505509853363037  scale  0.7298110723495483\n",
      "weight quant params: \n",
      "e bits  0.2526826560497284  m bits  1.5086439847946167  scale  -0.5071516036987305\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.3948298394680023  m bits  1.5018579959869385  scale  0.71418696641922\n",
      "weight quant params: \n",
      "e bits  0.2938680350780487  m bits  0.6465468406677246  scale  -1.4580392837524414\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.329654723405838  m bits  1.1098815202713013  scale  -0.4992623031139374\n",
      "weight quant params: \n",
      "e bits  0.2505641281604767  m bits  1.4961446523666382  scale  -1.5188183784484863\n",
      "Train set: Average loss: 0.4850\n",
      "Test set: Average loss: 0.9289, Accuracy: 0.862 (86.20%) bit penalty 3.6371073722839355\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5376604795455933  m bits  3.623300075531006  scale  -1.5601532459259033\n",
      "weight quant params: \n",
      "e bits  0.5036922097206116  m bits  3.780588388442993  scale  -1.4496649503707886\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.029108762741089  m bits  2.5038650035858154  scale  0.4981939196586609\n",
      "weight quant params: \n",
      "e bits  0.5106996893882751  m bits  1.866106390953064  scale  -0.49640700221061707\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5099215507507324  m bits  2.37684965133667  scale  0.5011749267578125\n",
      "weight quant params: \n",
      "e bits  0.49389082193374634  m bits  2.509073257446289  scale  -0.4922733008861542\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.256027340888977  m bits  2.47849702835083  scale  0.5059760808944702\n",
      "weight quant params: \n",
      "e bits  0.835830807685852  m bits  1.4998936653137207  scale  -1.4993081092834473\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5218734741210938  m bits  2.1388490200042725  scale  0.5472067594528198\n",
      "weight quant params: \n",
      "e bits  0.528043270111084  m bits  1.504063367843628  scale  -1.4888352155685425\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5010401606559753  m bits  1.669313669204712  scale  0.7533131241798401\n",
      "weight quant params: \n",
      "e bits  0.507269024848938  m bits  1.347591519355774  scale  -1.4959903955459595\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.49860328435897827  m bits  1.5055296421051025  scale  0.7931032776832581\n",
      "weight quant params: \n",
      "e bits  0.2393965870141983  m bits  1.5205966234207153  scale  -0.5120797157287598\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.3725012540817261  m bits  1.502557396888733  scale  0.8068093657493591\n",
      "weight quant params: \n",
      "e bits  0.27267441153526306  m bits  0.598945677280426  scale  -1.423090934753418\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.31448137760162354  m bits  1.0251911878585815  scale  -0.46695443987846375\n",
      "weight quant params: \n",
      "e bits  0.23091427981853485  m bits  1.3534901142120361  scale  -1.633296251296997\n",
      "Train set: Average loss: 0.4707\n",
      "Test set: Average loss: 0.9687, Accuracy: 0.8484 (84.84%) bit penalty 3.6071884632110596\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.4974466562271118  m bits  3.4974873065948486  scale  -1.5993249416351318\n",
      "weight quant params: \n",
      "e bits  0.5014503002166748  m bits  3.855696439743042  scale  -1.4694015979766846\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.0943069458007812  m bits  2.524240016937256  scale  0.4967688024044037\n",
      "weight quant params: \n",
      "e bits  0.4892466962337494  m bits  1.8108586072921753  scale  -0.4964936673641205\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5061081647872925  m bits  2.4909169673919678  scale  0.5000026226043701\n",
      "weight quant params: \n",
      "e bits  0.49094706773757935  m bits  2.5061838626861572  scale  -0.48103392124176025\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.2651922702789307  m bits  2.505704402923584  scale  0.5069945454597473\n",
      "weight quant params: \n",
      "e bits  0.861911952495575  m bits  1.5018527507781982  scale  -1.4998950958251953\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5000917315483093  m bits  1.9956034421920776  scale  0.571791410446167\n",
      "weight quant params: \n",
      "e bits  0.5374491214752197  m bits  1.5085649490356445  scale  -1.5079782009124756\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5010630488395691  m bits  1.5191051959991455  scale  0.8784244656562805\n",
      "weight quant params: \n",
      "e bits  0.5053874254226685  m bits  1.3356176614761353  scale  -1.4935702085494995\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5007669925689697  m bits  1.5010756254196167  scale  0.8699250817298889\n",
      "weight quant params: \n",
      "e bits  0.22869142889976501  m bits  1.4983210563659668  scale  -0.5101492404937744\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.3544105291366577  m bits  1.5027775764465332  scale  0.9080311059951782\n",
      "weight quant params: \n",
      "e bits  0.24913817644119263  m bits  0.5501699447631836  scale  -1.4153416156768799\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.28826117515563965  m bits  0.9950464367866516  scale  -0.49668648838996887\n",
      "weight quant params: \n",
      "e bits  0.21506331861019135  m bits  1.2432317733764648  scale  -1.6927930116653442\n",
      "Train set: Average loss: 0.4553\n",
      "Test set: Average loss: 0.8675, Accuracy: 0.8739 (87.39%) bit penalty 3.552468776702881\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5091652870178223  m bits  3.5299606323242188  scale  -1.6169899702072144\n",
      "weight quant params: \n",
      "e bits  0.5012584924697876  m bits  3.4184682369232178  scale  -1.481411099433899\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.124993324279785  m bits  2.5007083415985107  scale  0.49808192253112793\n",
      "weight quant params: \n",
      "e bits  0.4927765727043152  m bits  1.7682465314865112  scale  -0.4894796907901764\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5063061714172363  m bits  2.452486276626587  scale  0.49954304099082947\n",
      "weight quant params: \n",
      "e bits  0.4997114837169647  m bits  2.516629934310913  scale  -0.4872835874557495\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.1882681846618652  m bits  2.4991025924682617  scale  0.5026013851165771\n",
      "weight quant params: \n",
      "e bits  0.8377047777175903  m bits  1.5017740726470947  scale  -1.4939234256744385\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.49962496757507324  m bits  1.816288709640503  scale  0.6580899953842163\n",
      "weight quant params: \n",
      "e bits  0.5795923471450806  m bits  1.5130892992019653  scale  -1.4942280054092407\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5019118785858154  m bits  1.504888892173767  scale  0.9233181476593018\n",
      "weight quant params: \n",
      "e bits  0.5063958764076233  m bits  1.3280844688415527  scale  -1.5080968141555786\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.49970006942749023  m bits  1.5006452798843384  scale  0.9251546859741211\n",
      "weight quant params: \n",
      "e bits  0.21842138469219208  m bits  1.465835452079773  scale  -0.507135808467865\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.3376995027065277  m bits  1.49739408493042  scale  0.9099684953689575\n",
      "weight quant params: \n",
      "e bits  0.2292465716600418  m bits  0.5073558688163757  scale  -1.3516842126846313\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.2696380317211151  m bits  0.9597831964492798  scale  -0.49618473649024963\n",
      "weight quant params: \n",
      "e bits  0.20125430822372437  m bits  1.160024642944336  scale  -1.73104989528656\n",
      "Train set: Average loss: 0.4530\n",
      "Test set: Average loss: 0.9201, Accuracy: 0.8637 (86.37%) bit penalty 3.5283665657043457\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.506379246711731  m bits  3.586449384689331  scale  -1.641869306564331\n",
      "weight quant params: \n",
      "e bits  0.5637798309326172  m bits  3.1880786418914795  scale  -1.4747096300125122\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.178736686706543  m bits  2.4596266746520996  scale  0.5043419003486633\n",
      "weight quant params: \n",
      "e bits  0.4614197909832001  m bits  1.6993443965911865  scale  -0.4780833125114441\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.4974287748336792  m bits  2.4892101287841797  scale  0.4981946051120758\n",
      "weight quant params: \n",
      "e bits  0.488540917634964  m bits  2.525892972946167  scale  -0.49779024720191956\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.1615631580352783  m bits  2.4953765869140625  scale  0.5020947456359863\n",
      "weight quant params: \n",
      "e bits  0.9217687249183655  m bits  1.5179493427276611  scale  -1.4991693496704102\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5022503137588501  m bits  1.6667420864105225  scale  0.7310119867324829\n",
      "weight quant params: \n",
      "e bits  0.573891818523407  m bits  1.5031557083129883  scale  -1.4891210794448853\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.4996577203273773  m bits  1.576731562614441  scale  0.9564082622528076\n",
      "weight quant params: \n",
      "e bits  0.5175352692604065  m bits  1.3271507024765015  scale  -1.500108003616333\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5003033876419067  m bits  1.5077331066131592  scale  0.983761727809906\n",
      "weight quant params: \n",
      "e bits  0.2096855640411377  m bits  1.441847324371338  scale  -0.5097448229789734\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.32244306802749634  m bits  1.5047916173934937  scale  0.9676396250724792\n",
      "weight quant params: \n",
      "e bits  0.21248312294483185  m bits  0.4990881383419037  scale  -1.500683069229126\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.25291964411735535  m bits  0.8968775272369385  scale  -0.4657677412033081\n",
      "weight quant params: \n",
      "e bits  0.18830883502960205  m bits  1.065456748008728  scale  -1.7931551933288574\n",
      "Train set: Average loss: 0.4430\n",
      "Test set: Average loss: 0.9012, Accuracy: 0.8701 (87.01%) bit penalty 3.4878437519073486\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5282078981399536  m bits  3.4892072677612305  scale  -1.6361067295074463\n",
      "weight quant params: \n",
      "e bits  0.5041490793228149  m bits  2.8560662269592285  scale  -1.4863674640655518\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.2669007778167725  m bits  2.5274860858917236  scale  0.4984181523323059\n",
      "weight quant params: \n",
      "e bits  0.44059041142463684  m bits  1.6161119937896729  scale  -0.49388113617897034\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.507697343826294  m bits  2.516371726989746  scale  0.49614202976226807\n",
      "weight quant params: \n",
      "e bits  0.5023474097251892  m bits  2.512136459350586  scale  -0.47783827781677246\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.1435006856918335  m bits  2.518340587615967  scale  0.5018084049224854\n",
      "weight quant params: \n",
      "e bits  0.9773560166358948  m bits  1.5032427310943604  scale  -1.5018426179885864\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5008813142776489  m bits  1.5899685621261597  scale  0.7863794565200806\n",
      "weight quant params: \n",
      "e bits  0.5541462302207947  m bits  1.5047404766082764  scale  -1.4979257583618164\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.49969395995140076  m bits  1.507054328918457  scale  1.0391045808792114\n",
      "weight quant params: \n",
      "e bits  0.5077199339866638  m bits  1.3077219724655151  scale  -1.5031712055206299\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.4994378387928009  m bits  1.4987646341323853  scale  1.0403437614440918\n",
      "weight quant params: \n",
      "e bits  0.2004225254058838  m bits  1.4447129964828491  scale  -0.5198919773101807\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.3070840835571289  m bits  1.501165509223938  scale  1.0403870344161987\n",
      "weight quant params: \n",
      "e bits  0.20451666414737701  m bits  0.5004931688308716  scale  -1.5048160552978516\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.23299790918827057  m bits  0.8416480422019958  scale  -0.47777724266052246\n",
      "weight quant params: \n",
      "e bits  0.17623326182365417  m bits  0.9920766353607178  scale  -1.8526228666305542\n",
      "Train set: Average loss: 0.4436\n",
      "Test set: Average loss: 1.0538, Accuracy: 0.8004 (80.04%) bit penalty 3.4845826625823975\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5271409749984741  m bits  3.5152390003204346  scale  -1.646089792251587\n",
      "weight quant params: \n",
      "e bits  0.6200469136238098  m bits  2.8419389724731445  scale  -1.486682415008545\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.3293817043304443  m bits  2.4989631175994873  scale  0.5025022625923157\n",
      "weight quant params: \n",
      "e bits  0.49954742193222046  m bits  1.5663727521896362  scale  -0.4352819621562958\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.515005350112915  m bits  2.480438709259033  scale  0.5004028081893921\n",
      "weight quant params: \n",
      "e bits  0.5179869532585144  m bits  2.505222797393799  scale  -0.45569974184036255\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.114011287689209  m bits  2.5083632469177246  scale  0.5105193257331848\n",
      "weight quant params: \n",
      "e bits  1.1762001514434814  m bits  1.5103625059127808  scale  -1.4946898221969604\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5042698979377747  m bits  1.497321605682373  scale  0.876909613609314\n",
      "weight quant params: \n",
      "e bits  0.5709070563316345  m bits  1.5034271478652954  scale  -1.4991285800933838\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.49964839220046997  m bits  1.5066684484481812  scale  1.0986037254333496\n",
      "weight quant params: \n",
      "e bits  0.5024150609970093  m bits  1.248113989830017  scale  -1.496062159538269\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.4999285042285919  m bits  1.4960919618606567  scale  1.1253539323806763\n",
      "weight quant params: \n",
      "e bits  0.1928253173828125  m bits  1.389673113822937  scale  -0.49957960844039917\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.2943146824836731  m bits  1.5085532665252686  scale  1.125399112701416\n",
      "weight quant params: \n",
      "e bits  0.195738285779953  m bits  0.4931882917881012  scale  -1.5028445720672607\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.2147839516401291  m bits  0.7882869839668274  scale  -0.4671250581741333\n",
      "weight quant params: \n",
      "e bits  0.16558928787708282  m bits  0.9245235323905945  scale  -1.9256738424301147\n",
      "Train set: Average loss: 0.4348\n",
      "Test set: Average loss: 1.5434, Accuracy: 0.7375 (73.75%) bit penalty 3.463874340057373\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5050724744796753  m bits  3.5233986377716064  scale  -1.6579447984695435\n",
      "weight quant params: \n",
      "e bits  0.6873226761817932  m bits  2.6024184226989746  scale  -1.4859594106674194\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.3809165954589844  m bits  2.514279842376709  scale  0.5012947916984558\n",
      "weight quant params: \n",
      "e bits  0.45005086064338684  m bits  1.4980981349945068  scale  -0.47554439306259155\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5233725309371948  m bits  2.506566286087036  scale  0.4999789595603943\n",
      "weight quant params: \n",
      "e bits  0.5231149196624756  m bits  2.4753448963165283  scale  -0.47561243176460266\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  1.057971477508545  m bits  2.5018677711486816  scale  0.5044974088668823\n",
      "weight quant params: \n",
      "e bits  1.259657621383667  m bits  1.4988948106765747  scale  -1.4963397979736328\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5022008419036865  m bits  1.5252090692520142  scale  0.9124710559844971\n",
      "weight quant params: \n",
      "e bits  0.5956084728240967  m bits  1.4993679523468018  scale  -1.484958529472351\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5014969706535339  m bits  1.5063267946243286  scale  1.1614688634872437\n",
      "weight quant params: \n",
      "e bits  0.512679934501648  m bits  1.1845757961273193  scale  -1.502140760421753\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.49849674105644226  m bits  1.4999111890792847  scale  1.1712019443511963\n",
      "weight quant params: \n",
      "e bits  0.18406571447849274  m bits  1.37101411819458  scale  -0.5003714561462402\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.2813132703304291  m bits  1.5160789489746094  scale  1.1783041954040527\n",
      "weight quant params: \n",
      "e bits  0.18785429000854492  m bits  0.499293714761734  scale  -1.4941266775131226\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.20175887644290924  m bits  0.7537281513214111  scale  -0.48064881563186646\n",
      "weight quant params: \n",
      "e bits  0.15561170876026154  m bits  0.8648021817207336  scale  -1.9683256149291992\n",
      "Train set: Average loss: 0.4292\n",
      "Test set: Average loss: 1.0255, Accuracy: 0.8587 (85.87%) bit penalty 3.4636716842651367\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5021753311157227  m bits  3.5130014419555664  scale  -1.634027123451233\n",
      "weight quant params: \n",
      "e bits  0.7032174468040466  m bits  2.5588927268981934  scale  -1.5007745027542114\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.4431817531585693  m bits  2.4865477085113525  scale  0.5019254088401794\n",
      "weight quant params: \n",
      "e bits  0.4470617175102234  m bits  1.5046809911727905  scale  -0.4986961781978607\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.53787362575531  m bits  2.5116019248962402  scale  0.502673327922821\n",
      "weight quant params: \n",
      "e bits  0.5208706259727478  m bits  2.5214319229125977  scale  -0.5045410394668579\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  0.9842550158500671  m bits  2.4962117671966553  scale  0.5040702819824219\n",
      "weight quant params: \n",
      "e bits  1.5051140785217285  m bits  1.505654215812683  scale  -1.498542070388794\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5054831504821777  m bits  1.5367131233215332  scale  0.969508945941925\n",
      "weight quant params: \n",
      "e bits  0.5835831165313721  m bits  1.4890438318252563  scale  -1.5012331008911133\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.49999740719795227  m bits  1.5212613344192505  scale  1.2124773263931274\n",
      "weight quant params: \n",
      "e bits  0.5029258728027344  m bits  1.1307684183120728  scale  -1.501720666885376\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5004345774650574  m bits  1.5073000192642212  scale  1.2299220561981201\n",
      "weight quant params: \n",
      "e bits  0.1762588620185852  m bits  1.3467851877212524  scale  -0.5093117356300354\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.2755270004272461  m bits  1.5024601221084595  scale  1.2838752269744873\n",
      "weight quant params: \n",
      "e bits  0.1801271289587021  m bits  0.5002491474151611  scale  -1.4975900650024414\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.1902909278869629  m bits  0.7023375630378723  scale  -0.48596665263175964\n",
      "weight quant params: \n",
      "e bits  0.14573508501052856  m bits  0.8070330619812012  scale  -2.050990581512451\n",
      "Train set: Average loss: 0.4356\n",
      "Test set: Average loss: 0.7952, Accuracy: 0.8914 (89.14%) bit penalty 3.4584438800811768\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5034430027008057  m bits  3.4748873710632324  scale  -1.6380152702331543\n",
      "weight quant params: \n",
      "e bits  0.8010558485984802  m bits  2.548734664916992  scale  -1.478249192237854\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.4652259349823  m bits  2.4848151206970215  scale  0.5005084872245789\n",
      "weight quant params: \n",
      "e bits  0.47404012084007263  m bits  1.5820499658584595  scale  -0.49501535296440125\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5034233331680298  m bits  2.5236847400665283  scale  0.49068349599838257\n",
      "weight quant params: \n",
      "e bits  0.4967779517173767  m bits  2.492331027984619  scale  -0.4935450553894043\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  0.9822019934654236  m bits  2.509514570236206  scale  0.5110184550285339\n",
      "weight quant params: \n",
      "e bits  1.4658644199371338  m bits  1.4913406372070312  scale  -1.5024511814117432\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5003378391265869  m bits  1.5629994869232178  scale  1.0168310403823853\n",
      "weight quant params: \n",
      "e bits  0.6351830363273621  m bits  1.4892785549163818  scale  -1.4950765371322632\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.4989643394947052  m bits  1.558297872543335  scale  1.2579175233840942\n",
      "weight quant params: \n",
      "e bits  0.5091730356216431  m bits  1.0769236087799072  scale  -1.4932966232299805\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.5002455115318298  m bits  1.5017715692520142  scale  1.2789793014526367\n",
      "weight quant params: \n",
      "e bits  0.17017832398414612  m bits  1.303515076637268  scale  -0.5030225515365601\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.26517966389656067  m bits  1.4986331462860107  scale  1.343472957611084\n",
      "weight quant params: \n",
      "e bits  0.17114326357841492  m bits  0.49540963768959045  scale  -1.5040082931518555\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.17748451232910156  m bits  0.6519561409950256  scale  -0.47504153847694397\n",
      "weight quant params: \n",
      "e bits  0.1363612711429596  m bits  0.7495548129081726  scale  -2.1367433071136475\n",
      "Train set: Average loss: 0.4268\n",
      "Test set: Average loss: 1.2354, Accuracy: 0.8058 (80.58%) bit penalty 3.4488725662231445\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5194896459579468  m bits  3.5091500282287598  scale  -1.644640564918518\n",
      "weight quant params: \n",
      "e bits  0.7914744019508362  m bits  2.3318123817443848  scale  -1.5000351667404175\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.5783631801605225  m bits  2.50978946685791  scale  0.528135359287262\n",
      "weight quant params: \n",
      "e bits  0.4912817180156708  m bits  1.5517202615737915  scale  -0.5026805996894836\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.6121867895126343  m bits  2.5296995639801025  scale  0.5617728233337402\n",
      "weight quant params: \n",
      "e bits  0.5026180744171143  m bits  2.448181390762329  scale  -0.4839175343513489\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  0.9287328720092773  m bits  2.491107940673828  scale  0.5083803534507751\n",
      "weight quant params: \n",
      "e bits  1.448472499847412  m bits  1.5008975267410278  scale  -1.5023821592330933\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5075878500938416  m bits  1.5556849241256714  scale  1.091617226600647\n",
      "weight quant params: \n",
      "e bits  0.7149313688278198  m bits  1.5085409879684448  scale  -1.4996776580810547\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5000641345977783  m bits  1.5000858306884766  scale  1.3377964496612549\n",
      "weight quant params: \n",
      "e bits  0.5100431442260742  m bits  1.058985710144043  scale  -1.5028271675109863\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.4996621906757355  m bits  1.5047056674957275  scale  1.3360310792922974\n",
      "weight quant params: \n",
      "e bits  0.16346679627895355  m bits  1.277273416519165  scale  -0.501061201095581\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.2555796205997467  m bits  1.4981447458267212  scale  1.415616750717163\n",
      "weight quant params: \n",
      "e bits  0.16398359835147858  m bits  0.4981827437877655  scale  -1.500923991203308\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.16733571887016296  m bits  0.614276647567749  scale  -0.47482284903526306\n",
      "weight quant params: \n",
      "e bits  0.12887577712535858  m bits  0.7073221206665039  scale  -2.1967551708221436\n",
      "Train set: Average loss: 0.4246\n",
      "Test set: Average loss: 0.7776, Accuracy: 0.8882 (88.82%) bit penalty 3.467447280883789\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5083739757537842  m bits  3.512061595916748  scale  -1.647770643234253\n",
      "weight quant params: \n",
      "e bits  1.1745303869247437  m bits  2.4347622394561768  scale  -1.4952433109283447\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.3918802738189697  m bits  2.4788503646850586  scale  0.49738186597824097\n",
      "weight quant params: \n",
      "e bits  0.5055252313613892  m bits  1.5521855354309082  scale  -0.5030871629714966\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5298153162002563  m bits  2.5148370265960693  scale  0.5200856924057007\n",
      "weight quant params: \n",
      "e bits  0.5010141134262085  m bits  2.5057475566864014  scale  -0.5028946399688721\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  0.9435298442840576  m bits  2.561352014541626  scale  0.5093909502029419\n",
      "weight quant params: \n",
      "e bits  1.4988892078399658  m bits  1.5053925514221191  scale  -1.495171070098877\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5000728368759155  m bits  1.5744118690490723  scale  1.094031572341919\n",
      "weight quant params: \n",
      "e bits  0.8027845621109009  m bits  1.4948413372039795  scale  -1.4913480281829834\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5005531311035156  m bits  1.543154001235962  scale  1.3663840293884277\n",
      "weight quant params: \n",
      "e bits  0.5063288807868958  m bits  1.039925456047058  scale  -1.503530502319336\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.498794287443161  m bits  1.4947125911712646  scale  1.3758690357208252\n",
      "weight quant params: \n",
      "e bits  0.15682566165924072  m bits  1.2528259754180908  scale  -0.5049245953559875\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.24574537575244904  m bits  1.5027118921279907  scale  1.4604727029800415\n",
      "weight quant params: \n",
      "e bits  0.15719102323055267  m bits  0.4994446039199829  scale  -1.5028400421142578\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.15700891613960266  m bits  0.5816302299499512  scale  -0.48027968406677246\n",
      "weight quant params: \n",
      "e bits  0.12182002514600754  m bits  0.6645177006721497  scale  -2.2746329307556152\n",
      "Train set: Average loss: 0.4166\n",
      "Test set: Average loss: 0.8768, Accuracy: 0.854 (85.40%) bit penalty 3.4532532691955566\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.501127004623413  m bits  3.493347644805908  scale  -1.652355432510376\n",
      "weight quant params: \n",
      "e bits  1.3722951412200928  m bits  2.485438108444214  scale  -1.4971262216567993\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.4611194133758545  m bits  2.4901633262634277  scale  0.5024211406707764\n",
      "weight quant params: \n",
      "e bits  0.4970838129520416  m bits  1.5035163164138794  scale  -0.4793618619441986\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.4856147766113281  m bits  2.4976959228515625  scale  0.5005017518997192\n",
      "weight quant params: \n",
      "e bits  0.5006964206695557  m bits  2.504082202911377  scale  -0.5021568536758423\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  0.9144054055213928  m bits  2.505002021789551  scale  0.5063789486885071\n",
      "weight quant params: \n",
      "e bits  1.336137056350708  m bits  1.5033645629882812  scale  -1.4815316200256348\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5019631385803223  m bits  1.515573501586914  scale  1.1553794145584106\n",
      "weight quant params: \n",
      "e bits  0.8620350360870361  m bits  1.5095540285110474  scale  -1.487611174583435\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.4999474287033081  m bits  1.5161795616149902  scale  1.4353123903274536\n",
      "weight quant params: \n",
      "e bits  0.4997042417526245  m bits  1.005638837814331  scale  -1.4951541423797607\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.49938997626304626  m bits  1.497975468635559  scale  1.4154387712478638\n",
      "weight quant params: \n",
      "e bits  0.150681734085083  m bits  1.2174497842788696  scale  -0.5057727694511414\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.2338826209306717  m bits  1.501587986946106  scale  1.4891413450241089\n",
      "weight quant params: \n",
      "e bits  0.1511249989271164  m bits  0.49633097648620605  scale  -1.4972083568572998\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.1488504707813263  m bits  0.5562036633491516  scale  -0.5018174052238464\n",
      "weight quant params: \n",
      "e bits  0.11562712490558624  m bits  0.6277691721916199  scale  -2.323192834854126\n",
      "Train set: Average loss: 0.4234\n",
      "Test set: Average loss: 0.9327, Accuracy: 0.8585 (85.85%) bit penalty 3.4813220500946045\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5014657974243164  m bits  3.4442226886749268  scale  -1.643243432044983\n",
      "weight quant params: \n",
      "e bits  1.6846610307693481  m bits  2.530590295791626  scale  -1.4984767436981201\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.429792881011963  m bits  2.501591205596924  scale  0.4957256317138672\n",
      "weight quant params: \n",
      "e bits  0.5266621708869934  m bits  1.575360655784607  scale  -0.4990657567977905\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5883468389511108  m bits  2.495349645614624  scale  0.5364105701446533\n",
      "weight quant params: \n",
      "e bits  0.5021262168884277  m bits  2.5022995471954346  scale  -0.5006143450737\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  0.8909603953361511  m bits  2.5257394313812256  scale  0.49919211864471436\n",
      "weight quant params: \n",
      "e bits  1.275658369064331  m bits  1.499601125717163  scale  -1.479809045791626\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5091310143470764  m bits  1.5263148546218872  scale  1.2046970129013062\n",
      "weight quant params: \n",
      "e bits  0.9065583348274231  m bits  1.4958149194717407  scale  -1.4888173341751099\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5008350610733032  m bits  1.5074166059494019  scale  1.4835236072540283\n",
      "weight quant params: \n",
      "e bits  0.5261543989181519  m bits  0.9683605432510376  scale  -1.5025969743728638\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.4984295070171356  m bits  1.498335838317871  scale  1.4886503219604492\n",
      "weight quant params: \n",
      "e bits  0.1441030353307724  m bits  1.1846325397491455  scale  -0.5058931112289429\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.22452345490455627  m bits  1.6953495740890503  scale  1.434580683708191\n",
      "weight quant params: \n",
      "e bits  0.1445857584476471  m bits  0.4898437559604645  scale  -1.4858860969543457\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.1416231244802475  m bits  0.5230680704116821  scale  -0.49759921431541443\n",
      "weight quant params: \n",
      "e bits  0.1095833033323288  m bits  0.5947052836418152  scale  -2.4014809131622314\n",
      "Train set: Average loss: 0.4205\n",
      "Test set: Average loss: 0.8897, Accuracy: 0.8707 (87.07%) bit penalty 3.4691357612609863\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e bits:  1.5003516674041748  m bits  3.5029635429382324  scale  -1.6440685987472534\n",
      "weight quant params: \n",
      "e bits  1.6729435920715332  m bits  2.526247501373291  scale  -1.4985932111740112\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e bits:  2.52649188041687  m bits  2.495384931564331  scale  0.5160328149795532\n",
      "weight quant params: \n",
      "e bits  0.486837774515152  m bits  1.519783854484558  scale  -0.49634847044944763\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e bits:  1.5048288106918335  m bits  2.4982831478118896  scale  0.4906873404979706\n",
      "weight quant params: \n",
      "e bits  0.5043928623199463  m bits  2.5016989707946777  scale  -0.5012281537055969\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e bits:  0.8746898174285889  m bits  2.478640079498291  scale  0.510052502155304\n",
      "weight quant params: \n",
      "e bits  1.3125321865081787  m bits  1.5257307291030884  scale  -1.4955945014953613\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e bits:  0.5007559061050415  m bits  1.5063343048095703  scale  1.254241943359375\n",
      "weight quant params: \n",
      "e bits  0.9172716736793518  m bits  1.5153324604034424  scale  -1.4988279342651367\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e bits:  0.5010169148445129  m bits  1.5368635654449463  scale  1.4958350658416748\n",
      "weight quant params: \n",
      "e bits  0.5460939407348633  m bits  0.9213621020317078  scale  -1.504022240638733\n",
      "module:  fc7\n",
      "input quant params: \n",
      "e bits:  0.49984559416770935  m bits  1.547384262084961  scale  1.501145362854004\n",
      "weight quant params: \n",
      "e bits  0.1386566162109375  m bits  1.1222527027130127  scale  -0.49963611364364624\n",
      "module:  fc8\n",
      "input quant params: \n",
      "e bits:  0.21217288076877594  m bits  1.6043211221694946  scale  1.4976855516433716\n",
      "weight quant params: \n",
      "e bits  0.13838334381580353  m bits  0.49035903811454773  scale  -1.487368106842041\n",
      "module:  fc9\n",
      "input quant params: \n",
      "e bits:  0.13755680620670319  m bits  0.5040663480758667  scale  -0.4846978187561035\n",
      "weight quant params: \n",
      "e bits  0.1043238416314125  m bits  0.568281888961792  scale  -2.4568581581115723\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"using divice \", device)\n",
    "\n",
    "QuantClass = QuantWrapperFloatingPoint\n",
    "\n",
    "base_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_base_model.pth\"\n",
    "best_accuracy_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_best_accuracy_model.pth\"\n",
    "less_bits_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_less_bits_model.pth\"\n",
    "\n",
    "load_model_path = None\n",
    "if(os.path.isfile(best_accuracy_model_path)):\n",
    "    load_model_path = best_accuracy_model_path\n",
    "elif(os.path.isfile(base_model_path)):\n",
    "    load_model_path = base_model_path   \n",
    "else:\n",
    "    best_accuracy_model_path = base_model_path\n",
    "    less_bits_model_path = base_model_path\n",
    "\n",
    "if(load_model_path):\n",
    "    # Create model\n",
    "    # model = SimpleQuantizedMLP(e_bits=4.0, m_bits=4.0, num_classes=len(classes)).to(device)\n",
    "    model = QuantSimpleCIFAR10Model(QuantClass, num_classes=len(classes), optimizeQuant=True).to(device)\n",
    "    #model = SimpleCIFAR10Model(num_classes=len(classes)).to(device)\n",
    "    model.load_state_dict(torch.load(load_model_path, weights_only=True))\n",
    "else:\n",
    "    model = QuantSimpleCIFAR10Model(QuantClass, num_classes=len(classes), optimizeQuant=False).to(device)\n",
    "\n",
    "#criterion = SqrHingeLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = label_smoothing_loss\n",
    "\n",
    "bit_width_criterion = bitwidth_sum\n",
    "\n",
    "# Create optimizer (SGD or Adam)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)  # Adjusted Cosine Annealing with warm-up strategy\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_penalty_bw = 100000.0\n",
    "# Train for some epochs\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, bit_width_criterion, epoch)\n",
    "    if scheduler != None:\n",
    "        scheduler.step()\n",
    "    accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "    penalty_bw = bitwidth_sum(model)\n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), best_accuracy_model_path)\n",
    "    if(penalty_bw < best_penalty_bw):\n",
    "        best_penalty_bw = penalty_bw\n",
    "        torch.save(model.state_dict(), less_bits_model_path)\n",
    "    printBitWidths(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(base_model_path, weights_only=True))\n",
    "base_accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "base_penalty_bw = bitwidth_round_sum(model).detach().item()\n",
    "model.load_state_dict(torch.load(best_accuracy_model_path, weights_only=True))\n",
    "best_accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "best_accuracy_penalty_bw = bitwidth_round_sum(model).detach().item()\n",
    "model.load_state_dict(torch.load(less_bits_model_path, weights_only=True))\n",
    "lest_bits_accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "less_bits_penalty_bw = bitwidth_round_sum(model).detach().item()\n",
    "print(\"base model accuracy: \", base_accuracy, \" penalty bw \", base_penalty_bw)\n",
    "print(\"best accuracy model accuracy: \", best_accuracy, \" penalty bw \", best_accuracy_penalty_bw)\n",
    "print(\"less bits model accuracy: \", lest_bits_accuracy, \" penalty bw \", less_bits_penalty_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_bits_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_best_accuracy_model.pth\"\n",
    "state_dict = torch.load(best_accuracy_model_path, weights_only=True)\n",
    "state_dict_numpy = {}\n",
    "for key in state_dict:\n",
    "    #print(f\"{key}: {type(state_dict[key])}\")\n",
    "    state_dict_numpy[key] = state_dict[key].cpu().detach().numpy().tolist()\n",
    "    print(key)\n",
    "    print(state_dict_numpy[key])\n",
    "    #with open(\"mnist_cnn.pkl\", \"wb\") as f:\n",
    "    #    pickle.dump(state_dict_numpy, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
