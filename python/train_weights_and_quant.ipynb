{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "\"\"\"\n",
    "# 1) MNIST Dataset & Dataloaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "input_size = (1, 28, 28)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 2) CIFAR-10 dataset\n",
    "train_transform = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "])\n",
    "\n",
    "# Transformations for testing: just convert and normalize\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "input_size = (3, 32, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9894737..2.1264887].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: a batch of images in shape (B, C, H, W) or a single image in (C, H, W).\n",
    "    \"\"\"\n",
    "    # If it's a batch of images (4D), make a grid first\n",
    "    if len(img_tensor.shape) == 4:\n",
    "        img_tensor = torchvision.utils.make_grid(img_tensor)\n",
    "    # Unnormalize\n",
    "    #img_tensor = unnormalize(img_tensor)\n",
    "    # Convert to numpy\n",
    "    npimg = img_tensor.numpy()\n",
    "    # Transpose from (C, H, W) to (H, W, C)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_bit(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(x)\n",
    "\n",
    "def bit_to_param(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(x)\n",
    "\n",
    "def fake_float_truncate(x: torch.Tensor, e_bits: int, m_bits: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate 'float' with e_bits exponent bits and m_bits mantissa bits.\n",
    "    Simplified approach: unbiased exponent in integer range + truncated mantissa.\n",
    "    \"\"\"\n",
    "    eps = 1e-45\n",
    "    abs_x = x.abs().clamp(min=eps)\n",
    "    sign = x.sign()\n",
    "    \n",
    "    # exponent\n",
    "    e = torch.floor(torch.log2(abs_x))\n",
    "    min_e = -(2**(e_bits)) + 1\n",
    "    max_e =  (2**(e_bits)) - 1\n",
    "    e_clamped = torch.clamp(e, min_e, max_e)\n",
    "    \n",
    "    # fraction in [1,2) if x >= eps\n",
    "    frac = abs_x / (2.0 ** e_clamped)\n",
    "    \n",
    "    # truncate mantissa\n",
    "    scale = 2.0 ** m_bits\n",
    "    frac_trunc = torch.floor(frac * scale) / scale\n",
    "    \n",
    "    return sign * (2.0 ** e_clamped) * frac_trunc\n",
    "\n",
    "\n",
    "class FakeFloatFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e_bits_param, m_bits_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, e_bits_param, m_bits_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).clamp(min=0.0).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).clamp(min=1.0).item())\n",
    "        \n",
    "        out = fake_float_truncate(x, e_bits_int, m_bits_int)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, e_bits_param, m_bits_param = ctx.saved_tensors\n",
    "        \n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).clamp(min=0.0).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).clamp(min=1.0).item())\n",
    "        \n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output.clone()\n",
    "        \n",
    "        # 2) Gradient wrt e_bits: approximate with central difference\n",
    "        grad_e_bits = None\n",
    "        if e_bits_param.requires_grad:\n",
    "            delta = 0.01\n",
    "            e_plus2_int  = int(torch.round(param_to_bit(e_bits_param + 2*delta)).clamp(min=0.0).item())\n",
    "            e_plus_int   = int(torch.round(param_to_bit(e_bits_param +   delta)).clamp(min=0.0).item())\n",
    "            e_minus_int  = int(torch.round(param_to_bit(e_bits_param -   delta)).clamp(min=0.0).item())\n",
    "            e_minus2_int = int(torch.round(param_to_bit(e_bits_param - 2*delta)).clamp(min=0.0).item())\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_plus2_int,  m_bits_int)\n",
    "            f_plus   = fake_float_truncate(x, e_plus_int,   m_bits_int)\n",
    "            f_minus  = fake_float_truncate(x, e_minus_int,  m_bits_int)\n",
    "            f_minus2 = fake_float_truncate(x, e_minus2_int, m_bits_int)\n",
    "            \n",
    "            #diff_e = (f_plus - f_minus) * grad_output\n",
    "            #grad_e_bits = diff_e.sum() / (2.0 * delta)\n",
    "            \n",
    "            diff_e = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) * grad_output\n",
    "            grad_e_bits = diff_e.sum() / (12.0 * delta)\n",
    "        \n",
    "        # 3) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_m_bits = None\n",
    "        if m_bits_param.requires_grad:\n",
    "            delta = 0.01\n",
    "            m_plus2_int  = int(torch.round(param_to_bit(m_bits_param + 2*delta)).clamp(min=1.0).item())\n",
    "            m_plus_int   = int(torch.round(param_to_bit(m_bits_param +   delta)).clamp(min=1.0).item())\n",
    "            m_minus_int  = int(torch.round(param_to_bit(m_bits_param -   delta)).clamp(min=1.0).item())\n",
    "            m_minus2_int = int(torch.round(param_to_bit(m_bits_param - 2*delta)).clamp(min=1.0).item())\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_bits_int, m_plus2_int)\n",
    "            f_plus   = fake_float_truncate(x, e_bits_int, m_plus_int)\n",
    "            f_minus  = fake_float_truncate(x, e_bits_int, m_minus_int)\n",
    "            f_minus2 = fake_float_truncate(x, e_bits_int, m_minus2_int)\n",
    "            \n",
    "            #diff_e = (f_plus - f_minus) * grad_output\n",
    "            #grad_e_bits = diff_e.sum() / (2.0 * delta)\n",
    "            \n",
    "            diff_e = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) * grad_output\n",
    "            grad_m_bits = diff_e.sum() / (12.0 * delta)\n",
    "        \n",
    "        return grad_x, grad_e_bits, grad_m_bits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * input_size[1]//4 * input_size[2]//4, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleCIFAR10Model2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * input_size[1]//4 * input_size[2]//4, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SimpleQuantizedMLP(nn.Module):\n",
    "    def __init__(self, e_bits=4.0, m_bits=4.0, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Now we make them trainable:\n",
    "        self.e_bits = nn.Parameter(torch.tensor(e_bits))\n",
    "        self.m_bits = nn.Parameter(torch.tensor(m_bits))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        w1 = FakeFloatFunction.apply(self.fc1.weight, self.e_bits, self.m_bits)\n",
    "        b1 = FakeFloatFunction.apply(self.fc1.bias,   self.e_bits, self.m_bits)\n",
    "        x  = F.relu(F.linear(x, w1, b1))\n",
    "\n",
    "        w2 = FakeFloatFunction.apply(self.fc2.weight, self.e_bits, self.m_bits)\n",
    "        b2 = FakeFloatFunction.apply(self.fc2.bias,   self.e_bits, self.m_bits)\n",
    "        x  = F.linear(x, w2, b2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SimpleQuantizedCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_size[0],  out_channels=32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * input_size[1]//4 * input_size[2]//4, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        learnInput = True\n",
    "        learnInter = False\n",
    "        inputsE = 8.0\n",
    "        inputsM = 23.0\n",
    "        learnConvBias = False\n",
    "        learnConvWeights = True\n",
    "        convE = 8.0\n",
    "        convM = 23.0\n",
    "        learnLineBias = True\n",
    "        learnLineWeights = True\n",
    "        lineE = 8.0\n",
    "        lineM = 23.0\n",
    "        \n",
    "        self.i_e_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInput),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsE)), requires_grad=learnInter),\n",
    "        ])\n",
    "        \n",
    "        self.i_m_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInput),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(inputsM)), requires_grad=learnInter),\n",
    "        ])\n",
    "        \n",
    "        self.w_e_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvWeights),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineWeights),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineWeights),\n",
    "        ])\n",
    "        \n",
    "        self.w_m_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvWeights),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvWeights),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineWeights),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineWeights),\n",
    "        ])\n",
    "          \n",
    "        self.b_e_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvBias),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convE)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineBias),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineE)), requires_grad=learnLineBias),\n",
    "        ])\n",
    "        \n",
    "        self.b_m_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvBias),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(convM)), requires_grad=learnConvBias),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineBias),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(lineM)), requires_grad=learnLineBias),\n",
    "        ])\n",
    "              \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = FakeFloatFunction.apply(x, self.i_e_bits_param[0], self.i_m_bits_param[0])\n",
    "        w1 = FakeFloatFunction.apply(self.conv1.weight, self.w_e_bits_param[0], self.w_m_bits_param[0])\n",
    "        b1 = FakeFloatFunction.apply(self.conv1.bias,   self.b_e_bits_param[0], self.b_m_bits_param[0]) if self.conv1.bias is not None else None\n",
    "        x = F.conv2d(x, w1, b1, stride=1, padding=1)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)  # 32x32 -> 16x16\n",
    "        \n",
    "        #x = FakeFloatFunction.apply(x, self.i_e_bits_param[1], self.i_m_bits_param[1])\n",
    "        w2 = FakeFloatFunction.apply(self.conv2.weight, self.w_e_bits_param[1], self.w_m_bits_param[1])\n",
    "        b2 = FakeFloatFunction.apply(self.conv2.bias,   self.b_e_bits_param[1], self.b_m_bits_param[1]) if self.conv2.bias is not None else None\n",
    "        x = F.conv2d(x, w2, b2, stride=1, padding=1)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)  # 16x16 -> 8x8\n",
    "        \n",
    "        x  = x.view(x.size(0), -1)\n",
    "        \n",
    "        #x = FakeFloatFunction.apply(x, self.i_e_bits_param[2], self.i_m_bits_param[2])\n",
    "        w_fc1 = FakeFloatFunction.apply(self.fc1.weight, self.w_e_bits_param[2], self.w_m_bits_param[2])\n",
    "        b_fc1 = FakeFloatFunction.apply(self.fc1.bias,   self.b_e_bits_param[2], self.b_m_bits_param[2])\n",
    "        x = F.linear(x, w_fc1, b_fc1)\n",
    "        #x = F.dropout(x)\n",
    "\n",
    "        #x = FakeFloatFunction.apply(x, self.i_e_bits_param[3], self.i_m_bits_param[3])\n",
    "        w_fc2 = FakeFloatFunction.apply(self.fc2.weight, self.w_e_bits_param[3], self.w_m_bits_param[3])\n",
    "        b_fc2 = FakeFloatFunction.apply(self.fc2.bias,   self.b_e_bits_param[3], self.b_m_bits_param[3])\n",
    "        x  = F.linear(x, w_fc2, b_fc2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def printBitWidths(self):\n",
    "        for i, (eb, mb) in enumerate(zip(self.i_e_bits_param, self.i_m_bits_param)):\n",
    "            print(f\"Layer {i} input e_bits (float) = {param_to_bit(eb).item()},  m_bits (float) = {param_to_bit(mb).item()}\")\n",
    "        for i, (eb, mb) in enumerate(zip(self.w_e_bits_param, self.w_m_bits_param)):\n",
    "            print(f\"Layer {i} weight e_bits (float) = {param_to_bit(eb).item()},  m_bits (float) = {param_to_bit(mb).item()}\")\n",
    "        for i, (eb, mb) in enumerate(zip(self.b_e_bits_param, self.b_m_bits_param)):\n",
    "            print(f\"Layer {i} bias e_bits (float) = {param_to_bit(eb).item()},  m_bits (float) = {param_to_bit(mb).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwidth_sum(model):\n",
    "    \"\"\"\n",
    "    Computes a penalty term for the bitwidth parameters in 'model'.\n",
    "    'lambda_bw' is the weight/scale for this regularization.\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "    \n",
    "    # If the model has multiple layers with e_bits and m_bits in a ParameterList:\n",
    "    for eb, mb in zip(model.w_e_bits_param, model.w_m_bits_param):\n",
    "        # Option A: Penalize the raw float value (the \"continuous\" version)\n",
    "        penalty += eb + mb\n",
    "        \n",
    "        # Option B (alternative): Penalize the rounded integer version\n",
    "        # penalty += torch.round(eb) + torch.round(mb)\n",
    "    \n",
    "    for eb, mb in zip(model.b_e_bits_param, model.b_m_bits_param):\n",
    "        penalty += eb + mb\n",
    "\n",
    "    for eb, mb in zip(model.i_e_bits_param, model.i_m_bits_param):\n",
    "        penalty += eb + mb\n",
    "                    \n",
    "    return penalty\n",
    "\n",
    "\n",
    "def bitwidth_squared(model):\n",
    "    \"\"\"\n",
    "    Computes a penalty term for the bitwidth parameters in 'model'.\n",
    "    'lambda_bw' is the weight/scale for this regularization.\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "    \n",
    "    # If the model has multiple layers with e_bits and m_bits in a ParameterList:\n",
    "    for eb, mb in zip(model.w_e_bits_param, model.w_m_bits_param):\n",
    "        # Option A: Penalize the raw float value (the \"continuous\" version)\n",
    "        \n",
    "        eb_ = eb - bit_to_param(torch.tensor(0.1))\n",
    "        mb_ = mb - bit_to_param(torch.tensor(1.0))\n",
    "        \n",
    "        penalty += eb_*eb_ + mb_*mb_\n",
    "        \n",
    "        # Option B (alternative): Penalize the rounded integer version\n",
    "        # penalty += torch.round(eb) + torch.round(mb)\n",
    "    \n",
    "    for eb, mb in zip(model.b_e_bits_param, model.b_m_bits_param):\n",
    "        eb_ = eb - bit_to_param(torch.tensor(0.1))\n",
    "        mb_ = mb - bit_to_param(torch.tensor(1.0))\n",
    "        penalty += eb_*eb_ + mb_*mb_\n",
    "\n",
    "    for eb, mb in zip(model.i_e_bits_param, model.i_m_bits_param):\n",
    "        eb_ = eb - bit_to_param(torch.tensor(0.1))\n",
    "        mb_ = mb - bit_to_param(torch.tensor(1.0))\n",
    "        penalty += eb_*eb_ + mb_*mb_\n",
    "                    \n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, lambda_bw=1.0e-3):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_ce = F.cross_entropy(output, target)\n",
    "        penalty_bw = bitwidth_squared(model) \n",
    "        #penalty_bw = bitwidth_sum(model) \n",
    "        loss = loss_ce + lambda_bw*penalty_bw\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #if batch_idx % 200 == 0:\n",
    "        #    print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "        #          f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Train set: Average loss: {train_loss:.4f}\")\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy} ({100.0*accuracy:.2f}%)\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using divice  cuda\n",
      "Train set: Average loss: 2.0584\n",
      "Test set: Average loss: 1.3179, Accuracy: 0.5279 (52.79%)\n",
      "Layer 0 input e_bits (float) = 6.587833881378174,  m_bits (float) = 18.95368194580078\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 6.587832927703857,  m_bits (float) = 18.953723907470703\n",
      "Layer 1 weight e_bits (float) = 6.587832927703857,  m_bits (float) = 18.953678131103516\n",
      "Layer 2 weight e_bits (float) = 6.587835788726807,  m_bits (float) = 18.95358657836914\n",
      "Layer 3 weight e_bits (float) = 6.587831497192383,  m_bits (float) = 18.95359230041504\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 6.587832927703857,  m_bits (float) = 18.95366859436035\n",
      "Layer 3 bias e_bits (float) = 6.587832927703857,  m_bits (float) = 18.953664779663086\n",
      "Train set: Average loss: 1.6044\n",
      "Test set: Average loss: 1.2226, Accuracy: 0.5583 (55.83%)\n",
      "Layer 0 input e_bits (float) = 5.447665691375732,  m_bits (float) = 15.711761474609375\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 5.447664737701416,  m_bits (float) = 15.711532592773438\n",
      "Layer 1 weight e_bits (float) = 5.447664737701416,  m_bits (float) = 15.711746215820312\n",
      "Layer 2 weight e_bits (float) = 5.447666168212891,  m_bits (float) = 15.711761474609375\n",
      "Layer 3 weight e_bits (float) = 5.447663307189941,  m_bits (float) = 15.711727142333984\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 5.447664737701416,  m_bits (float) = 15.711697578430176\n",
      "Layer 3 bias e_bits (float) = 5.447664737701416,  m_bits (float) = 15.711705207824707\n",
      "Train set: Average loss: 1.4791\n",
      "Test set: Average loss: 1.1182, Accuracy: 0.6034 (60.34%)\n",
      "Layer 0 input e_bits (float) = 4.523100852966309,  m_bits (float) = 13.097283363342285\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 4.523099899291992,  m_bits (float) = 13.100034713745117\n",
      "Layer 1 weight e_bits (float) = 4.523099899291992,  m_bits (float) = 13.099575996398926\n",
      "Layer 2 weight e_bits (float) = 4.523101329803467,  m_bits (float) = 13.096327781677246\n",
      "Layer 3 weight e_bits (float) = 4.523098945617676,  m_bits (float) = 13.095812797546387\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 4.523099899291992,  m_bits (float) = 13.099820137023926\n",
      "Layer 3 bias e_bits (float) = 4.523099899291992,  m_bits (float) = 13.09988784790039\n",
      "Train set: Average loss: 1.3925\n",
      "Test set: Average loss: 1.1359, Accuracy: 0.6029 (60.29%)\n",
      "Layer 0 input e_bits (float) = 3.7701263427734375,  m_bits (float) = 10.979955673217773\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 3.770125389099121,  m_bits (float) = 10.98687744140625\n",
      "Layer 1 weight e_bits (float) = 3.770125389099121,  m_bits (float) = 10.987896919250488\n",
      "Layer 2 weight e_bits (float) = 3.7701268196105957,  m_bits (float) = 10.979257583618164\n",
      "Layer 3 weight e_bits (float) = 3.7701244354248047,  m_bits (float) = 10.976762771606445\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 3.770125389099121,  m_bits (float) = 10.984004020690918\n",
      "Layer 3 bias e_bits (float) = 3.770125389099121,  m_bits (float) = 10.984161376953125\n",
      "Train set: Average loss: 1.3482\n",
      "Test set: Average loss: 1.1777, Accuracy: 0.5959 (59.59%)\n",
      "Layer 0 input e_bits (float) = 3.1543726921081543,  m_bits (float) = 9.25932788848877\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 3.1542279720306396,  m_bits (float) = 9.239302635192871\n",
      "Layer 1 weight e_bits (float) = 3.1543939113616943,  m_bits (float) = 9.265233039855957\n",
      "Layer 2 weight e_bits (float) = 3.154261350631714,  m_bits (float) = 9.231375694274902\n",
      "Layer 3 weight e_bits (float) = 3.154404640197754,  m_bits (float) = 9.237014770507812\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 3.1543657779693604,  m_bits (float) = 9.261096954345703\n",
      "Layer 3 bias e_bits (float) = 3.15437912940979,  m_bits (float) = 9.261160850524902\n",
      "Train set: Average loss: 1.2887\n",
      "Test set: Average loss: 1.0347, Accuracy: 0.6414 (64.14%)\n",
      "Layer 0 input e_bits (float) = 2.648864269256592,  m_bits (float) = 7.875419616699219\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 2.648777484893799,  m_bits (float) = 7.856775760650635\n",
      "Layer 1 weight e_bits (float) = 2.648876428604126,  m_bits (float) = 7.848485946655273\n",
      "Layer 2 weight e_bits (float) = 2.648800849914551,  m_bits (float) = 7.815317153930664\n",
      "Layer 3 weight e_bits (float) = 2.648883104324341,  m_bits (float) = 7.833269119262695\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 2.648860216140747,  m_bits (float) = 7.851519584655762\n",
      "Layer 3 bias e_bits (float) = 2.6488683223724365,  m_bits (float) = 7.850495338439941\n",
      "Train set: Average loss: 1.2535\n",
      "Test set: Average loss: 1.1347, Accuracy: 0.6011 (60.11%)\n",
      "Layer 0 input e_bits (float) = 2.2329788208007812,  m_bits (float) = 6.839914798736572\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 2.4320266246795654,  m_bits (float) = 6.898474216461182\n",
      "Layer 1 weight e_bits (float) = 2.2676947116851807,  m_bits (float) = 6.7993292808532715\n",
      "Layer 2 weight e_bits (float) = 2.296034574508667,  m_bits (float) = 6.687506675720215\n",
      "Layer 3 weight e_bits (float) = 2.3756115436553955,  m_bits (float) = 6.695425510406494\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 2.2291252613067627,  m_bits (float) = 6.693471908569336\n",
      "Layer 3 bias e_bits (float) = 2.232048749923706,  m_bits (float) = 6.695337772369385\n",
      "Train set: Average loss: 1.2066\n",
      "Test set: Average loss: 0.9853, Accuracy: 0.6606 (66.06%)\n",
      "Layer 0 input e_bits (float) = 1.8883178234100342,  m_bits (float) = 5.8520050048828125\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 2.2863521575927734,  m_bits (float) = 6.533267021179199\n",
      "Layer 1 weight e_bits (float) = 1.9764589071273804,  m_bits (float) = 6.026110649108887\n",
      "Layer 2 weight e_bits (float) = 2.1090641021728516,  m_bits (float) = 5.921092510223389\n",
      "Layer 3 weight e_bits (float) = 2.1636574268341064,  m_bits (float) = 6.592126846313477\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 1.885894775390625,  m_bits (float) = 5.741148471832275\n",
      "Layer 3 bias e_bits (float) = 1.8879729509353638,  m_bits (float) = 5.732186317443848\n",
      "Train set: Average loss: 1.1857\n",
      "Test set: Average loss: 0.9980, Accuracy: 0.6533 (65.33%)\n",
      "Layer 0 input e_bits (float) = 1.602376937866211,  m_bits (float) = 5.539247512817383\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 2.137441635131836,  m_bits (float) = 6.572101593017578\n",
      "Layer 1 weight e_bits (float) = 1.7188408374786377,  m_bits (float) = 5.594918251037598\n",
      "Layer 2 weight e_bits (float) = 1.9253891706466675,  m_bits (float) = 5.657121181488037\n",
      "Layer 3 weight e_bits (float) = 1.9587647914886475,  m_bits (float) = 6.283398628234863\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 1.600876808166504,  m_bits (float) = 4.937490940093994\n",
      "Layer 3 bias e_bits (float) = 1.6023756265640259,  m_bits (float) = 4.95758581161499\n",
      "Train set: Average loss: 1.1570\n",
      "Test set: Average loss: 1.0308, Accuracy: 0.644 (64.40%)\n",
      "Layer 0 input e_bits (float) = 1.3702032566070557,  m_bits (float) = 5.56372594833374\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.9869985580444336,  m_bits (float) = 6.35008430480957\n",
      "Layer 1 weight e_bits (float) = 1.4954426288604736,  m_bits (float) = 5.584531784057617\n",
      "Layer 2 weight e_bits (float) = 1.747905969619751,  m_bits (float) = 5.35973596572876\n",
      "Layer 3 weight e_bits (float) = 1.7638747692108154,  m_bits (float) = 6.054164409637451\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 1.3704204559326172,  m_bits (float) = 4.23740291595459\n",
      "Layer 3 bias e_bits (float) = 1.3597100973129272,  m_bits (float) = 4.406076431274414\n",
      "Train set: Average loss: 1.1364\n",
      "Test set: Average loss: 1.0591, Accuracy: 0.6352 (63.52%)\n",
      "Layer 0 input e_bits (float) = 1.1774604320526123,  m_bits (float) = 5.564550876617432\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.8371224403381348,  m_bits (float) = 6.234721660614014\n",
      "Layer 1 weight e_bits (float) = 1.393754005432129,  m_bits (float) = 5.594911098480225\n",
      "Layer 2 weight e_bits (float) = 1.578872561454773,  m_bits (float) = 5.303995609283447\n",
      "Layer 3 weight e_bits (float) = 1.5810908079147339,  m_bits (float) = 5.813354015350342\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 1.1841790676116943,  m_bits (float) = 3.7701826095581055\n",
      "Layer 3 bias e_bits (float) = 1.1849949359893799,  m_bits (float) = 4.090372085571289\n",
      "Train set: Average loss: 1.1280\n",
      "Test set: Average loss: 1.0091, Accuracy: 0.6482 (64.82%)\n",
      "Layer 0 input e_bits (float) = 1.014117956161499,  m_bits (float) = 5.566299915313721\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.6897369623184204,  m_bits (float) = 6.112581253051758\n",
      "Layer 1 weight e_bits (float) = 1.3212270736694336,  m_bits (float) = 5.577563285827637\n",
      "Layer 2 weight e_bits (float) = 1.499606966972351,  m_bits (float) = 5.242860794067383\n",
      "Layer 3 weight e_bits (float) = 1.5547672510147095,  m_bits (float) = 5.567401885986328\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 1.0245269536972046,  m_bits (float) = 3.3790669441223145\n",
      "Layer 3 bias e_bits (float) = 1.0325945615768433,  m_bits (float) = 3.7873640060424805\n",
      "Train set: Average loss: 1.0953\n",
      "Test set: Average loss: 0.9978, Accuracy: 0.6584 (65.84%)\n",
      "Layer 0 input e_bits (float) = 0.8756292462348938,  m_bits (float) = 5.5460052490234375\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5465514659881592,  m_bits (float) = 5.980932712554932\n",
      "Layer 1 weight e_bits (float) = 1.2470425367355347,  m_bits (float) = 5.565874099731445\n",
      "Layer 2 weight e_bits (float) = 1.500016450881958,  m_bits (float) = 5.176074504852295\n",
      "Layer 3 weight e_bits (float) = 1.529982328414917,  m_bits (float) = 5.56178617477417\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.8878758549690247,  m_bits (float) = 3.057640314102173\n",
      "Layer 3 bias e_bits (float) = 0.9002137184143066,  m_bits (float) = 3.4017536640167236\n",
      "Train set: Average loss: 1.0927\n",
      "Test set: Average loss: 0.9553, Accuracy: 0.6692 (66.92%)\n",
      "Layer 0 input e_bits (float) = 0.7581204175949097,  m_bits (float) = 5.558096408843994\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5312446355819702,  m_bits (float) = 5.839822769165039\n",
      "Layer 1 weight e_bits (float) = 1.1719021797180176,  m_bits (float) = 5.597293853759766\n",
      "Layer 2 weight e_bits (float) = 1.5002402067184448,  m_bits (float) = 5.103418827056885\n",
      "Layer 3 weight e_bits (float) = 1.5255032777786255,  m_bits (float) = 5.601352691650391\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.7710079550743103,  m_bits (float) = 2.7694175243377686\n",
      "Layer 3 bias e_bits (float) = 0.7855756282806396,  m_bits (float) = 3.2190728187561035\n",
      "Train set: Average loss: 1.0729\n",
      "Test set: Average loss: 0.9872, Accuracy: 0.6593 (65.93%)\n",
      "Layer 0 input e_bits (float) = 0.6583146452903748,  m_bits (float) = 5.556321620941162\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.530161738395691,  m_bits (float) = 5.689377784729004\n",
      "Layer 1 weight e_bits (float) = 1.0965222120285034,  m_bits (float) = 5.586284160614014\n",
      "Layer 2 weight e_bits (float) = 1.500058650970459,  m_bits (float) = 5.024687767028809\n",
      "Layer 3 weight e_bits (float) = 1.520904302597046,  m_bits (float) = 5.570754528045654\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.6710904836654663,  m_bits (float) = 2.512655735015869\n",
      "Layer 3 bias e_bits (float) = 0.6865221261978149,  m_bits (float) = 3.0452637672424316\n",
      "Train set: Average loss: 1.0617\n",
      "Test set: Average loss: 0.9189, Accuracy: 0.6832 (68.32%)\n",
      "Layer 0 input e_bits (float) = 0.5734506249427795,  m_bits (float) = 5.5700273513793945\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5185739994049072,  m_bits (float) = 5.550002574920654\n",
      "Layer 1 weight e_bits (float) = 1.021615982055664,  m_bits (float) = 5.56295108795166\n",
      "Layer 2 weight e_bits (float) = 1.5000722408294678,  m_bits (float) = 4.939706802368164\n",
      "Layer 3 weight e_bits (float) = 1.5238065719604492,  m_bits (float) = 5.631641864776611\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.5856617093086243,  m_bits (float) = 2.2852230072021484\n",
      "Layer 3 bias e_bits (float) = 0.6010680198669434,  m_bits (float) = 2.873645305633545\n",
      "Train set: Average loss: 1.0467\n",
      "Test set: Average loss: 0.9355, Accuracy: 0.677 (67.70%)\n",
      "Layer 0 input e_bits (float) = 0.505940318107605,  m_bits (float) = 5.587037086486816\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.523788332939148,  m_bits (float) = 5.6116437911987305\n",
      "Layer 1 weight e_bits (float) = 0.9478753209114075,  m_bits (float) = 5.555177688598633\n",
      "Layer 2 weight e_bits (float) = 1.499776840209961,  m_bits (float) = 4.848319053649902\n",
      "Layer 3 weight e_bits (float) = 1.5230213403701782,  m_bits (float) = 5.593889236450195\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.5125994682312012,  m_bits (float) = 2.0848212242126465\n",
      "Layer 3 bias e_bits (float) = 0.5274250507354736,  m_bits (float) = 2.7057156562805176\n",
      "Train set: Average loss: 1.0355\n",
      "Test set: Average loss: 0.8996, Accuracy: 0.6837 (68.37%)\n",
      "Layer 0 input e_bits (float) = 0.45532166957855225,  m_bits (float) = 5.539098739624023\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5332986116409302,  m_bits (float) = 5.263969898223877\n",
      "Layer 1 weight e_bits (float) = 0.8759505152702332,  m_bits (float) = 5.5911078453063965\n",
      "Layer 2 weight e_bits (float) = 1.4997972249984741,  m_bits (float) = 4.750420093536377\n",
      "Layer 3 weight e_bits (float) = 1.5194693803787231,  m_bits (float) = 5.564596176147461\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.45008644461631775,  m_bits (float) = 1.909117579460144\n",
      "Layer 3 bias e_bits (float) = 0.4640038013458252,  m_bits (float) = 2.5428428649902344\n",
      "Train set: Average loss: 1.0217\n",
      "Test set: Average loss: 0.9490, Accuracy: 0.6733 (67.33%)\n",
      "Layer 0 input e_bits (float) = 0.4200262427330017,  m_bits (float) = 5.488427639007568\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5370032787322998,  m_bits (float) = 5.174269676208496\n",
      "Layer 1 weight e_bits (float) = 0.8064311742782593,  m_bits (float) = 5.594886302947998\n",
      "Layer 2 weight e_bits (float) = 1.5000944137573242,  m_bits (float) = 4.645958423614502\n",
      "Layer 3 weight e_bits (float) = 1.5038567781448364,  m_bits (float) = 5.656426429748535\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.3965722620487213,  m_bits (float) = 1.7558366060256958\n",
      "Layer 3 bias e_bits (float) = 0.4094098210334778,  m_bits (float) = 2.3862411975860596\n",
      "Train set: Average loss: 1.0117\n",
      "Test set: Average loss: 0.9480, Accuracy: 0.6793 (67.93%)\n",
      "Layer 0 input e_bits (float) = 0.3866364657878876,  m_bits (float) = 5.555401802062988\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5224406719207764,  m_bits (float) = 5.0780463218688965\n",
      "Layer 1 weight e_bits (float) = 0.7398303747177124,  m_bits (float) = 5.5782623291015625\n",
      "Layer 2 weight e_bits (float) = 1.4994237422943115,  m_bits (float) = 4.597620964050293\n",
      "Layer 3 weight e_bits (float) = 1.5092068910598755,  m_bits (float) = 5.607411861419678\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.35073843598365784,  m_bits (float) = 1.6228175163269043\n",
      "Layer 3 bias e_bits (float) = 0.36242735385894775,  m_bits (float) = 2.236948013305664\n",
      "Train set: Average loss: 1.0129\n",
      "Test set: Average loss: 0.9165, Accuracy: 0.6835 (68.35%)\n",
      "Layer 0 input e_bits (float) = 0.35530102252960205,  m_bits (float) = 5.548519611358643\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.530643343925476,  m_bits (float) = 4.975115776062012\n",
      "Layer 1 weight e_bits (float) = 0.6765720248222351,  m_bits (float) = 5.611246109008789\n",
      "Layer 2 weight e_bits (float) = 1.4994717836380005,  m_bits (float) = 4.188969135284424\n",
      "Layer 3 weight e_bits (float) = 1.499080777168274,  m_bits (float) = 5.620309829711914\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.3114657998085022,  m_bits (float) = 1.508051872253418\n",
      "Layer 3 bias e_bits (float) = 0.3220037519931793,  m_bits (float) = 2.0958070755004883\n",
      "Train set: Average loss: 1.0018\n",
      "Test set: Average loss: 0.8751, Accuracy: 0.6988 (69.88%)\n",
      "Layer 0 input e_bits (float) = 0.3261164724826813,  m_bits (float) = 5.531668663024902\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5309741497039795,  m_bits (float) = 4.865400791168213\n",
      "Layer 1 weight e_bits (float) = 0.6169840693473816,  m_bits (float) = 5.616114616394043\n",
      "Layer 2 weight e_bits (float) = 1.4999428987503052,  m_bits (float) = 4.149353504180908\n",
      "Layer 3 weight e_bits (float) = 1.5007829666137695,  m_bits (float) = 5.572948455810547\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.2778053879737854,  m_bits (float) = 1.4096933603286743\n",
      "Layer 3 bias e_bits (float) = 0.2872316539287567,  m_bits (float) = 1.9634665250778198\n",
      "Train set: Average loss: 0.9953\n",
      "Test set: Average loss: 0.9383, Accuracy: 0.6766 (67.66%)\n",
      "Layer 0 input e_bits (float) = 0.29913026094436646,  m_bits (float) = 5.499086856842041\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.530287265777588,  m_bits (float) = 4.748894214630127\n",
      "Layer 1 weight e_bits (float) = 0.5612956881523132,  m_bits (float) = 5.613061428070068\n",
      "Layer 2 weight e_bits (float) = 1.4998372793197632,  m_bits (float) = 4.106712818145752\n",
      "Layer 3 weight e_bits (float) = 1.5004907846450806,  m_bits (float) = 5.6320037841796875\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.24895481765270233,  m_bits (float) = 1.3260573148727417\n",
      "Layer 3 bias e_bits (float) = 0.25733140110969543,  m_bits (float) = 1.840386152267456\n",
      "Train set: Average loss: 0.9866\n",
      "Test set: Average loss: 0.8886, Accuracy: 0.694 (69.40%)\n",
      "Layer 0 input e_bits (float) = 0.2743464410305023,  m_bits (float) = 5.523932933807373\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.531846523284912,  m_bits (float) = 4.625667095184326\n",
      "Layer 1 weight e_bits (float) = 0.509712278842926,  m_bits (float) = 5.615819931030273\n",
      "Layer 2 weight e_bits (float) = 1.5008389949798584,  m_bits (float) = 4.060503005981445\n",
      "Layer 3 weight e_bits (float) = 1.499787449836731,  m_bits (float) = 5.57094669342041\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.22423392534255981,  m_bits (float) = 1.2556082010269165\n",
      "Layer 3 bias e_bits (float) = 0.231636181473732,  m_bits (float) = 1.7268412113189697\n",
      "Train set: Average loss: 0.9765\n",
      "Test set: Average loss: 0.9314, Accuracy: 0.6817 (68.17%)\n",
      "Layer 0 input e_bits (float) = 0.25173255801200867,  m_bits (float) = 5.547841548919678\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5294196605682373,  m_bits (float) = 4.581658840179443\n",
      "Layer 1 weight e_bits (float) = 0.5102882981300354,  m_bits (float) = 5.607366561889648\n",
      "Layer 2 weight e_bits (float) = 1.5003429651260376,  m_bits (float) = 4.0105299949646\n",
      "Layer 3 weight e_bits (float) = 1.5043201446533203,  m_bits (float) = 5.569723129272461\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.20306746661663055,  m_bits (float) = 1.1969401836395264\n",
      "Layer 3 bias e_bits (float) = 0.20957481861114502,  m_bits (float) = 1.6229475736618042\n",
      "Train set: Average loss: 0.9636\n",
      "Test set: Average loss: 1.0100, Accuracy: 0.6611 (66.11%)\n",
      "Layer 0 input e_bits (float) = 0.23122510313987732,  m_bits (float) = 5.523348331451416\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5313619375228882,  m_bits (float) = 4.6074113845825195\n",
      "Layer 1 weight e_bits (float) = 0.5058525800704956,  m_bits (float) = 5.529270648956299\n",
      "Layer 2 weight e_bits (float) = 1.5003548860549927,  m_bits (float) = 3.9566051959991455\n",
      "Layer 3 weight e_bits (float) = 1.499984860420227,  m_bits (float) = 5.591798305511475\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1849689483642578,  m_bits (float) = 1.1487537622451782\n",
      "Layer 3 bias e_bits (float) = 0.19066032767295837,  m_bits (float) = 1.52867591381073\n",
      "Train set: Average loss: 0.9580\n",
      "Test set: Average loss: 0.8567, Accuracy: 0.7037 (70.37%)\n",
      "Layer 0 input e_bits (float) = 0.2127380073070526,  m_bits (float) = 5.572571754455566\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5261383056640625,  m_bits (float) = 4.506804466247559\n",
      "Layer 1 weight e_bits (float) = 0.5062537789344788,  m_bits (float) = 5.568821430206299\n",
      "Layer 2 weight e_bits (float) = 1.499879002571106,  m_bits (float) = 3.898552894592285\n",
      "Layer 3 weight e_bits (float) = 1.501786708831787,  m_bits (float) = 5.579438209533691\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.16952601075172424,  m_bits (float) = 1.1098304986953735\n",
      "Layer 3 bias e_bits (float) = 0.17447739839553833,  m_bits (float) = 1.4438772201538086\n",
      "Train set: Average loss: 0.9526\n",
      "Test set: Average loss: 0.9405, Accuracy: 0.689 (68.90%)\n",
      "Layer 0 input e_bits (float) = 0.1961684226989746,  m_bits (float) = 5.551960468292236\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5372827053070068,  m_bits (float) = 4.566934585571289\n",
      "Layer 1 weight e_bits (float) = 0.505449116230011,  m_bits (float) = 5.560480117797852\n",
      "Layer 2 weight e_bits (float) = 1.5006977319717407,  m_bits (float) = 3.8362197875976562\n",
      "Layer 3 weight e_bits (float) = 1.5001205205917358,  m_bits (float) = 5.534930229187012\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.15638960897922516,  m_bits (float) = 1.0790122747421265\n",
      "Layer 3 bias e_bits (float) = 0.16067184507846832,  m_bits (float) = 1.3683011531829834\n",
      "Train set: Average loss: 0.9434\n",
      "Test set: Average loss: 0.8741, Accuracy: 0.6932 (69.32%)\n",
      "Layer 0 input e_bits (float) = 0.18140330910682678,  m_bits (float) = 5.560853004455566\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5311883687973022,  m_bits (float) = 4.562622547149658\n",
      "Layer 1 weight e_bits (float) = 0.5054559707641602,  m_bits (float) = 5.581435680389404\n",
      "Layer 2 weight e_bits (float) = 1.5006059408187866,  m_bits (float) = 3.769468069076538\n",
      "Layer 3 weight e_bits (float) = 1.5038014650344849,  m_bits (float) = 5.5803608894348145\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.14526303112506866,  m_bits (float) = 1.0551841259002686\n",
      "Layer 3 bias e_bits (float) = 0.1489415168762207,  m_bits (float) = 1.3016166687011719\n",
      "Train set: Average loss: 0.9371\n",
      "Test set: Average loss: 0.8841, Accuracy: 0.6961 (69.61%)\n",
      "Layer 0 input e_bits (float) = 0.16832400858402252,  m_bits (float) = 5.555856227874756\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5325900316238403,  m_bits (float) = 4.550405025482178\n",
      "Layer 1 weight e_bits (float) = 0.5096213221549988,  m_bits (float) = 5.594705581665039\n",
      "Layer 2 weight e_bits (float) = 1.500469446182251,  m_bits (float) = 3.6982028484344482\n",
      "Layer 3 weight e_bits (float) = 1.49772047996521,  m_bits (float) = 5.536283493041992\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.13589294254779816,  m_bits (float) = 1.0372685194015503\n",
      "Layer 3 bias e_bits (float) = 0.1390271931886673,  m_bits (float) = 1.2434250116348267\n",
      "Train set: Average loss: 0.9314\n",
      "Test set: Average loss: 0.8726, Accuracy: 0.705 (70.50%)\n",
      "Layer 0 input e_bits (float) = 0.15681040287017822,  m_bits (float) = 5.567115306854248\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5342859029769897,  m_bits (float) = 4.585724353790283\n",
      "Layer 1 weight e_bits (float) = 0.5071699023246765,  m_bits (float) = 5.516392707824707\n",
      "Layer 2 weight e_bits (float) = 1.5004452466964722,  m_bits (float) = 3.6223626136779785\n",
      "Layer 3 weight e_bits (float) = 1.500683307647705,  m_bits (float) = 5.581704616546631\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.12806177139282227,  m_bits (float) = 1.0242308378219604\n",
      "Layer 3 bias e_bits (float) = 0.13070626556873322,  m_bits (float) = 1.1932722330093384\n",
      "Train set: Average loss: 0.9308\n",
      "Test set: Average loss: 0.8860, Accuracy: 0.703 (70.30%)\n",
      "Layer 0 input e_bits (float) = 0.14674419164657593,  m_bits (float) = 5.518477439880371\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.530997633934021,  m_bits (float) = 4.562549114227295\n",
      "Layer 1 weight e_bits (float) = 0.509915292263031,  m_bits (float) = 5.588248252868652\n",
      "Layer 2 weight e_bits (float) = 1.503068447113037,  m_bits (float) = 3.5256006717681885\n",
      "Layer 3 weight e_bits (float) = 1.4997209310531616,  m_bits (float) = 5.563824653625488\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.12158035486936569,  m_bits (float) = 1.0150933265686035\n",
      "Layer 3 bias e_bits (float) = 0.12378526479005814,  m_bits (float) = 1.1506551504135132\n",
      "Train set: Average loss: 0.9203\n",
      "Test set: Average loss: 0.8500, Accuracy: 0.7153 (71.53%)\n",
      "Layer 0 input e_bits (float) = 0.1380099356174469,  m_bits (float) = 5.528656005859375\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5471845865249634,  m_bits (float) = 4.5440592765808105\n",
      "Layer 1 weight e_bits (float) = 0.5103842616081238,  m_bits (float) = 5.5662336349487305\n",
      "Layer 2 weight e_bits (float) = 1.5006768703460693,  m_bits (float) = 3.591860055923462\n",
      "Layer 3 weight e_bits (float) = 1.5005264282226562,  m_bits (float) = 5.526778697967529\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.11628179997205734,  m_bits (float) = 1.0089598894119263\n",
      "Layer 3 bias e_bits (float) = 0.11809395253658295,  m_bits (float) = 1.115025281906128\n",
      "Train set: Average loss: 0.9068\n",
      "Test set: Average loss: 0.8455, Accuracy: 0.718 (71.80%)\n",
      "Layer 0 input e_bits (float) = 0.130497545003891,  m_bits (float) = 5.493128776550293\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5313808917999268,  m_bits (float) = 4.550124168395996\n",
      "Layer 1 weight e_bits (float) = 0.5074378848075867,  m_bits (float) = 5.559218406677246\n",
      "Layer 2 weight e_bits (float) = 1.50044584274292,  m_bits (float) = 3.58872389793396\n",
      "Layer 3 weight e_bits (float) = 1.5004318952560425,  m_bits (float) = 5.565008163452148\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.11201639473438263,  m_bits (float) = 1.0050395727157593\n",
      "Layer 3 bias e_bits (float) = 0.11347991973161697,  m_bits (float) = 1.0857911109924316\n",
      "Train set: Average loss: 0.9097\n",
      "Test set: Average loss: 0.8459, Accuracy: 0.7168 (71.68%)\n",
      "Layer 0 input e_bits (float) = 0.12410163134336472,  m_bits (float) = 5.535275459289551\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5300230979919434,  m_bits (float) = 4.597644805908203\n",
      "Layer 1 weight e_bits (float) = 0.506615400314331,  m_bits (float) = 5.562227249145508\n",
      "Layer 2 weight e_bits (float) = 1.5007188320159912,  m_bits (float) = 3.583956003189087\n",
      "Layer 3 weight e_bits (float) = 1.5007981061935425,  m_bits (float) = 5.5683746337890625\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1086464524269104,  m_bits (float) = 1.0026687383651733\n",
      "Layer 3 bias e_bits (float) = 0.10980410873889923,  m_bits (float) = 1.0623196363449097\n",
      "Train set: Average loss: 0.9086\n",
      "Test set: Average loss: 0.8349, Accuracy: 0.7106 (71.06%)\n",
      "Layer 0 input e_bits (float) = 0.11872106045484543,  m_bits (float) = 5.499581336975098\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5242302417755127,  m_bits (float) = 4.577418804168701\n",
      "Layer 1 weight e_bits (float) = 0.5020825862884521,  m_bits (float) = 5.557028293609619\n",
      "Layer 2 weight e_bits (float) = 1.4997682571411133,  m_bits (float) = 3.5787081718444824\n",
      "Layer 3 weight e_bits (float) = 1.4980050325393677,  m_bits (float) = 5.616410732269287\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10604391992092133,  m_bits (float) = 1.0013213157653809\n",
      "Layer 3 bias e_bits (float) = 0.10693700611591339,  m_bits (float) = 1.0439414978027344\n",
      "Train set: Average loss: 0.9011\n",
      "Test set: Average loss: 0.8890, Accuracy: 0.7036 (70.36%)\n",
      "Layer 0 input e_bits (float) = 0.11425859481096268,  m_bits (float) = 5.5390706062316895\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5260579586029053,  m_bits (float) = 4.567613124847412\n",
      "Layer 1 weight e_bits (float) = 0.5052098631858826,  m_bits (float) = 5.557910919189453\n",
      "Layer 2 weight e_bits (float) = 1.5006697177886963,  m_bits (float) = 3.57293701171875\n",
      "Layer 3 weight e_bits (float) = 1.4988818168640137,  m_bits (float) = 5.590597152709961\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1040874496102333,  m_bits (float) = 1.0006067752838135\n",
      "Layer 3 bias e_bits (float) = 0.1047566756606102,  m_bits (float) = 1.029961347579956\n",
      "Train set: Average loss: 0.8900\n",
      "Test set: Average loss: 0.8150, Accuracy: 0.7223 (72.23%)\n",
      "Layer 0 input e_bits (float) = 0.11061947047710419,  m_bits (float) = 5.559057712554932\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.53646981716156,  m_bits (float) = 4.57512903213501\n",
      "Layer 1 weight e_bits (float) = 0.5051134824752808,  m_bits (float) = 5.557173728942871\n",
      "Layer 2 weight e_bits (float) = 1.4999046325683594,  m_bits (float) = 3.5292322635650635\n",
      "Layer 3 weight e_bits (float) = 1.5016573667526245,  m_bits (float) = 5.517952919006348\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10266265273094177,  m_bits (float) = 1.0002564191818237\n",
      "Layer 3 bias e_bits (float) = 0.10314728319644928,  m_bits (float) = 1.0196731090545654\n",
      "Train set: Average loss: 0.8941\n",
      "Test set: Average loss: 0.8441, Accuracy: 0.7198 (71.98%)\n",
      "Layer 0 input e_bits (float) = 0.1077103465795517,  m_bits (float) = 5.519683361053467\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5225518941879272,  m_bits (float) = 4.591479301452637\n",
      "Layer 1 weight e_bits (float) = 0.5058820247650146,  m_bits (float) = 5.564774513244629\n",
      "Layer 2 weight e_bits (float) = 1.5000959634780884,  m_bits (float) = 3.6007590293884277\n",
      "Layer 3 weight e_bits (float) = 1.5020428895950317,  m_bits (float) = 5.562265872955322\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10166274011135101,  m_bits (float) = 1.000098705291748\n",
      "Layer 3 bias e_bits (float) = 0.1020001769065857,  m_bits (float) = 1.0123826265335083\n",
      "Train set: Average loss: 0.8916\n",
      "Test set: Average loss: 0.8086, Accuracy: 0.7218 (72.18%)\n",
      "Layer 0 input e_bits (float) = 0.105438731610775,  m_bits (float) = 5.538832187652588\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5317573547363281,  m_bits (float) = 4.5969038009643555\n",
      "Layer 1 weight e_bits (float) = 0.5044490098953247,  m_bits (float) = 5.570433616638184\n",
      "Layer 2 weight e_bits (float) = 1.499442458152771,  m_bits (float) = 3.5939738750457764\n",
      "Layer 3 weight e_bits (float) = 1.4991607666015625,  m_bits (float) = 5.57389497756958\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1009899377822876,  m_bits (float) = 1.000034213066101\n",
      "Layer 3 bias e_bits (float) = 0.10121464729309082,  m_bits (float) = 1.007433295249939\n",
      "Train set: Average loss: 0.8920\n",
      "Test set: Average loss: 0.8478, Accuracy: 0.7094 (70.94%)\n",
      "Layer 0 input e_bits (float) = 0.10371293127536774,  m_bits (float) = 5.56308126449585\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5317773818969727,  m_bits (float) = 4.589852333068848\n",
      "Layer 1 weight e_bits (float) = 0.506005048751831,  m_bits (float) = 5.557234287261963\n",
      "Layer 2 weight e_bits (float) = 1.5003046989440918,  m_bits (float) = 3.586514711380005\n",
      "Layer 3 weight e_bits (float) = 1.499921441078186,  m_bits (float) = 5.544248580932617\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10055883228778839,  m_bits (float) = 1.0000104904174805\n",
      "Layer 3 bias e_bits (float) = 0.10070088505744934,  m_bits (float) = 1.0042320489883423\n",
      "Train set: Average loss: 0.8825\n",
      "Test set: Average loss: 0.8777, Accuracy: 0.7062 (70.62%)\n",
      "Layer 0 input e_bits (float) = 0.10244279354810715,  m_bits (float) = 5.54074239730835\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5354965925216675,  m_bits (float) = 4.55387020111084\n",
      "Layer 1 weight e_bits (float) = 0.5053327083587646,  m_bits (float) = 5.562274932861328\n",
      "Layer 2 weight e_bits (float) = 1.5008171796798706,  m_bits (float) = 3.5783185958862305\n",
      "Layer 3 weight e_bits (float) = 1.5027838945388794,  m_bits (float) = 5.55704927444458\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10029714554548264,  m_bits (float) = 1.0000028610229492\n",
      "Layer 3 bias e_bits (float) = 0.1003820076584816,  m_bits (float) = 1.0022711753845215\n",
      "Train set: Average loss: 0.8790\n",
      "Test set: Average loss: 0.8314, Accuracy: 0.7137 (71.37%)\n",
      "Layer 0 input e_bits (float) = 0.10154154896736145,  m_bits (float) = 5.500786304473877\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5329583883285522,  m_bits (float) = 4.5914716720581055\n",
      "Layer 1 weight e_bits (float) = 0.5054270029067993,  m_bits (float) = 5.558063983917236\n",
      "Layer 2 weight e_bits (float) = 1.4999908208847046,  m_bits (float) = 3.565824508666992\n",
      "Layer 3 weight e_bits (float) = 1.4946779012680054,  m_bits (float) = 5.575131416320801\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1001477837562561,  m_bits (float) = 1.0000007152557373\n",
      "Layer 3 bias e_bits (float) = 0.10019531100988388,  m_bits (float) = 1.0011411905288696\n",
      "Train set: Average loss: 0.8681\n",
      "Test set: Average loss: 0.8621, Accuracy: 0.7001 (70.01%)\n",
      "Layer 0 input e_bits (float) = 0.10092848539352417,  m_bits (float) = 5.558189868927002\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5240191221237183,  m_bits (float) = 4.552704334259033\n",
      "Layer 1 weight e_bits (float) = 0.5059946775436401,  m_bits (float) = 5.571949481964111\n",
      "Layer 2 weight e_bits (float) = 1.499783992767334,  m_bits (float) = 3.600053071975708\n",
      "Layer 3 weight e_bits (float) = 1.4997928142547607,  m_bits (float) = 5.5305705070495605\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10006825625896454,  m_bits (float) = 1.0000001192092896\n",
      "Layer 3 bias e_bits (float) = 0.10009297728538513,  m_bits (float) = 1.000532865524292\n",
      "Train set: Average loss: 0.8652\n",
      "Test set: Average loss: 0.8153, Accuracy: 0.7126 (71.26%)\n",
      "Layer 0 input e_bits (float) = 0.10053078830242157,  m_bits (float) = 5.531712055206299\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.530509352684021,  m_bits (float) = 4.593071460723877\n",
      "Layer 1 weight e_bits (float) = 0.5089396834373474,  m_bits (float) = 5.610411167144775\n",
      "Layer 2 weight e_bits (float) = 1.4999161958694458,  m_bits (float) = 3.5914506912231445\n",
      "Layer 3 weight e_bits (float) = 1.4994479417800903,  m_bits (float) = 5.50784969329834\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10002896934747696,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10004086792469025,  m_bits (float) = 1.0002292394638062\n",
      "Train set: Average loss: 0.8598\n",
      "Test set: Average loss: 0.8259, Accuracy: 0.7176 (71.76%)\n",
      "Layer 0 input e_bits (float) = 0.10028617829084396,  m_bits (float) = 5.561120986938477\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5360395908355713,  m_bits (float) = 4.596546649932861\n",
      "Layer 1 weight e_bits (float) = 0.5065484046936035,  m_bits (float) = 5.595535755157471\n",
      "Layer 2 weight e_bits (float) = 1.5002495050430298,  m_bits (float) = 3.582003116607666\n",
      "Layer 3 weight e_bits (float) = 1.4980428218841553,  m_bits (float) = 5.617732524871826\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10001120716333389,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10001645237207413,  m_bits (float) = 1.0000900030136108\n",
      "Train set: Average loss: 0.8651\n",
      "Test set: Average loss: 0.8275, Accuracy: 0.7101 (71.01%)\n",
      "Layer 0 input e_bits (float) = 0.10014460980892181,  m_bits (float) = 5.521526336669922\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.530655026435852,  m_bits (float) = 4.596077919006348\n",
      "Layer 1 weight e_bits (float) = 0.5066133141517639,  m_bits (float) = 5.560689926147461\n",
      "Layer 2 weight e_bits (float) = 1.4994118213653564,  m_bits (float) = 3.571634292602539\n",
      "Layer 3 weight e_bits (float) = 1.4995315074920654,  m_bits (float) = 5.589908599853516\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000421851873398,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000605881214142,  m_bits (float) = 1.0000319480895996\n",
      "Train set: Average loss: 0.8584\n",
      "Test set: Average loss: 0.8287, Accuracy: 0.7115 (71.15%)\n",
      "Layer 0 input e_bits (float) = 0.1000678688287735,  m_bits (float) = 5.568719387054443\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5336271524429321,  m_bits (float) = 4.591324806213379\n",
      "Layer 1 weight e_bits (float) = 0.5090603232383728,  m_bits (float) = 5.558011054992676\n",
      "Layer 2 weight e_bits (float) = 1.4998292922973633,  m_bits (float) = 3.594806671142578\n",
      "Layer 3 weight e_bits (float) = 1.5079883337020874,  m_bits (float) = 5.571155548095703\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000190138816833,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000190138816833,  m_bits (float) = 1.0000101327896118\n",
      "Train set: Average loss: 0.8494\n",
      "Test set: Average loss: 0.8313, Accuracy: 0.7176 (71.76%)\n",
      "Layer 0 input e_bits (float) = 0.100029356777668,  m_bits (float) = 5.569919586181641\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5187299251556396,  m_bits (float) = 4.550783634185791\n",
      "Layer 1 weight e_bits (float) = 0.5015444159507751,  m_bits (float) = 5.576170444488525\n",
      "Layer 2 weight e_bits (float) = 1.5001400709152222,  m_bits (float) = 3.588624954223633\n",
      "Layer 3 weight e_bits (float) = 1.5157548189163208,  m_bits (float) = 5.555285930633545\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000181198120117,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000188648700714,  m_bits (float) = 1.0000028610229492\n",
      "Train set: Average loss: 0.8557\n",
      "Test set: Average loss: 0.8453, Accuracy: 0.7108 (71.08%)\n",
      "Layer 0 input e_bits (float) = 0.10001158714294434,  m_bits (float) = 5.555308818817139\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5255264043807983,  m_bits (float) = 4.615233421325684\n",
      "Layer 1 weight e_bits (float) = 0.5070246458053589,  m_bits (float) = 5.565707683563232\n",
      "Layer 2 weight e_bits (float) = 1.4999580383300781,  m_bits (float) = 3.581266164779663\n",
      "Layer 3 weight e_bits (float) = 1.515689730644226,  m_bits (float) = 5.518619537353516\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000164806842804,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000170767307281,  m_bits (float) = 1.0000007152557373\n",
      "Train set: Average loss: 0.8437\n",
      "Test set: Average loss: 0.8237, Accuracy: 0.7215 (72.15%)\n",
      "Layer 0 input e_bits (float) = 0.10000447928905487,  m_bits (float) = 5.540808200836182\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5187585353851318,  m_bits (float) = 4.59663724899292\n",
      "Layer 1 weight e_bits (float) = 0.5074657201766968,  m_bits (float) = 5.567183017730713\n",
      "Layer 2 weight e_bits (float) = 1.5007989406585693,  m_bits (float) = 3.5731818675994873\n",
      "Layer 3 weight e_bits (float) = 1.519363522529602,  m_bits (float) = 5.484269142150879\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1000014990568161,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000154376029968,  m_bits (float) = 1.0000001192092896\n",
      "Train set: Average loss: 0.8433\n",
      "Test set: Average loss: 0.8530, Accuracy: 0.7064 (70.64%)\n",
      "Layer 0 input e_bits (float) = 0.10000192373991013,  m_bits (float) = 5.567615985870361\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5065042972564697,  m_bits (float) = 4.592781066894531\n",
      "Layer 1 weight e_bits (float) = 0.504909098148346,  m_bits (float) = 5.556311130523682\n",
      "Layer 2 weight e_bits (float) = 1.4995813369750977,  m_bits (float) = 3.578543186187744\n",
      "Layer 3 weight e_bits (float) = 1.5218065977096558,  m_bits (float) = 5.481211185455322\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000135749578476,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000140219926834,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8395\n",
      "Test set: Average loss: 0.8250, Accuracy: 0.7207 (72.07%)\n",
      "Layer 0 input e_bits (float) = 0.10000185668468475,  m_bits (float) = 5.528297424316406\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5160725116729736,  m_bits (float) = 4.518627643585205\n",
      "Layer 1 weight e_bits (float) = 0.5094307661056519,  m_bits (float) = 5.613700866699219\n",
      "Layer 2 weight e_bits (float) = 1.5000827312469482,  m_bits (float) = 3.56298565864563\n",
      "Layer 3 weight e_bits (float) = 1.5191302299499512,  m_bits (float) = 5.4062886238098145\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000121593475342,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000128298997879,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8395\n",
      "Test set: Average loss: 0.8542, Accuracy: 0.7161 (71.61%)\n",
      "Layer 0 input e_bits (float) = 0.10000169277191162,  m_bits (float) = 5.523257255554199\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5203845500946045,  m_bits (float) = 4.6143293380737305\n",
      "Layer 1 weight e_bits (float) = 0.5051692128181458,  m_bits (float) = 5.569066047668457\n",
      "Layer 2 weight e_bits (float) = 1.500576138496399,  m_bits (float) = 3.5672173500061035\n",
      "Layer 3 weight e_bits (float) = 1.5159603357315063,  m_bits (float) = 5.359230995178223\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000111907720566,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000114142894745,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8350\n",
      "Test set: Average loss: 0.8328, Accuracy: 0.7227 (72.27%)\n",
      "Layer 0 input e_bits (float) = 0.10000152140855789,  m_bits (float) = 5.5866923332214355\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5131804943084717,  m_bits (float) = 4.590478897094727\n",
      "Layer 1 weight e_bits (float) = 0.5022942423820496,  m_bits (float) = 5.557912826538086\n",
      "Layer 2 weight e_bits (float) = 1.500159502029419,  m_bits (float) = 3.542546272277832\n",
      "Layer 3 weight e_bits (float) = 1.5245596170425415,  m_bits (float) = 5.318388938903809\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000099986791611,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000105202198029,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8362\n",
      "Test set: Average loss: 0.8356, Accuracy: 0.7219 (72.19%)\n",
      "Layer 0 input e_bits (float) = 0.10000137984752655,  m_bits (float) = 5.567054271697998\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.517392873764038,  m_bits (float) = 4.561796188354492\n",
      "Layer 1 weight e_bits (float) = 0.5004422664642334,  m_bits (float) = 5.564628601074219\n",
      "Layer 2 weight e_bits (float) = 1.5003910064697266,  m_bits (float) = 3.5950965881347656\n",
      "Layer 3 weight e_bits (float) = 1.5166441202163696,  m_bits (float) = 5.273922920227051\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000090301036835,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000094771385193,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8340\n",
      "Test set: Average loss: 0.8221, Accuracy: 0.7182 (71.82%)\n",
      "Layer 0 input e_bits (float) = 0.1000012680888176,  m_bits (float) = 5.565843105316162\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5330517292022705,  m_bits (float) = 4.5721964836120605\n",
      "Layer 1 weight e_bits (float) = 0.506693422794342,  m_bits (float) = 5.55999755859375\n",
      "Layer 2 weight e_bits (float) = 1.4995518922805786,  m_bits (float) = 3.5873677730560303\n",
      "Layer 3 weight e_bits (float) = 1.5233479738235474,  m_bits (float) = 5.2255682945251465\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000083595514297,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000085830688477,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8380\n",
      "Test set: Average loss: 0.7975, Accuracy: 0.7264 (72.64%)\n",
      "Layer 0 input e_bits (float) = 0.10000114142894745,  m_bits (float) = 5.591094017028809\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.525296926498413,  m_bits (float) = 4.583052635192871\n",
      "Layer 1 weight e_bits (float) = 0.5065140128135681,  m_bits (float) = 5.573668479919434\n",
      "Layer 2 weight e_bits (float) = 1.4998071193695068,  m_bits (float) = 3.5788772106170654\n",
      "Layer 3 weight e_bits (float) = 1.5270707607269287,  m_bits (float) = 5.173063278198242\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000073909759521,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.1000007838010788,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8256\n",
      "Test set: Average loss: 0.8342, Accuracy: 0.72 (72.00%)\n",
      "Layer 0 input e_bits (float) = 0.1000010222196579,  m_bits (float) = 5.582211494445801\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5310828685760498,  m_bits (float) = 4.50034236907959\n",
      "Layer 1 weight e_bits (float) = 0.5033383369445801,  m_bits (float) = 5.59158182144165\n",
      "Layer 2 weight e_bits (float) = 1.50043523311615,  m_bits (float) = 3.570378065109253\n",
      "Layer 3 weight e_bits (float) = 1.525277018547058,  m_bits (float) = 5.116143703460693\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000066459178925,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000070929527283,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8219\n",
      "Test set: Average loss: 0.8093, Accuracy: 0.7221 (72.21%)\n",
      "Layer 0 input e_bits (float) = 0.10000092536211014,  m_bits (float) = 5.576154708862305\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5328608751296997,  m_bits (float) = 4.591642379760742\n",
      "Layer 1 weight e_bits (float) = 0.5089675188064575,  m_bits (float) = 5.563232421875\n",
      "Layer 2 weight e_bits (float) = 1.499373197555542,  m_bits (float) = 3.575606346130371\n",
      "Layer 3 weight e_bits (float) = 1.5313374996185303,  m_bits (float) = 5.05454158782959\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000061988830566,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000064224004745,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8151\n",
      "Test set: Average loss: 0.8220, Accuracy: 0.7263 (72.63%)\n",
      "Layer 0 input e_bits (float) = 0.10000083595514297,  m_bits (float) = 5.579816818237305\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5305867195129395,  m_bits (float) = 4.589312553405762\n",
      "Layer 1 weight e_bits (float) = 0.5080942511558533,  m_bits (float) = 5.612287521362305\n",
      "Layer 2 weight e_bits (float) = 1.5005422830581665,  m_bits (float) = 3.537003755569458\n",
      "Layer 3 weight e_bits (float) = 1.5304194688796997,  m_bits (float) = 4.987997055053711\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1000005453824997,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000056773424149,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8115\n",
      "Test set: Average loss: 0.8243, Accuracy: 0.7207 (72.07%)\n",
      "Layer 0 input e_bits (float) = 0.100000761449337,  m_bits (float) = 5.513693332672119\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5255730152130127,  m_bits (float) = 4.563858985900879\n",
      "Layer 1 weight e_bits (float) = 0.5084159970283508,  m_bits (float) = 5.606045246124268\n",
      "Layer 2 weight e_bits (float) = 1.4998997449874878,  m_bits (float) = 3.546967029571533\n",
      "Layer 3 weight e_bits (float) = 1.5305371284484863,  m_bits (float) = 4.916261672973633\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000049322843552,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.1000005230307579,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8160\n",
      "Test set: Average loss: 0.8733, Accuracy: 0.712 (71.20%)\n",
      "Layer 0 input e_bits (float) = 0.10000068694353104,  m_bits (float) = 5.54739236831665\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5305367708206177,  m_bits (float) = 4.559399604797363\n",
      "Layer 1 weight e_bits (float) = 0.5077832937240601,  m_bits (float) = 5.5080885887146\n",
      "Layer 2 weight e_bits (float) = 1.5005446672439575,  m_bits (float) = 3.5387344360351562\n",
      "Layer 3 weight e_bits (float) = 1.5312694311141968,  m_bits (float) = 4.839106559753418\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000044852495193,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000047087669373,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8164\n",
      "Test set: Average loss: 0.8370, Accuracy: 0.7139 (71.39%)\n",
      "Layer 0 input e_bits (float) = 0.10000061988830566,  m_bits (float) = 5.55706262588501\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5202497243881226,  m_bits (float) = 4.557081699371338\n",
      "Layer 1 weight e_bits (float) = 0.5094201564788818,  m_bits (float) = 5.564706802368164\n",
      "Layer 2 weight e_bits (float) = 1.4993733167648315,  m_bits (float) = 3.5643444061279297\n",
      "Layer 3 weight e_bits (float) = 1.5305906534194946,  m_bits (float) = 4.756324291229248\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000040382146835,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000043362379074,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8133\n",
      "Test set: Average loss: 0.8448, Accuracy: 0.7202 (72.02%)\n",
      "Layer 0 input e_bits (float) = 0.10000056773424149,  m_bits (float) = 5.580723762512207\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5324152708053589,  m_bits (float) = 4.599435329437256\n",
      "Layer 1 weight e_bits (float) = 0.5058786273002625,  m_bits (float) = 5.561186790466309\n",
      "Layer 2 weight e_bits (float) = 1.501031517982483,  m_bits (float) = 3.5931129455566406\n",
      "Layer 3 weight e_bits (float) = 1.5292385816574097,  m_bits (float) = 4.667748928070068\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000038146972656,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000038146972656,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8109\n",
      "Test set: Average loss: 0.7925, Accuracy: 0.7264 (72.64%)\n",
      "Layer 0 input e_bits (float) = 0.10000049322843552,  m_bits (float) = 5.552029132843018\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.530281662940979,  m_bits (float) = 4.597151756286621\n",
      "Layer 1 weight e_bits (float) = 0.5053879618644714,  m_bits (float) = 5.558753967285156\n",
      "Layer 2 weight e_bits (float) = 1.4999970197677612,  m_bits (float) = 3.5838892459869385\n",
      "Layer 3 weight e_bits (float) = 1.5332304239273071,  m_bits (float) = 4.578684329986572\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000032931566238,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000035911798477,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8110\n",
      "Test set: Average loss: 0.8725, Accuracy: 0.7123 (71.23%)\n",
      "Layer 0 input e_bits (float) = 0.10000044852495193,  m_bits (float) = 5.556521892547607\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5359593629837036,  m_bits (float) = 4.592024803161621\n",
      "Layer 1 weight e_bits (float) = 0.5071083307266235,  m_bits (float) = 5.609184741973877\n",
      "Layer 2 weight e_bits (float) = 1.501253366470337,  m_bits (float) = 3.5737385749816895\n",
      "Layer 3 weight e_bits (float) = 1.527457594871521,  m_bits (float) = 4.487905979156494\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1000003069639206,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.1000003069639206,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8013\n",
      "Test set: Average loss: 0.8030, Accuracy: 0.7274 (72.74%)\n",
      "Layer 0 input e_bits (float) = 0.10000043362379074,  m_bits (float) = 5.572667121887207\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5299558639526367,  m_bits (float) = 4.592504978179932\n",
      "Layer 1 weight e_bits (float) = 0.5054020881652832,  m_bits (float) = 5.577754020690918\n",
      "Layer 2 weight e_bits (float) = 1.4993042945861816,  m_bits (float) = 3.516982316970825\n",
      "Layer 3 weight e_bits (float) = 1.5328865051269531,  m_bits (float) = 4.570251941680908\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000025480985641,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.1000002846121788,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8050\n",
      "Test set: Average loss: 0.7961, Accuracy: 0.7291 (72.91%)\n",
      "Layer 0 input e_bits (float) = 0.10000038146972656,  m_bits (float) = 5.5720367431640625\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5308183431625366,  m_bits (float) = 4.551157474517822\n",
      "Layer 1 weight e_bits (float) = 0.5029832124710083,  m_bits (float) = 5.5692877769470215\n",
      "Layer 2 weight e_bits (float) = 1.4993292093276978,  m_bits (float) = 3.57462739944458\n",
      "Layer 3 weight e_bits (float) = 1.5338095426559448,  m_bits (float) = 4.512587070465088\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000023245811462,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000025480985641,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7959\n",
      "Test set: Average loss: 0.8422, Accuracy: 0.7263 (72.63%)\n",
      "Layer 0 input e_bits (float) = 0.10000032931566238,  m_bits (float) = 5.562923908233643\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5269198417663574,  m_bits (float) = 4.564615249633789\n",
      "Layer 1 weight e_bits (float) = 0.5019027590751648,  m_bits (float) = 5.575375556945801\n",
      "Layer 2 weight e_bits (float) = 1.5006223917007446,  m_bits (float) = 3.5521552562713623\n",
      "Layer 3 weight e_bits (float) = 1.5349146127700806,  m_bits (float) = 4.406876087188721\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000021755695343,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000023245811462,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7968\n",
      "Test set: Average loss: 0.8301, Accuracy: 0.7225 (72.25%)\n",
      "Layer 0 input e_bits (float) = 0.1000003069639206,  m_bits (float) = 5.556801795959473\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.531161904335022,  m_bits (float) = 4.570587635040283\n",
      "Layer 1 weight e_bits (float) = 0.5061173439025879,  m_bits (float) = 5.579655647277832\n",
      "Layer 2 weight e_bits (float) = 1.5006980895996094,  m_bits (float) = 3.540663719177246\n",
      "Layer 3 weight e_bits (float) = 1.5241832733154297,  m_bits (float) = 4.370841026306152\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000018775463104,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000021755695343,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7964\n",
      "Test set: Average loss: 0.8452, Accuracy: 0.7223 (72.23%)\n",
      "Layer 0 input e_bits (float) = 0.1000002846121788,  m_bits (float) = 5.5602874755859375\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5377871990203857,  m_bits (float) = 4.592001914978027\n",
      "Layer 1 weight e_bits (float) = 0.5058258771896362,  m_bits (float) = 5.574563980102539\n",
      "Layer 2 weight e_bits (float) = 1.5003899335861206,  m_bits (float) = 3.5711185932159424\n",
      "Layer 3 weight e_bits (float) = 1.5304309129714966,  m_bits (float) = 4.332116603851318\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000018775463104,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000018775463104,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.8022\n",
      "Test set: Average loss: 0.8377, Accuracy: 0.723 (72.30%)\n",
      "Layer 0 input e_bits (float) = 0.10000025480985641,  m_bits (float) = 5.518671035766602\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5309648513793945,  m_bits (float) = 4.5940632820129395\n",
      "Layer 1 weight e_bits (float) = 0.5008501410484314,  m_bits (float) = 5.570844650268555\n",
      "Layer 2 weight e_bits (float) = 1.4999281167984009,  m_bits (float) = 3.544781446456909\n",
      "Layer 3 weight e_bits (float) = 1.5383402109146118,  m_bits (float) = 4.290073871612549\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000016540288925,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000016540288925,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7982\n",
      "Test set: Average loss: 0.8187, Accuracy: 0.7186 (71.86%)\n",
      "Layer 0 input e_bits (float) = 0.10000023245811462,  m_bits (float) = 5.561546325683594\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5322773456573486,  m_bits (float) = 4.590933322906494\n",
      "Layer 1 weight e_bits (float) = 0.5101153254508972,  m_bits (float) = 5.56396484375\n",
      "Layer 2 weight e_bits (float) = 1.4996663331985474,  m_bits (float) = 3.544501781463623\n",
      "Layer 3 weight e_bits (float) = 1.530806541442871,  m_bits (float) = 4.244504451751709\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000014305114746,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000014305114746,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7924\n",
      "Test set: Average loss: 0.8676, Accuracy: 0.7213 (72.13%)\n",
      "Layer 0 input e_bits (float) = 0.10000021755695343,  m_bits (float) = 5.526763916015625\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5266039371490479,  m_bits (float) = 4.625889778137207\n",
      "Layer 1 weight e_bits (float) = 0.5089735388755798,  m_bits (float) = 5.568197727203369\n",
      "Layer 2 weight e_bits (float) = 1.5003862380981445,  m_bits (float) = 3.5672295093536377\n",
      "Layer 3 weight e_bits (float) = 1.534429907798767,  m_bits (float) = 4.195196628570557\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000014305114746,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000014305114746,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7845\n",
      "Test set: Average loss: 0.8288, Accuracy: 0.7297 (72.97%)\n",
      "Layer 0 input e_bits (float) = 0.10000018775463104,  m_bits (float) = 5.541408538818359\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5169527530670166,  m_bits (float) = 4.6103339195251465\n",
      "Layer 1 weight e_bits (float) = 0.5080757737159729,  m_bits (float) = 5.578444957733154\n",
      "Layer 2 weight e_bits (float) = 1.5011564493179321,  m_bits (float) = 3.5440194606781006\n",
      "Layer 3 weight e_bits (float) = 1.5242401361465454,  m_bits (float) = 4.141946792602539\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000011324882507,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000011324882507,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7912\n",
      "Test set: Average loss: 0.8002, Accuracy: 0.7276 (72.76%)\n",
      "Layer 0 input e_bits (float) = 0.10000016540288925,  m_bits (float) = 5.537097930908203\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5312799215316772,  m_bits (float) = 4.592045307159424\n",
      "Layer 1 weight e_bits (float) = 0.5051743388175964,  m_bits (float) = 5.5661091804504395\n",
      "Layer 2 weight e_bits (float) = 1.499876618385315,  m_bits (float) = 3.547381639480591\n",
      "Layer 3 weight e_bits (float) = 1.541170597076416,  m_bits (float) = 4.0845561027526855\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000009089708328,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000011324882507,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7875\n",
      "Test set: Average loss: 0.8235, Accuracy: 0.7193 (71.93%)\n",
      "Layer 0 input e_bits (float) = 0.10000014305114746,  m_bits (float) = 5.5654754638671875\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5270330905914307,  m_bits (float) = 4.5912322998046875\n",
      "Layer 1 weight e_bits (float) = 0.5052527785301208,  m_bits (float) = 5.569554805755615\n",
      "Layer 2 weight e_bits (float) = 1.4986438751220703,  m_bits (float) = 3.5110366344451904\n",
      "Layer 3 weight e_bits (float) = 1.529392123222351,  m_bits (float) = 4.022844314575195\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000009089708328,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000009089708328,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7865\n",
      "Test set: Average loss: 0.8117, Accuracy: 0.7213 (72.13%)\n",
      "Layer 0 input e_bits (float) = 0.10000014305114746,  m_bits (float) = 5.5672831535339355\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.530676245689392,  m_bits (float) = 4.593625068664551\n",
      "Layer 1 weight e_bits (float) = 0.5083053112030029,  m_bits (float) = 5.573641777038574\n",
      "Layer 2 weight e_bits (float) = 1.4983797073364258,  m_bits (float) = 3.5708580017089844\n",
      "Layer 3 weight e_bits (float) = 1.5366942882537842,  m_bits (float) = 3.9566502571105957\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000009089708328,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000009089708328,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7842\n",
      "Test set: Average loss: 0.8445, Accuracy: 0.7187 (71.87%)\n",
      "Layer 0 input e_bits (float) = 0.10000011324882507,  m_bits (float) = 5.576318740844727\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.521500825881958,  m_bits (float) = 4.584708213806152\n",
      "Layer 1 weight e_bits (float) = 0.5035817623138428,  m_bits (float) = 5.537960529327393\n",
      "Layer 2 weight e_bits (float) = 1.4990606307983398,  m_bits (float) = 3.570892572402954\n",
      "Layer 3 weight e_bits (float) = 1.5408512353897095,  m_bits (float) = 3.885836124420166\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000006854534149,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000006854534149,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7822\n",
      "Test set: Average loss: 0.8450, Accuracy: 0.7222 (72.22%)\n",
      "Layer 0 input e_bits (float) = 0.10000011324882507,  m_bits (float) = 5.56837272644043\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5073537826538086,  m_bits (float) = 4.553423881530762\n",
      "Layer 1 weight e_bits (float) = 0.5053141117095947,  m_bits (float) = 5.55759334564209\n",
      "Layer 2 weight e_bits (float) = 1.5007375478744507,  m_bits (float) = 3.533719062805176\n",
      "Layer 3 weight e_bits (float) = 1.5322927236557007,  m_bits (float) = 3.810309886932373\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000006854534149,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000006854534149,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7856\n",
      "Test set: Average loss: 0.8104, Accuracy: 0.7253 (72.53%)\n",
      "Layer 0 input e_bits (float) = 0.10000009089708328,  m_bits (float) = 5.504386901855469\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5316885709762573,  m_bits (float) = 4.591371059417725\n",
      "Layer 1 weight e_bits (float) = 0.5104917883872986,  m_bits (float) = 5.577823162078857\n",
      "Layer 2 weight e_bits (float) = 1.499314308166504,  m_bits (float) = 3.5425620079040527\n",
      "Layer 3 weight e_bits (float) = 1.5318748950958252,  m_bits (float) = 3.730010986328125\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000006854534149,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000006854534149,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7840\n",
      "Test set: Average loss: 0.8253, Accuracy: 0.7217 (72.17%)\n",
      "Layer 0 input e_bits (float) = 0.10000009089708328,  m_bits (float) = 5.4999308586120605\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5190973281860352,  m_bits (float) = 4.54742956161499\n",
      "Layer 1 weight e_bits (float) = 0.5074881315231323,  m_bits (float) = 5.561351299285889\n",
      "Layer 2 weight e_bits (float) = 1.499136209487915,  m_bits (float) = 3.5245413780212402\n",
      "Layer 3 weight e_bits (float) = 1.5349301099777222,  m_bits (float) = 3.644930124282837\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1000000387430191,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.1000000387430191,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7741\n",
      "Test set: Average loss: 0.8257, Accuracy: 0.7309 (73.09%)\n",
      "Layer 0 input e_bits (float) = 0.10000006854534149,  m_bits (float) = 5.491518497467041\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5152087211608887,  m_bits (float) = 4.5591607093811035\n",
      "Layer 1 weight e_bits (float) = 0.5069571733474731,  m_bits (float) = 5.6001667976379395\n",
      "Layer 2 weight e_bits (float) = 1.500648021697998,  m_bits (float) = 3.5455410480499268\n",
      "Layer 3 weight e_bits (float) = 1.5309901237487793,  m_bits (float) = 3.5741372108459473\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1000000387430191,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.1000000387430191,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7780\n",
      "Test set: Average loss: 0.8361, Accuracy: 0.7172 (71.72%)\n",
      "Layer 0 input e_bits (float) = 0.10000006854534149,  m_bits (float) = 5.519773960113525\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5092142820358276,  m_bits (float) = 4.575349807739258\n",
      "Layer 1 weight e_bits (float) = 0.5064554214477539,  m_bits (float) = 5.573442459106445\n",
      "Layer 2 weight e_bits (float) = 1.4987772703170776,  m_bits (float) = 3.53670072555542\n",
      "Layer 3 weight e_bits (float) = 1.531083583831787,  m_bits (float) = 3.58365797996521\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1000000387430191,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.1000000387430191,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7739\n",
      "Test set: Average loss: 0.7929, Accuracy: 0.7307 (73.07%)\n",
      "Layer 0 input e_bits (float) = 0.10000006854534149,  m_bits (float) = 5.541635513305664\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5236692428588867,  m_bits (float) = 4.561323165893555\n",
      "Layer 1 weight e_bits (float) = 0.5040252804756165,  m_bits (float) = 5.6010003089904785\n",
      "Layer 2 weight e_bits (float) = 1.499373435974121,  m_bits (float) = 3.539656162261963\n",
      "Layer 3 weight e_bits (float) = 1.5368825197219849,  m_bits (float) = 3.5618398189544678\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.1000000387430191,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.1000000387430191,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7710\n",
      "Test set: Average loss: 0.8329, Accuracy: 0.7257 (72.57%)\n",
      "Layer 0 input e_bits (float) = 0.1000000387430191,  m_bits (float) = 5.505718231201172\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5305315256118774,  m_bits (float) = 4.54574728012085\n",
      "Layer 1 weight e_bits (float) = 0.5047087073326111,  m_bits (float) = 5.61238431930542\n",
      "Layer 2 weight e_bits (float) = 1.5031700134277344,  m_bits (float) = 3.5722265243530273\n",
      "Layer 3 weight e_bits (float) = 1.5435740947723389,  m_bits (float) = 3.572490930557251\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7667\n",
      "Test set: Average loss: 0.8160, Accuracy: 0.73 (73.00%)\n",
      "Layer 0 input e_bits (float) = 0.1000000387430191,  m_bits (float) = 5.558313846588135\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5304874181747437,  m_bits (float) = 4.509716033935547\n",
      "Layer 1 weight e_bits (float) = 0.5056951642036438,  m_bits (float) = 5.583241939544678\n",
      "Layer 2 weight e_bits (float) = 1.50117027759552,  m_bits (float) = 3.546848773956299\n",
      "Layer 3 weight e_bits (float) = 1.5301965475082397,  m_bits (float) = 3.570148229598999\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7722\n",
      "Test set: Average loss: 0.8394, Accuracy: 0.7248 (72.48%)\n",
      "Layer 0 input e_bits (float) = 0.1000000387430191,  m_bits (float) = 5.562816143035889\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5304851531982422,  m_bits (float) = 4.593280792236328\n",
      "Layer 1 weight e_bits (float) = 0.5087906718254089,  m_bits (float) = 5.609870433807373\n",
      "Layer 2 weight e_bits (float) = 1.5009377002716064,  m_bits (float) = 3.5341086387634277\n",
      "Layer 3 weight e_bits (float) = 1.530838131904602,  m_bits (float) = 3.3662266731262207\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7669\n",
      "Test set: Average loss: 0.8283, Accuracy: 0.7289 (72.89%)\n",
      "Layer 0 input e_bits (float) = 0.1000000387430191,  m_bits (float) = 5.506911277770996\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.530729055404663,  m_bits (float) = 4.549206256866455\n",
      "Layer 1 weight e_bits (float) = 0.507343590259552,  m_bits (float) = 5.596962928771973\n",
      "Layer 2 weight e_bits (float) = 1.5013225078582764,  m_bits (float) = 3.5345749855041504\n",
      "Layer 3 weight e_bits (float) = 1.5306785106658936,  m_bits (float) = 3.311248540878296\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7691\n",
      "Test set: Average loss: 0.8200, Accuracy: 0.7253 (72.53%)\n",
      "Layer 0 input e_bits (float) = 0.10000002384185791,  m_bits (float) = 5.5579376220703125\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5303679704666138,  m_bits (float) = 4.549170970916748\n",
      "Layer 1 weight e_bits (float) = 0.5062019228935242,  m_bits (float) = 5.566752910614014\n",
      "Layer 2 weight e_bits (float) = 1.4936095476150513,  m_bits (float) = 3.5761470794677734\n",
      "Layer 3 weight e_bits (float) = 1.5368435382843018,  m_bits (float) = 3.252530813217163\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7630\n",
      "Test set: Average loss: 0.8426, Accuracy: 0.7185 (71.85%)\n",
      "Layer 0 input e_bits (float) = 0.10000002384185791,  m_bits (float) = 5.597707748413086\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5269815921783447,  m_bits (float) = 4.575923919677734\n",
      "Layer 1 weight e_bits (float) = 0.5077722668647766,  m_bits (float) = 5.555605411529541\n",
      "Layer 2 weight e_bits (float) = 1.4999338388442993,  m_bits (float) = 3.553128480911255\n",
      "Layer 3 weight e_bits (float) = 1.5545457601547241,  m_bits (float) = 3.1900103092193604\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7663\n",
      "Test set: Average loss: 0.8418, Accuracy: 0.7151 (71.51%)\n",
      "Layer 0 input e_bits (float) = 0.10000002384185791,  m_bits (float) = 5.505886554718018\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5190675258636475,  m_bits (float) = 4.57189416885376\n",
      "Layer 1 weight e_bits (float) = 0.4985392689704895,  m_bits (float) = 5.612864971160889\n",
      "Layer 2 weight e_bits (float) = 1.4989274740219116,  m_bits (float) = 3.5571682453155518\n",
      "Layer 3 weight e_bits (float) = 1.537774682044983,  m_bits (float) = 3.1236603260040283\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000002384185791,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7628\n",
      "Test set: Average loss: 0.8622, Accuracy: 0.7227 (72.27%)\n",
      "Layer 0 input e_bits (float) = 0.10000002384185791,  m_bits (float) = 5.556924819946289\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5256569385528564,  m_bits (float) = 4.580028057098389\n",
      "Layer 1 weight e_bits (float) = 0.5056216716766357,  m_bits (float) = 5.583700656890869\n",
      "Layer 2 weight e_bits (float) = 1.501342535018921,  m_bits (float) = 3.574247121810913\n",
      "Layer 3 weight e_bits (float) = 1.5400397777557373,  m_bits (float) = 3.053499937057495\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7645\n",
      "Test set: Average loss: 0.8447, Accuracy: 0.7233 (72.33%)\n",
      "Layer 0 input e_bits (float) = 0.10000002384185791,  m_bits (float) = 5.528467178344727\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5154767036437988,  m_bits (float) = 4.5404815673828125\n",
      "Layer 1 weight e_bits (float) = 0.5032365322113037,  m_bits (float) = 5.5899434089660645\n",
      "Layer 2 weight e_bits (float) = 1.4988772869110107,  m_bits (float) = 3.572005033493042\n",
      "Layer 3 weight e_bits (float) = 1.531720519065857,  m_bits (float) = 2.9795987606048584\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7615\n",
      "Test set: Average loss: 0.8616, Accuracy: 0.7136 (71.36%)\n",
      "Layer 0 input e_bits (float) = 0.10000002384185791,  m_bits (float) = 5.489658832550049\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5243408679962158,  m_bits (float) = 4.595891952514648\n",
      "Layer 1 weight e_bits (float) = 0.5050528049468994,  m_bits (float) = 5.479854583740234\n",
      "Layer 2 weight e_bits (float) = 1.497509479522705,  m_bits (float) = 3.5696921348571777\n",
      "Layer 3 weight e_bits (float) = 1.5383681058883667,  m_bits (float) = 2.9020800590515137\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7656\n",
      "Test set: Average loss: 0.8179, Accuracy: 0.7263 (72.63%)\n",
      "Layer 0 input e_bits (float) = 0.10000002384185791,  m_bits (float) = 5.558104038238525\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.514931559562683,  m_bits (float) = 4.550547122955322\n",
      "Layer 1 weight e_bits (float) = 0.507834255695343,  m_bits (float) = 5.669625282287598\n",
      "Layer 2 weight e_bits (float) = 1.4990365505218506,  m_bits (float) = 3.5414202213287354\n",
      "Layer 3 weight e_bits (float) = 1.5344760417938232,  m_bits (float) = 2.821134328842163\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7625\n",
      "Test set: Average loss: 0.8343, Accuracy: 0.7204 (72.04%)\n",
      "Layer 0 input e_bits (float) = 0.10000000149011612,  m_bits (float) = 5.556676864624023\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.5240801572799683,  m_bits (float) = 4.592681407928467\n",
      "Layer 1 weight e_bits (float) = 0.5054211616516113,  m_bits (float) = 5.628139019012451\n",
      "Layer 2 weight e_bits (float) = 1.498536467552185,  m_bits (float) = 3.5557212829589844\n",
      "Layer 3 weight e_bits (float) = 1.5311132669448853,  m_bits (float) = 2.7370147705078125\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7572\n",
      "Test set: Average loss: 0.8869, Accuracy: 0.7164 (71.64%)\n",
      "Layer 0 input e_bits (float) = 0.10000000149011612,  m_bits (float) = 5.561712741851807\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.536788821220398,  m_bits (float) = 4.568591594696045\n",
      "Layer 1 weight e_bits (float) = 0.506816565990448,  m_bits (float) = 5.609641075134277\n",
      "Layer 2 weight e_bits (float) = 1.5002126693725586,  m_bits (float) = 3.563756227493286\n",
      "Layer 3 weight e_bits (float) = 1.5323067903518677,  m_bits (float) = 2.6500449180603027\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Train set: Average loss: 0.7536\n",
      "Test set: Average loss: 0.8910, Accuracy: 0.7175 (71.75%)\n",
      "Layer 0 input e_bits (float) = 0.10000000149011612,  m_bits (float) = 5.494398593902588\n",
      "Layer 1 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 3 input e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 0 weight e_bits (float) = 1.532065749168396,  m_bits (float) = 4.560824871063232\n",
      "Layer 1 weight e_bits (float) = 0.5055113434791565,  m_bits (float) = 5.5687150955200195\n",
      "Layer 2 weight e_bits (float) = 1.5001256465911865,  m_bits (float) = 3.547790288925171\n",
      "Layer 3 weight e_bits (float) = 1.5437748432159424,  m_bits (float) = 2.5606186389923096\n",
      "Layer 0 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 1 bias e_bits (float) = 8.0,  m_bits (float) = 23.000001907348633\n",
      "Layer 2 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n",
      "Layer 3 bias e_bits (float) = 0.10000000149011612,  m_bits (float) = 1.0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"using divice \", device)\n",
    "\n",
    "# Create model\n",
    "# model = SimpleQuantizedMLP(e_bits=4.0, m_bits=4.0, num_classes=len(classes)).to(device)\n",
    "model = SimpleQuantizedCNN(num_classes=len(classes)).to(device)\n",
    "#model = SimpleCIFAR10Model2(num_classes=len(classes)).to(device)\n",
    "\n",
    "# Create optimizer (SGD or Adam)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "# Train for some epochs\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    accuracy = test(model, device, test_loader)\n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), \"train_weights_and_quant_best_model.pth\")\n",
    "        \n",
    "    model.printBitWidths()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
