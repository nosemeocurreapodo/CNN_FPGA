{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "batch_size = 1024\n",
    "dataset = \"MNIST\"\n",
    "#dataset = \"CIFAR10\"\n",
    "#dataset = \"CIFAR100\"\n",
    "\n",
    "if(dataset == \"MNIST\"):\n",
    "    # 1) MNIST Dataset & Dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "    input_size = (1, 32, 32)\n",
    "\n",
    "if(dataset == \"CIFAR10\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    input_size = (3, 32, 32)\n",
    "\n",
    "if(dataset == \"CIFAR100\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = [x for x in range(100)]\n",
    "\n",
    "    input_size = (3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.42421296..2.8214867].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: a batch of images in shape (B, C, H, W) or a single image in (C, H, W).\n",
    "    \"\"\"\n",
    "    # If it's a batch of images (4D), make a grid first\n",
    "    if len(img_tensor.shape) == 4:\n",
    "        img_tensor = torchvision.utils.make_grid(img_tensor)\n",
    "    # Unnormalize\n",
    "    #img_tensor = unnormalize(img_tensor)\n",
    "    # Convert to numpy\n",
    "    npimg = img_tensor.numpy()\n",
    "    # Transpose from (C, H, W) to (H, W, C)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_bit(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(x)\n",
    "\n",
    "def bit_to_param(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_float_truncate(x: torch.Tensor, e_bits: int, m_bits: int, s: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate 'float' with e_bits exponent bits and m_bits mantissa bits.\n",
    "    Simplified approach: unbiased exponent in integer range + truncated mantissa.\n",
    "    \"\"\"\n",
    "    \n",
    "    sign = x.sign()\n",
    "    abs_x = x.abs().clamp(min=1e-45)\n",
    "\n",
    "    exponent = torch.floor(torch.log2(abs_x))\n",
    "    mantissa = abs_x / (2**exponent)\n",
    "    \n",
    "    #assert(mantissa >= 1.0 and mantissa <= 2.0)\n",
    "\n",
    "    #for 2 bits (0  3), the midpoint shoud be in 1.5\n",
    "    #for 1 bit (0 - 1) the mid point shoud be in 0.5\n",
    "    #for 0 bits, the midpoint is zero\n",
    "    exponent_s = exponent - s\n",
    "    exponent_midi = (2**e_bits - 1)/2\n",
    "    exponent_n = exponent_s + exponent_midi\n",
    "\n",
    "    # truncate exponent\n",
    "    exponent_n_trunc = torch.clip(exponent_n, 0, 2**e_bits - 1)\n",
    "    exponent_d_trunc = exponent_n_trunc - exponent_midi\n",
    "        \n",
    "    # truncate mantissa\n",
    "    mantissa_n = mantissa - 1\n",
    "    m_scale = 2.0 ** m_bits\n",
    "    mantissa_trunc = torch.floor(mantissa_n * m_scale) / m_scale\n",
    "    \n",
    "    x_trunc = sign * (2**exponent_d_trunc) * mantissa_trunc * (2**s)\n",
    "    \n",
    "    #if(m_bits == 0 and x_trunc.any()):\n",
    "    #    print(\"bits: \", e_bits, \" \", m_bits, \" \", s)\n",
    "    #    print(\"x: \", x)\n",
    "    #    print(\"mantissa: \", mantissa)\n",
    "    #    print(\"exponent: \", exponent)\n",
    "    #    print(\"mantissa_n: \", mantissa_n)\n",
    "    #    print(\"mantissa_trunc: \", mantissa_trunc)\n",
    "    #    print(\"exponent s: \", exponent_s)\n",
    "    #    print(\"exponent mid: \", exponent_midi)\n",
    "    #    print(\"exponent_normalized: \", exponent_n)\n",
    "    #    print(\"exponent_trunc: \", exponent_n_trunc)\n",
    "    #    print(\"x_trunc: \", x_trunc)\n",
    "    \n",
    "    return x_trunc\n",
    "\n",
    "class FakeFloatFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e_bits_param, m_bits_param, s_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, e_bits_param, m_bits_param, s_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).item())\n",
    "        s_int = int(torch.round(s_param).item())\n",
    "\n",
    "        out = fake_float_truncate(x, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, e_bits_param, m_bits_param, s_param = ctx.saved_tensors\n",
    "        \n",
    "        e_bits = param_to_bit(e_bits_param)\n",
    "        m_bits = param_to_bit(m_bits_param)\n",
    "        s = s_param\n",
    "                \n",
    "        e_bits_int = int(torch.round(e_bits).item())\n",
    "        m_bits_int = int(torch.round(m_bits).item())\n",
    "        s_int = int(torch.round(s).item())\n",
    "\n",
    "        #print(\"shape x: \", x.shape)\n",
    "        #print(\"shape grad_output: \", grad_output.shape)\n",
    "\n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output\n",
    "        \n",
    "        # 1) Gradient wrt x: approximate with central difference\n",
    "        \"\"\"\n",
    "        grad_x = None\n",
    "        if True:\n",
    "            delta = 0.01            \n",
    "\n",
    "            f_plus2  = fake_float_truncate(x + 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_plus   = fake_float_truncate(x + 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus  = fake_float_truncate(x - 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus2 = fake_float_truncate(x - 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "            grad_x = grad_output * der\n",
    "        \"\"\"\n",
    "                \n",
    "        # 2) Gradient wrt e_bits: approximate with central difference\n",
    "        grad_e_bits = None\n",
    "        if e_bits_param.requires_grad:\n",
    "            \n",
    "            if(e_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, s_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int    , m_bits_int, s_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int + 2, m_bits_int, s_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, s_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int - 1, m_bits_int, s_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int - 2, m_bits_int, s_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_e_bits = grad_output * der * e_bits\n",
    "        \n",
    "        # 3) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_m_bits = None\n",
    "        if m_bits_param.requires_grad:\n",
    "            \n",
    "            if(m_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, s_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int    , s_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int + 2, s_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, s_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int - 1, s_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int - 2, s_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_m_bits = grad_output * der * m_bits\n",
    "       \n",
    "        # 4) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_s_bits = None\n",
    "        if s_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int, s_int + 2)\n",
    "            f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int, s_int + 1)\n",
    "            f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int, s_int - 1)\n",
    "            f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int, s_int - 2)\n",
    "            \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0 \n",
    "            grad_s_bits = grad_output * der\n",
    "             \n",
    "        return grad_x, grad_e_bits, grad_m_bits, grad_s_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### differentiable Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoundSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        return grad_output\n",
    "    \n",
    "class RoundFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.round(input + 2*delta)\n",
    "        f_plus   = torch.round(input + 1*delta)\n",
    "        f_minus  = torch.round(input - 1*delta)\n",
    "        f_minus2 = torch.round(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "class RoundSIG(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function that does a hard round in forward,\n",
    "    but uses a sigmoid-based approximation for the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, alpha=10.0):\n",
    "        \"\"\"\n",
    "        Forward pass: returns torch.round(input).\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.alpha = alpha\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: approximate the gradient of round(x)\n",
    "        with the derivative of a sigmoid centered at the fractional midpoint (0.5).\n",
    "        \"\"\"\n",
    "        (input,) = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "\n",
    "        # Fractional part\n",
    "        frac = input - torch.floor(input)\n",
    "\n",
    "        # Sigmoid of (fractional_part - 0.5), scaled by alpha\n",
    "        s = torch.sigmoid(alpha * (frac - 0.5))\n",
    "\n",
    "        # Derivative of sigmoid = alpha * s * (1 - s)\n",
    "        grad_input = alpha * s * (1 - s) * grad_output\n",
    "        return grad_input, None  # alpha is not a tensor that requires grad\n",
    "    \n",
    "def diff_round(x):\n",
    "    #return RoundSTE.apply(x)\n",
    "    return RoundFDE.apply(x)\n",
    "    #return RoundSIG.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable Floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloorSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass uses standard floor\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-through pass: just return the gradient as-is\n",
    "        return grad_output\n",
    "\n",
    "class FloorFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.floor(input + 2*delta)\n",
    "        f_plus   = torch.floor(input + 1*delta)\n",
    "        f_minus  = torch.floor(input - 1*delta)\n",
    "        f_minus2 = torch.floor(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "def diff_floor(input):\n",
    "    #return FloorSTE.apply(input)\n",
    "    return FloorFDE.apply(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxObserver(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We store running min/max\n",
    "        self.register_buffer(\"min_val\", torch.tensor(float(\"inf\")))\n",
    "        self.register_buffer(\"max_val\", torch.tensor(float(\"-inf\")))\n",
    "        # You could also store averaging stats, etc.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update running min/max\n",
    "        self.min_val = torch.min(self.min_val, x.detach().min())\n",
    "        self.max_val = torch.max(self.max_val, x.detach().max())\n",
    "        return x  # Just pass through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed point quanizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, observer, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.observer = observer\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        \n",
    "        # 1) Get min/max from observer\n",
    "        min_val = self.observer.min_val\n",
    "        max_val = self.observer.max_val\n",
    "\n",
    "        # If they're not valid, skip\n",
    "        #if min_val >= max_val:\n",
    "        #    return x\n",
    "\n",
    "        # 2) Compute scale and zero_point\n",
    "        # For an unsigned 4-bit range, we can hold values 0..15\n",
    "        # qmin, qmax = 0, (1 << b_int) - 1  # e.g. 0..15\n",
    "        qmin, qmax = torch.tensor(float(0)), 2**b_int - 1  # e.g. 0..15\n",
    "        \n",
    "        qmin = qmin.to(x.device)\n",
    "        #qmax = qmax.to(x.device)\n",
    "        max_val = max_val.to(x.device)\n",
    "        min_val = min_val.to(x.device)\n",
    "\n",
    "        # Typical formula for scale/zero-point:\n",
    "        scale = (max_val - min_val) / float(qmax - qmin)\n",
    "        zero_point = qmin - diff_round(min_val / scale)\n",
    "\n",
    "        # 3) Quantize (in floating point)\n",
    "        # clamp to range of [qmin, qmax]\n",
    "        q_x = torch.clamp(diff_round(x / scale + zero_point), qmin, qmax)\n",
    "\n",
    "        # 4) Dequantize back to float\n",
    "        fq_x = (q_x - zero_point) * scale\n",
    "        return fq_x\n",
    "\n",
    "class FixedPointFakeQuantize2(nn.Module):\n",
    "    def __init__(self, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(0)), requires_grad=requires_grad)\n",
    "        self.zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        bits_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        zero_point_int = diff_round(self.zero_point)\n",
    "        \n",
    "        qmin = torch.tensor(float(0)).to(x.device)\n",
    "        qmax = 2**bits_int - 1  # e.g. 0..15\n",
    "        \n",
    "        #from float to fixed point, and quantize accordingly\n",
    "        q_x = torch.clamp(diff_round(x / 2**scale_int + zero_point_int), qmin, qmax)\n",
    "\n",
    "        # from quantized fixed point to float\n",
    "        fq_x = (q_x - zero_point_int) * 2**scale_int\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"bits: \", self.bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())\n",
    "        print(\"zero point: \", self.zero_point.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatingPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, m_bits=23, e_bits=8, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.e_bits = nn.Parameter(torch.tensor(float(e_bits)), requires_grad=requires_grad)\n",
    "        self.m_bits = nn.Parameter(torch.tensor(float(m_bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(0)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e_bits_int = torch.clamp(diff_round(self.e_bits), 0, 32)\n",
    "        m_bits_int = torch.clamp(diff_round(self.m_bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        \n",
    "        sign = x.sign()\n",
    "        abs_x = x.abs().clamp(min=1e-45)\n",
    "\n",
    "        #recover the floatint point representation\n",
    "        #exponent \\in {-2**7,..,2**7-1}\n",
    "        #mantissa \\in {1.0,...,2.0}\n",
    "\n",
    "        exponent = diff_floor(torch.log2(abs_x)).clamp(min=1e-45)\n",
    "        mantissa = abs_x / (2**exponent)\n",
    "    \n",
    "        # truncate exponent\n",
    "        # lets parameterize the exponent as a constant value + a variable value\n",
    "        # the constant part is 2**7-1 in standar floating point, but we will learn it\n",
    "        # the variable part \\in {0,..,2**8-1}\n",
    "        # lets say exponent = v_exponent - 2**(bits-1)-1 + c_exponent\n",
    "        # so v_exponent = exponent + 2**(bits-1)-1 - c_exponent\n",
    "        c_exponent = scale_int\n",
    "        v_exponent = exponent + (2**(e_bits_int-1)-1) - c_exponent\n",
    "        \n",
    "        # the valriable part is clamped to the alloted bits\n",
    "        q_min = torch.tensor(float(0)).to(x.device)\n",
    "        q_max = 2**e_bits_int-1\n",
    "        q_exponent = torch.clamp(v_exponent, q_min, q_max) - (2**(e_bits_int-1)-1) + c_exponent\n",
    "    \n",
    "        # truncate mantissa\n",
    "        # this just removes the less significant bits\n",
    "        m_scale = 2.0 ** m_bits_int\n",
    "        q_mantissa = diff_floor(mantissa * m_scale) / m_scale\n",
    "    \n",
    "        # from quantized floatint point to float\n",
    "        fq_x = sign * (2**q_exponent) * q_mantissa\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.e_bits, self.m_bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"e_bits: \", self.e_bits.detach().item())\n",
    "        print(\"m_bits: \", self.m_bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float32 example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * input_size[1]//8 * input_size[2]//8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantWrapper(nn.Module):\n",
    "    def __init__(self, module, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        #self.observer = MinMaxObserver()\n",
    "        #self.fake_quant = FixedPointFakeQuantize(self.observer, requires_grad=optimizeQuant)\n",
    "        #self.fake_quant = FixedPointFakeQuantize2(requires_grad=optimizeQuant)\n",
    "        self.fake_quant_input = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "        self.fake_quant_weight = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.observer(x)\n",
    "        x = self.fake_quant_input(x)\n",
    "        w = self.fake_quant_weight(self.module.weight)\n",
    "        b = self.module.bias\n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            return F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            return F.Linear(x, w, b)\n",
    "        else:\n",
    "            return self.module(x)\n",
    "        \n",
    "    def getBits(self):\n",
    "        return self.fake_quant_input.getBits() + self.fake_quant_weight.getBits()\n",
    "        #return self.fake_quant_weight.getBits()\n",
    "    \n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        self.fake_quant_input.printParams()\n",
    "        print(\"weight quant params: \")\n",
    "        self.fake_quant_weight.printParams()\n",
    "\n",
    "class QuantSimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = QuantWrapper(nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = QuantWrapper(nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = QuantWrapper(nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = QuantWrapper(nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 = QuantWrapper(nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = QuantWrapper(nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * input_size[1]//8 * input_size[2]//8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwidth_squared(model):\n",
    "    s = 0.0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapper):\n",
    "            for bit in module.getBits():\n",
    "                s += bit ** 2\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBitWidths(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapper):\n",
    "            print(\"module: \", name)\n",
    "            module.printQuantParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, lambda_bw=1.0e-3):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_ce = F.cross_entropy(output, target)\n",
    "        penalty_bw = bitwidth_squared(model) \n",
    "        #penalty_bw = bitwidth_sum(model) \n",
    "        loss = loss_ce + lambda_bw*penalty_bw\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #if batch_idx % 200 == 0:\n",
    "        #    print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "        #          f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Train set: Average loss: {train_loss:.4f}\")\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy} ({100.0*accuracy:.2f}%)\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using divice  cuda\n",
      "Train set: Average loss: 7.1140\n",
      "Test set: Average loss: 0.0184, Accuracy: 0.994 (99.40%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.94096565246582\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.941030502319336\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.94104766845703\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.94103240966797\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.940954208374023\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.94103240966797\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.940996170043945\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.94103240966797\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.940929412841797\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.94103240966797\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.941043853759766\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.941058158874512\n",
      "m_bits:  22.94103240966797\n",
      "scale:  0.0\n",
      "Train set: Average loss: 7.0641\n",
      "Test set: Average loss: 0.0200, Accuracy: 0.9941 (99.41%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88219451904297\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88208770751953\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.882144927978516\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88212013244629\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88202667236328\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88212013244629\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88213348388672\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88212013244629\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88194465637207\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88212013244629\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.882164001464844\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.882312297821045\n",
      "m_bits:  22.88212013244629\n",
      "scale:  0.0\n",
      "Train set: Average loss: 7.0181\n",
      "Test set: Average loss: 0.0276, Accuracy: 0.9913 (99.13%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.823150634765625\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.82324981689453\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.82326889038086\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.823265075683594\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.82313346862793\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.823265075683594\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.823328018188477\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.823265075683594\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.822998046875\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.823265075683594\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.823375701904297\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.82377815246582\n",
      "m_bits:  22.823265075683594\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.9730\n",
      "Test set: Average loss: 0.0144, Accuracy: 0.9949 (99.49%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.76443862915039\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764511108398438\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764448165893555\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764516830444336\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764326095581055\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764516830444336\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764570236206055\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764516830444336\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764144897460938\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764516830444336\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.76467514038086\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.765450954437256\n",
      "m_bits:  22.764516830444336\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.9280\n",
      "Test set: Average loss: 0.0290, Accuracy: 0.9913 (99.13%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705644607543945\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.70582389831543\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705642700195312\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705812454223633\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705698013305664\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705812454223633\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705896377563477\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705812454223633\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705469131469727\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705812454223633\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705923080444336\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.707327842712402\n",
      "m_bits:  22.705812454223633\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.8841\n",
      "Test set: Average loss: 0.0224, Accuracy: 0.993 (99.30%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.647228240966797\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.647241592407227\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.646947860717773\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.64718246459961\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.64704704284668\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.64718246459961\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.647245407104492\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.64718246459961\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.646770477294922\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.64718246459961\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.647239685058594\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.6494059562683105\n",
      "m_bits:  22.64718246459961\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.8407\n",
      "Test set: Average loss: 0.0165, Accuracy: 0.9957 (99.57%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.588449478149414\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.588672637939453\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.588380813598633\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.58863067626953\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.5884952545166\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.58863067626953\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.588716506958008\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.58863067626953\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.58816146850586\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.58863067626953\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.588729858398438\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.591681003570557\n",
      "m_bits:  22.58863067626953\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.7994\n",
      "Test set: Average loss: 0.0160, Accuracy: 0.9955 (99.55%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.52994155883789\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.530166625976562\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.529958724975586\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.530113220214844\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.530004501342773\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.530113220214844\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.530261993408203\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.530113220214844\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.529539108276367\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.530113220214844\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.53029441833496\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.53415060043335\n",
      "m_bits:  22.530113220214844\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.7564\n",
      "Test set: Average loss: 0.0217, Accuracy: 0.9934 (99.34%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.471364974975586\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.471752166748047\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.471458435058594\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.47170639038086\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.471586227416992\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.47170639038086\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.471790313720703\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.47170639038086\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.47113037109375\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.47170639038086\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.47186851501465\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.476812362670898\n",
      "m_bits:  22.47170639038086\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.7139\n",
      "Test set: Average loss: 0.0255, Accuracy: 0.9937 (99.37%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.41278648376465\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413360595703125\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413118362426758\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413326263427734\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413166046142578\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413326263427734\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413427352905273\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413326263427734\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.412742614746094\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413326263427734\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413509368896484\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.419661998748779\n",
      "m_bits:  22.413326263427734\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.6721\n",
      "Test set: Average loss: 0.0259, Accuracy: 0.9936 (99.36%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.354293823242188\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.35506820678711\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.35478401184082\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.35503387451172\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.35484504699707\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.35503387451172\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.355112075805664\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.35503387451172\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.354463577270508\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.35503387451172\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.35521697998047\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.362697601318359\n",
      "m_bits:  22.35503387451172\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.6303\n",
      "Test set: Average loss: 0.0329, Accuracy: 0.9909 (99.09%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.2958984375\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.296810150146484\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.296531677246094\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.296783447265625\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.296554565429688\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.296783447265625\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.296884536743164\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.296783447265625\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.29625701904297\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.296783447265625\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.29698371887207\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.305914878845215\n",
      "m_bits:  22.296783447265625\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.5921\n",
      "Test set: Average loss: 0.0315, Accuracy: 0.991 (99.10%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.237377166748047\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238630294799805\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238378524780273\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238603591918945\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238422393798828\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238603591918945\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238712310791016\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238603591918945\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238035202026367\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238603591918945\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238800048828125\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.249311447143555\n",
      "m_bits:  22.238603591918945\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.5494\n",
      "Test set: Average loss: 0.0195, Accuracy: 0.9954 (99.54%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.179195404052734\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180496215820312\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180179595947266\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180477142333984\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180307388305664\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180477142333984\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180622100830078\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180477142333984\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.17995834350586\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180477142333984\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180665969848633\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.19288444519043\n",
      "m_bits:  22.180477142333984\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.5065\n",
      "Test set: Average loss: 0.0171, Accuracy: 0.9953 (99.53%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.121110916137695\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.12242889404297\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.122116088867188\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.12240982055664\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.122177124023438\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.12240982055664\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.12255096435547\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.12240982055664\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.121841430664062\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.12240982055664\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.122589111328125\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.136631011962891\n",
      "m_bits:  22.12240982055664\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.4655\n",
      "Test set: Average loss: 0.0173, Accuracy: 0.9957 (99.57%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.06304931640625\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064409255981445\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064102172851562\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064395904541016\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064149856567383\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064395904541016\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064516067504883\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064395904541016\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.063804626464844\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064395904541016\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064579010009766\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.0805487632751465\n",
      "m_bits:  22.064395904541016\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.4261\n",
      "Test set: Average loss: 0.0206, Accuracy: 0.9954 (99.54%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.004928588867188\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.006454467773438\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.00612449645996\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.006441116333008\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.006227493286133\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.006441116333008\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.006572723388672\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.006441116333008\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.005828857421875\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.006441116333008\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.006593704223633\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  7.024634838104248\n",
      "m_bits:  22.006441116333008\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.3883\n",
      "Test set: Average loss: 0.0232, Accuracy: 0.9934 (99.34%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.946964263916016\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.948537826538086\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.9482364654541\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.948530197143555\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.94825553894043\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.948530197143555\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.948692321777344\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.948530197143555\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.94801139831543\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.948530197143555\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.948665618896484\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.968886852264404\n",
      "m_bits:  21.948530197143555\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.3467\n",
      "Test set: Average loss: 0.0148, Accuracy: 0.9962 (99.62%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.88921356201172\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890695571899414\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890411376953125\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890687942504883\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890409469604492\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890687942504883\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890836715698242\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890687942504883\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890174865722656\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890687942504883\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890823364257812\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.913301467895508\n",
      "m_bits:  21.890687942504883\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.3052\n",
      "Test set: Average loss: 0.0166, Accuracy: 0.9959 (99.59%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.831424713134766\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.832870483398438\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.832597732543945\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.832866668701172\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.832571029663086\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.832866668701172\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.833024978637695\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.832866668701172\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.832382202148438\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.832866668701172\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.8330135345459\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.857875823974609\n",
      "m_bits:  21.832866668701172\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.2660\n",
      "Test set: Average loss: 0.0213, Accuracy: 0.9943 (99.43%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.7735652923584\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.7751407623291\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.774852752685547\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.775136947631836\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.7747859954834\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.775136947631836\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.77525520324707\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.775136947631836\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.77463150024414\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.775136947631836\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.775270462036133\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.802608489990234\n",
      "m_bits:  21.775136947631836\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.2274\n",
      "Test set: Average loss: 0.0203, Accuracy: 0.9957 (99.57%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.715822219848633\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.717411041259766\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.717145919799805\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.7174072265625\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.717058181762695\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.7174072265625\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.71755599975586\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.7174072265625\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.716934204101562\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.7174072265625\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.717548370361328\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.747497081756592\n",
      "m_bits:  21.7174072265625\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.1886\n",
      "Test set: Average loss: 0.0181, Accuracy: 0.9957 (99.57%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.658157348632812\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.65977668762207\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.659488677978516\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.65977668762207\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.659366607666016\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.65977668762207\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.659908294677734\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.65977668762207\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.659292221069336\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.65977668762207\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.65987777709961\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.692539215087891\n",
      "m_bits:  21.65977668762207\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.1497\n",
      "Test set: Average loss: 0.0198, Accuracy: 0.9951 (99.51%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.600627899169922\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.60215950012207\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.601850509643555\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.60215950012207\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.60175323486328\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.60215950012207\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.60230827331543\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.60215950012207\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.601648330688477\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.60215950012207\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.60226058959961\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.637732028961182\n",
      "m_bits:  21.60215950012207\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.1122\n",
      "Test set: Average loss: 0.0224, Accuracy: 0.996 (99.60%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.54296112060547\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.54458999633789\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.54423713684082\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.544591903686523\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.544206619262695\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.544591903686523\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.544782638549805\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.544591903686523\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.544044494628906\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.544591903686523\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.544702529907227\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.58307409286499\n",
      "m_bits:  21.544591903686523\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.0732\n",
      "Test set: Average loss: 0.0205, Accuracy: 0.9961 (99.61%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.48546600341797\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.487085342407227\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.48673439025879\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.48708724975586\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.48666000366211\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.48708724975586\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.487276077270508\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.48708724975586\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.48649787902832\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.48708724975586\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.487201690673828\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.528562545776367\n",
      "m_bits:  21.48708724975586\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.0358\n",
      "Test set: Average loss: 0.0568, Accuracy: 0.988 (98.80%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.427978515625\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.429580688476562\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.429248809814453\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.429582595825195\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.42919158935547\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.429582595825195\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.42978858947754\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.429582595825195\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.429019927978516\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.429582595825195\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.429702758789062\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.47419548034668\n",
      "m_bits:  21.429582595825195\n",
      "scale:  0.0\n",
      "Train set: Average loss: 6.0005\n",
      "Test set: Average loss: 0.0461, Accuracy: 0.9905 (99.05%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.370582580566406\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.372169494628906\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.371797561645508\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.372175216674805\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.37171173095703\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.372175216674805\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.372352600097656\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.372175216674805\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.371599197387695\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.372175216674805\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.372255325317383\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.419970512390137\n",
      "m_bits:  21.372175216674805\n",
      "scale:  0.0\n",
      "Train set: Average loss: 5.9597\n",
      "Test set: Average loss: 0.0180, Accuracy: 0.9959 (99.59%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.313167572021484\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.314777374267578\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.31440544128418\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.314783096313477\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.314350128173828\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.314783096313477\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.31496238708496\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.314783096313477\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.31416893005371\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.314783096313477\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.314863204956055\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.365886211395264\n",
      "m_bits:  21.314783096313477\n",
      "scale:  0.0\n",
      "Train set: Average loss: 5.9209\n",
      "Test set: Average loss: 0.0265, Accuracy: 0.9944 (99.44%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.255783081054688\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.25739288330078\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.257076263427734\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.257402420043945\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.25701904296875\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.257402420043945\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.257606506347656\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.257402420043945\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.256792068481445\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.257402420043945\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.25750732421875\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.311940670013428\n",
      "m_bits:  21.257402420043945\n",
      "scale:  0.0\n",
      "Train set: Average loss: 5.8835\n",
      "Test set: Average loss: 0.0252, Accuracy: 0.9946 (99.46%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.198421478271484\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.20011329650879\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.199796676635742\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.200122833251953\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.199695587158203\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.200122833251953\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.200319290161133\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.200122833251953\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.199474334716797\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.200122833251953\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.200214385986328\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.258131504058838\n",
      "m_bits:  21.200122833251953\n",
      "scale:  0.0\n",
      "Train set: Average loss: 5.8476\n",
      "Test set: Average loss: 0.0177, Accuracy: 0.9952 (99.52%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.14116668701172\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.142833709716797\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.142541885375977\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.14284324645996\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.142412185668945\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.14284324645996\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.14305877685547\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.14284324645996\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.1422061920166\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.14284324645996\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.1429386138916\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.2044572830200195\n",
      "m_bits:  21.14284324645996\n",
      "scale:  0.0\n",
      "Train set: Average loss: 5.8086\n",
      "Test set: Average loss: 0.0192, Accuracy: 0.9958 (99.58%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.083904266357422\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.085567474365234\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.08530044555664\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.08557891845703\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.08513832092285\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.08557891845703\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.085783004760742\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.08557891845703\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.084970474243164\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.08557891845703\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.085674285888672\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.150917053222656\n",
      "m_bits:  21.08557891845703\n",
      "scale:  0.0\n",
      "Train set: Average loss: 5.7711\n",
      "Test set: Average loss: 0.0161, Accuracy: 0.9966 (99.66%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.02672576904297\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.028400421142578\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.028114318847656\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.028411865234375\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.02794647216797\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.028411865234375\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.028615951538086\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.028411865234375\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.02779769897461\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.028411865234375\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.028505325317383\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.097508430480957\n",
      "m_bits:  21.028411865234375\n",
      "scale:  0.0\n",
      "Train set: Average loss: 5.7344\n",
      "Test set: Average loss: 0.0165, Accuracy: 0.9966 (99.66%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.969558715820312\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.971233367919922\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.970947265625\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.97124481201172\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.970779418945312\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.97124481201172\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.97144889831543\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.97124481201172\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.970630645751953\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.97124481201172\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.971338272094727\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  6.044229030609131\n",
      "m_bits:  20.97124481201172\n",
      "scale:  0.0\n",
      "Train set: Average loss: 5.6979\n",
      "Test set: Average loss: 0.0167, Accuracy: 0.9967 (99.67%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.912391662597656\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.914066314697266\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.913793563842773\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.914077758789062\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.913612365722656\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.914077758789062\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.914281845092773\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.914077758789062\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.913463592529297\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.914077758789062\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.91417121887207\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.991079330444336\n",
      "m_bits:  20.914077758789062\n",
      "scale:  0.0\n",
      "Train set: Average loss: 5.6616\n",
      "Test set: Average loss: 0.0167, Accuracy: 0.9967 (99.67%)\n",
      "module:  conv1\n",
      "input quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.855323791503906\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.85700225830078\n",
      "scale:  0.0\n",
      "module:  conv2\n",
      "input quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.856739044189453\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.85701560974121\n",
      "scale:  0.0\n",
      "module:  conv3\n",
      "input quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.85651397705078\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.85701560974121\n",
      "scale:  0.0\n",
      "module:  conv4\n",
      "input quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.857213973999023\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.85701560974121\n",
      "scale:  0.0\n",
      "module:  conv5\n",
      "input quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.856393814086914\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.85701560974121\n",
      "scale:  0.0\n",
      "module:  conv6\n",
      "input quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.85711669921875\n",
      "scale:  0.0\n",
      "weight quant params: \n",
      "e_bits:  5.938055515289307\n",
      "m_bits:  20.85701560974121\n",
      "scale:  0.0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"using divice \", device)\n",
    "\n",
    "best_model_path = f\"train_weights_and_quant_{dataset}_best_model.pth\"\n",
    "quant_model_path = f\"train_weights_and_quant_{dataset}_quant_model.pth\"\n",
    "\n",
    "load_model_path = None\n",
    "save_model_path = best_model_path\n",
    "\n",
    "if(os.path.isfile(quant_model_path)):\n",
    "    load_model_path = quant_model_path\n",
    "    save_model_path =quant_model_path\n",
    "elif(os.path.isfile(best_model_path)):\n",
    "    load_model_path = best_model_path   \n",
    "    save_model_path = quant_model_path\n",
    "    \n",
    "if(load_model_path):\n",
    "    # Create model\n",
    "    # model = SimpleQuantizedMLP(e_bits=4.0, m_bits=4.0, num_classes=len(classes)).to(device)\n",
    "    model = QuantSimpleCIFAR10Model(num_classes=len(classes), optimizeQuant=True).to(device)\n",
    "    #model = SimpleCIFAR10Model(num_classes=len(classes)).to(device)\n",
    "    model.load_state_dict(torch.load(quant_model_path, weights_only=True))\n",
    "else:\n",
    "    model = QuantSimpleCIFAR10Model(num_classes=len(classes), optimizeQuant=False).to(device)\n",
    "\n",
    "# Create optimizer (SGD or Adam)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "# Train for some epochs\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    accuracy = test(model, device, test_loader)\n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "        \n",
    "    printBitWidths(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
