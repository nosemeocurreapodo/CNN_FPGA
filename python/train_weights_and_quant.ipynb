{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "batch_size = 1024\n",
    "dataset = \"MNIST\"\n",
    "#dataset = \"CIFAR10\"\n",
    "#dataset = \"CIFAR100\"\n",
    "#dataset = \"FMNIST\"\n",
    "\n",
    "if(dataset == \"MNIST\"):\n",
    "    # 1) MNIST Dataset & Dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "    input_size = (1, 32, 32)\n",
    "\n",
    "if(dataset == \"FMNIST\"):\n",
    "    # 1) MNIST Dataset & Dataloaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset  = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "    input_size = (1, 32, 32)\n",
    "    \n",
    "if(dataset == \"CIFAR10\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    input_size = (3, 32, 32)\n",
    "\n",
    "if(dataset == \"CIFAR100\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = [x for x in range(100)]\n",
    "\n",
    "    input_size = (3, 32, 32)\n",
    "    \n",
    "if(dataset == \"IMAGENET\"):\n",
    "    # 2) CIFAR-10 dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(90, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))  # mean, std\n",
    "    ])\n",
    "\n",
    "    # Transformations for testing: just convert and normalize\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "        #                    (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageNet(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.ImageNet(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = [x for x in range(100)]\n",
    "\n",
    "    input_size = (3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: a batch of images in shape (B, C, H, W) or a single image in (C, H, W).\n",
    "    \"\"\"\n",
    "    # If it's a batch of images (4D), make a grid first\n",
    "    if len(img_tensor.shape) == 4:\n",
    "        img_tensor = torchvision.utils.make_grid(img_tensor)\n",
    "    # Unnormalize\n",
    "    #img_tensor = unnormalize(img_tensor)\n",
    "    # Convert to numpy\n",
    "    npimg = img_tensor.numpy()\n",
    "    # Transpose from (C, H, W) to (H, W, C)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(images[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_bit(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(x)\n",
    "\n",
    "def bit_to_param(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_float_truncate(x: torch.Tensor, e_bits_int: int, m_bits_int: int, scale_int: int) -> torch.Tensor:\n",
    "\n",
    "    sign = x.sign()\n",
    "    abs_x = x.abs().clamp(min=1e-45) / 2**scale_int\n",
    "\n",
    "    #recover the floatint point representation\n",
    "    #exponent \\in {-2**7,..,2**7-1}\n",
    "    #mantissa \\in {1.0,...,2.0}\n",
    "\n",
    "    exponent = torch.floor(torch.log2(abs_x)).clamp(min=1e-45)\n",
    "    mantissa = abs_x / (2**exponent)\n",
    "    \n",
    "    #print(\"exp: \", exponent)\n",
    "    #print(\"mantissa: \", mantissa)\n",
    "\n",
    "    # truncate exponent\n",
    "    # lets parameterize the exponent as a constant value + a variable value\n",
    "    # the constant part is 2**7-1 in standar floating point, but we will learn it\n",
    "    # the variable part \\in {0,..,2**8-1}\n",
    "    # lets say exponent = v_exponent - 2**(bits-1)-1 + c_exponent\n",
    "    # so v_exponent = exponent + 2**(bits-1)-1 - c_exponent\n",
    "    c_exponent = 0 #scale_int\n",
    "    z_exponent = (2**(e_bits_int-1)-1)\n",
    "    if e_bits_int == 0:\n",
    "        z_exponent = 0\n",
    "    v_exponent = exponent + z_exponent - c_exponent\n",
    "    \n",
    "    #print(\"v exponent: \", v_exponent)\n",
    "    \n",
    "    # the valriable part is clamped to the alloted bits\n",
    "    q_min = torch.tensor(float(0)).to(x.device)\n",
    "    q_max = 2**e_bits_int-1\n",
    "    q_v_exponent = torch.clamp(v_exponent, q_min, q_max)\n",
    "    q_exponent = q_v_exponent - z_exponent + c_exponent\n",
    "\n",
    "    #print(\"q v exponent: \", q_v_exponent)\n",
    "    #print(\"q exponent: \", q_exponent)\n",
    "\n",
    "    # truncate mantissa\n",
    "    # this just removes the less significant bits\n",
    "    m_scale = 2.0 ** m_bits_int\n",
    "    q_mantissa = torch.floor(mantissa * m_scale) / m_scale\n",
    "\n",
    "    #print(\"q mantissa \", q_mantissa)\n",
    "\n",
    "    # from quantized floatint point to float\n",
    "    fq_x = sign * (2**q_exponent) * q_mantissa * 2**scale_int\n",
    "    return fq_x\n",
    "\n",
    "class FakeFloatFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e_bits_param, m_bits_param, scale_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, e_bits_param, m_bits_param, scale_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).item())\n",
    "        s_int = int(torch.round(scale_param).item())\n",
    "\n",
    "        out = fake_float_truncate(x, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "        #if(m_bits_int == 0):\n",
    "        #    print(\"input\")\n",
    "        #    print(x)\n",
    "        #    print(\"output\")\n",
    "        #    print(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, e_bits_param, m_bits_param, scale_param = ctx.saved_tensors\n",
    "        \n",
    "        e_bits = param_to_bit(e_bits_param)\n",
    "        m_bits = param_to_bit(m_bits_param)\n",
    "        scale = scale_param\n",
    "                \n",
    "        e_bits_int = int(torch.round(e_bits).item())\n",
    "        m_bits_int = int(torch.round(m_bits).item())\n",
    "        scale_int = int(torch.round(scale).item())\n",
    "\n",
    "        #print(\"shape x: \", x.shape)\n",
    "        #print(\"shape grad_output: \", grad_output.shape)\n",
    "\n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output\n",
    "        \n",
    "        # 1) Gradient wrt x: approximate with central difference\n",
    "        \"\"\"\n",
    "        grad_x = None\n",
    "        if True:\n",
    "            delta = 0.01            \n",
    "\n",
    "            f_plus2  = fake_float_truncate(x + 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_plus   = fake_float_truncate(x + 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus  = fake_float_truncate(x - 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus2 = fake_float_truncate(x - 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "            grad_x = grad_output * der\n",
    "        \"\"\"\n",
    "                \n",
    "        # 2) Gradient wrt e_bits: approximate with central difference\n",
    "        grad_e_bits = None\n",
    "        if e_bits_param.requires_grad:\n",
    "            \n",
    "            if(e_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int    , m_bits_int, scale_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int + 2, m_bits_int, scale_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int + 1, m_bits_int, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int - 1, m_bits_int, scale_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int - 2, m_bits_int, scale_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_e_bits = grad_output * der * e_bits\n",
    "        \n",
    "        # 3) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_m_bits = None\n",
    "        if m_bits_param.requires_grad:\n",
    "            \n",
    "            if(m_bits_int < 2):\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int    , scale_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int + 2, scale_int)\n",
    "                f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int + 1, scale_int)\n",
    "                f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int - 1, scale_int)\n",
    "                f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int - 2, scale_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_m_bits = grad_output * der * m_bits\n",
    "       \n",
    "        # 4) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_scale_bits = None\n",
    "        if scale_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int + 2)\n",
    "            f_plus   = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int + 1)\n",
    "            f_minus  = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int - 1)\n",
    "            f_minus2 = fake_float_truncate(x, e_bits_int, m_bits_int, scale_int - 2)\n",
    "            \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0 \n",
    "            grad_scale_bits = grad_output * der\n",
    "             \n",
    "        return grad_x, grad_e_bits, grad_m_bits, grad_scale_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_bits  7  m_bits  29  scale  2\n",
      "in:  tensor([-1.7311])  out:  tensor([-1.7311])\n",
      "e_bits  8  m_bits  6  scale  0\n",
      "in:  tensor([-13.9572])  out:  tensor([-13.8750])\n",
      "e_bits  7  m_bits  19  scale  -1\n",
      "in:  tensor([-1.5395])  out:  tensor([-1.5395])\n",
      "e_bits  1  m_bits  29  scale  2\n",
      "in:  tensor([-32.1813])  out:  tensor([-8.0453])\n",
      "e_bits  5  m_bits  12  scale  1\n",
      "in:  tensor([-24.8515])  out:  tensor([-24.8477])\n",
      "e_bits  8  m_bits  6  scale  -2\n",
      "in:  tensor([-48.4376])  out:  tensor([-48.])\n",
      "e_bits  1  m_bits  24  scale  0\n",
      "in:  tensor([-26.6318])  out:  tensor([-3.3290])\n",
      "e_bits  8  m_bits  17  scale  2\n",
      "in:  tensor([18.2174])  out:  tensor([18.2173])\n",
      "e_bits  6  m_bits  29  scale  -2\n",
      "in:  tensor([-14.7085])  out:  tensor([-14.7085])\n",
      "e_bits  7  m_bits  11  scale  2\n",
      "in:  tensor([-39.0426])  out:  tensor([-39.0312])\n",
      "e_bits  8  m_bits  26  scale  0\n",
      "in:  tensor([-12.3447])  out:  tensor([-12.3447])\n",
      "e_bits  2  m_bits  20  scale  0\n",
      "in:  tensor([10.9504])  out:  tensor([5.4752])\n",
      "e_bits  3  m_bits  3  scale  -1\n",
      "in:  tensor([-7.1969])  out:  tensor([-7.])\n",
      "e_bits  8  m_bits  18  scale  2\n",
      "in:  tensor([4.5329])  out:  tensor([4.5329])\n",
      "e_bits  2  m_bits  20  scale  -2\n",
      "in:  tensor([-32.9332])  out:  tensor([-1.0292])\n",
      "e_bits  4  m_bits  17  scale  1\n",
      "in:  tensor([16.6882])  out:  tensor([16.6882])\n",
      "e_bits  6  m_bits  28  scale  0\n",
      "in:  tensor([22.8949])  out:  tensor([22.8949])\n",
      "e_bits  6  m_bits  30  scale  1\n",
      "in:  tensor([38.5656])  out:  tensor([38.5656])\n",
      "e_bits  4  m_bits  23  scale  2\n",
      "in:  tensor([-9.0521])  out:  tensor([-9.0521])\n",
      "e_bits  3  m_bits  2  scale  2\n",
      "in:  tensor([24.2858])  out:  tensor([24.])\n",
      "e_bits  2  m_bits  0  scale  1\n",
      "in:  tensor([-47.2844])  out:  tensor([-8.])\n",
      "e_bits  1  m_bits  25  scale  0\n",
      "in:  tensor([-16.0798])  out:  tensor([-2.0100])\n",
      "e_bits  9  m_bits  13  scale  2\n",
      "in:  tensor([0.8093])  out:  tensor([0.8091])\n",
      "e_bits  5  m_bits  8  scale  -2\n",
      "in:  tensor([34.2446])  out:  tensor([34.1250])\n",
      "e_bits  4  m_bits  11  scale  -2\n",
      "in:  tensor([11.2353])  out:  tensor([11.2344])\n",
      "e_bits  7  m_bits  19  scale  1\n",
      "in:  tensor([8.4632])  out:  tensor([8.4632])\n",
      "e_bits  1  m_bits  7  scale  1\n",
      "in:  tensor([-5.0219])  out:  tensor([-5.])\n",
      "e_bits  1  m_bits  2  scale  0\n",
      "in:  tensor([30.9407])  out:  tensor([3.5000])\n",
      "e_bits  7  m_bits  26  scale  -2\n",
      "in:  tensor([-22.6184])  out:  tensor([-22.6184])\n",
      "e_bits  1  m_bits  30  scale  1\n",
      "in:  tensor([-35.3356])  out:  tensor([-4.4170])\n",
      "e_bits  7  m_bits  20  scale  0\n",
      "in:  tensor([10.0013])  out:  tensor([10.0013])\n",
      "e_bits  6  m_bits  9  scale  -2\n",
      "in:  tensor([37.1094])  out:  tensor([37.0625])\n",
      "e_bits  3  m_bits  23  scale  1\n",
      "in:  tensor([-45.5584])  out:  tensor([-45.5584])\n",
      "e_bits  4  m_bits  11  scale  2\n",
      "in:  tensor([23.6332])  out:  tensor([23.6328])\n",
      "e_bits  5  m_bits  12  scale  2\n",
      "in:  tensor([24.5560])  out:  tensor([24.5547])\n",
      "e_bits  1  m_bits  28  scale  2\n",
      "in:  tensor([11.5996])  out:  tensor([11.5996])\n",
      "e_bits  1  m_bits  25  scale  2\n",
      "in:  tensor([-22.7288])  out:  tensor([-11.3644])\n",
      "e_bits  5  m_bits  16  scale  0\n",
      "in:  tensor([-45.3008])  out:  tensor([-45.3003])\n",
      "e_bits  5  m_bits  8  scale  0\n",
      "in:  tensor([16.5637])  out:  tensor([16.5625])\n",
      "e_bits  6  m_bits  13  scale  0\n",
      "in:  tensor([1.3354])  out:  tensor([1.3353])\n",
      "e_bits  7  m_bits  10  scale  0\n",
      "in:  tensor([-15.9907])  out:  tensor([-15.9844])\n",
      "e_bits  10  m_bits  13  scale  -2\n",
      "in:  tensor([8.4388])  out:  tensor([8.4385])\n",
      "e_bits  1  m_bits  25  scale  2\n",
      "in:  tensor([-3.1910])  out:  tensor([-3.1910])\n",
      "e_bits  4  m_bits  21  scale  1\n",
      "in:  tensor([-43.8761])  out:  tensor([-43.8761])\n",
      "e_bits  7  m_bits  6  scale  0\n",
      "in:  tensor([46.3393])  out:  tensor([46.])\n",
      "e_bits  3  m_bits  1  scale  1\n",
      "in:  tensor([-37.9370])  out:  tensor([-32.])\n",
      "e_bits  8  m_bits  30  scale  -1\n",
      "in:  tensor([-49.7378])  out:  tensor([-49.7378])\n",
      "e_bits  8  m_bits  14  scale  2\n",
      "in:  tensor([-15.0926])  out:  tensor([-15.0923])\n",
      "e_bits  8  m_bits  28  scale  1\n",
      "in:  tensor([14.0404])  out:  tensor([14.0404])\n",
      "e_bits  1  m_bits  3  scale  1\n",
      "in:  tensor([-32.4798])  out:  tensor([-4.])\n",
      "e_bits  8  m_bits  5  scale  1\n",
      "in:  tensor([1.3865])  out:  tensor([1.3750])\n",
      "e_bits  8  m_bits  21  scale  0\n",
      "in:  tensor([3.7868])  out:  tensor([3.7868])\n",
      "e_bits  3  m_bits  16  scale  0\n",
      "in:  tensor([38.3084])  out:  tensor([19.1541])\n",
      "e_bits  5  m_bits  2  scale  -1\n",
      "in:  tensor([-8.0436])  out:  tensor([-8.])\n",
      "e_bits  8  m_bits  28  scale  0\n",
      "in:  tensor([-25.3751])  out:  tensor([-25.3751])\n",
      "e_bits  2  m_bits  19  scale  -1\n",
      "in:  tensor([-17.4050])  out:  tensor([-2.1756])\n",
      "e_bits  10  m_bits  22  scale  0\n",
      "in:  tensor([-12.1390])  out:  tensor([-12.1390])\n",
      "e_bits  8  m_bits  11  scale  0\n",
      "in:  tensor([44.3297])  out:  tensor([44.3281])\n",
      "e_bits  7  m_bits  15  scale  -1\n",
      "in:  tensor([0.1789])  out:  tensor([0.1789])\n",
      "e_bits  1  m_bits  25  scale  -1\n",
      "in:  tensor([48.9633])  out:  tensor([1.5301])\n",
      "e_bits  2  m_bits  16  scale  2\n",
      "in:  tensor([29.4645])  out:  tensor([29.4644])\n",
      "e_bits  5  m_bits  5  scale  0\n",
      "in:  tensor([9.4972])  out:  tensor([9.2500])\n",
      "e_bits  2  m_bits  7  scale  0\n",
      "in:  tensor([45.4270])  out:  tensor([5.6562])\n",
      "e_bits  9  m_bits  24  scale  -1\n",
      "in:  tensor([-26.4129])  out:  tensor([-26.4129])\n",
      "e_bits  9  m_bits  3  scale  1\n",
      "in:  tensor([29.3041])  out:  tensor([28.])\n",
      "e_bits  3  m_bits  14  scale  2\n",
      "in:  tensor([29.3180])  out:  tensor([29.3174])\n",
      "e_bits  3  m_bits  18  scale  -2\n",
      "in:  tensor([-11.0581])  out:  tensor([-5.5290])\n",
      "e_bits  9  m_bits  10  scale  -1\n",
      "in:  tensor([20.8416])  out:  tensor([20.8281])\n",
      "e_bits  4  m_bits  18  scale  0\n",
      "in:  tensor([-23.7665])  out:  tensor([-23.7664])\n",
      "e_bits  6  m_bits  15  scale  0\n",
      "in:  tensor([-2.4205])  out:  tensor([-2.4205])\n",
      "e_bits  2  m_bits  23  scale  2\n",
      "in:  tensor([-49.6220])  out:  tensor([-24.8110])\n",
      "e_bits  4  m_bits  2  scale  -2\n",
      "in:  tensor([-36.0359])  out:  tensor([-32.])\n",
      "e_bits  0  m_bits  7  scale  2\n",
      "in:  tensor([39.0207])  out:  tensor([4.8750])\n",
      "e_bits  2  m_bits  4  scale  -2\n",
      "in:  tensor([9.7155])  out:  tensor([1.1875])\n",
      "e_bits  3  m_bits  2  scale  -2\n",
      "in:  tensor([-44.0361])  out:  tensor([-5.])\n",
      "e_bits  7  m_bits  22  scale  1\n",
      "in:  tensor([17.0230])  out:  tensor([17.0230])\n",
      "e_bits  3  m_bits  28  scale  0\n",
      "in:  tensor([24.6569])  out:  tensor([24.6569])\n",
      "e_bits  6  m_bits  17  scale  -1\n",
      "in:  tensor([-21.4824])  out:  tensor([-21.4823])\n",
      "e_bits  9  m_bits  7  scale  -1\n",
      "in:  tensor([-38.2086])  out:  tensor([-38.])\n",
      "e_bits  4  m_bits  30  scale  1\n",
      "in:  tensor([-9.0691])  out:  tensor([-9.0691])\n",
      "e_bits  4  m_bits  19  scale  1\n",
      "in:  tensor([3.2549])  out:  tensor([3.2549])\n",
      "e_bits  2  m_bits  10  scale  2\n",
      "in:  tensor([-23.6866])  out:  tensor([-23.6719])\n",
      "e_bits  10  m_bits  5  scale  1\n",
      "in:  tensor([6.4156])  out:  tensor([6.3750])\n",
      "e_bits  8  m_bits  14  scale  -2\n",
      "in:  tensor([-48.4470])  out:  tensor([-48.4453])\n",
      "e_bits  5  m_bits  13  scale  1\n",
      "in:  tensor([38.3376])  out:  tensor([38.3359])\n",
      "e_bits  1  m_bits  6  scale  1\n",
      "in:  tensor([-22.9734])  out:  tensor([-5.6875])\n",
      "e_bits  2  m_bits  18  scale  -2\n",
      "in:  tensor([39.3071])  out:  tensor([1.2283])\n",
      "e_bits  8  m_bits  23  scale  -2\n",
      "in:  tensor([-34.2908])  out:  tensor([-34.2908])\n",
      "e_bits  3  m_bits  9  scale  2\n",
      "in:  tensor([-19.9687])  out:  tensor([-19.9375])\n",
      "e_bits  2  m_bits  22  scale  0\n",
      "in:  tensor([-41.4750])  out:  tensor([-5.1844])\n",
      "e_bits  1  m_bits  13  scale  0\n",
      "in:  tensor([-13.5505])  out:  tensor([-3.3875])\n",
      "e_bits  1  m_bits  19  scale  2\n",
      "in:  tensor([-32.6606])  out:  tensor([-8.1651])\n",
      "e_bits  3  m_bits  20  scale  0\n",
      "in:  tensor([45.5521])  out:  tensor([22.7760])\n",
      "e_bits  5  m_bits  8  scale  1\n",
      "in:  tensor([-12.5122])  out:  tensor([-12.5000])\n",
      "e_bits  0  m_bits  28  scale  -1\n",
      "in:  tensor([-40.4601])  out:  tensor([-0.6322])\n",
      "e_bits  6  m_bits  10  scale  0\n",
      "in:  tensor([-1.2065])  out:  tensor([-1.2061])\n",
      "e_bits  8  m_bits  22  scale  1\n",
      "in:  tensor([-24.7632])  out:  tensor([-24.7632])\n",
      "e_bits  6  m_bits  21  scale  -1\n",
      "in:  tensor([0.4734])  out:  tensor([0.4734])\n",
      "e_bits  3  m_bits  14  scale  2\n",
      "in:  tensor([-8.0464])  out:  tensor([-8.0464])\n",
      "e_bits  2  m_bits  6  scale  -1\n",
      "in:  tensor([46.6964])  out:  tensor([2.9062])\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    in_test = (torch.rand(1) - 0.5)*100.0\n",
    "    e_bits = int(torch.round(torch.rand(1)*10).item())\n",
    "    m_bits = int(torch.round(torch.rand(1)*30).item())\n",
    "    scale = int(torch.round((torch.rand(1) - 0.5)*5.0).item())\n",
    "    out_test = fake_float_truncate(in_test, e_bits, m_bits, scale)\n",
    "    print(\"e_bits \", e_bits, \" m_bits \", m_bits, \" scale \", scale)\n",
    "    print(\"in: \", in_test, \" out: \", out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_fixed_truncate2(x: torch.Tensor, bits_int: int, scale_int: int, zero_point_int: int) -> torch.Tensor:\n",
    "    \n",
    "    qmin = 0\n",
    "    qmax = 2**bits_int - 1\n",
    "    \n",
    "    #from float to fixed point, and quantize accordingly\n",
    "    q_x = torch.clamp(torch.round(x * 2**(scale_int + bits_int//2) + 2**(bits_int-1) + zero_point_int), qmin, qmax)\n",
    "\n",
    "    # from quantized fixed point to float\n",
    "    fq_x = (q_x - 2**(bits_int-1) - zero_point_int) / 2**(scale_int + bits_int//2)\n",
    "        \n",
    "    return fq_x\n",
    "\n",
    "def fake_fixed_truncate(x: torch.Tensor, bits_int: int, scale_int: int, zero_point_int: int) -> torch.Tensor:\n",
    "    \n",
    "    qmin = 0\n",
    "    qmax = 2**bits_int - 1\n",
    "    \n",
    "    mantissa = x * 2**(scale_int + bits_int//2) + zero_point_int + 2**(bits_int-1)\n",
    "    \n",
    "    #from float to fixed point, and quantize accordingly\n",
    "    q_x = torch.clamp(torch.round(mantissa), qmin, qmax)\n",
    "\n",
    "    # from quantized fixed point to float\n",
    "    fq_x = (q_x - 2**(bits_int-1) - zero_point_int) / 2**(scale_int + bits_int//2)\n",
    "        \n",
    "    return fq_x\n",
    "\n",
    "class FakeFixedFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, bits_param, scale_param, zero_point_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, bits_param, scale_param, zero_point_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        bits_int = int(torch.round(param_to_bit(bits_param)).item())\n",
    "        scale_int = int(torch.round(scale_param).item())\n",
    "        zero_point_int = int(torch.round(zero_point_param).item())\n",
    "\n",
    "        out = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int)\n",
    "        \n",
    "        #print(\"input\")\n",
    "        #print(x)\n",
    "        #print(\"output\")\n",
    "        #print(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, bits_param, scale_param, zero_point_param = ctx.saved_tensors\n",
    "        \n",
    "        bits = param_to_bit(bits_param)\n",
    "        scale = scale_param\n",
    "        zero_point = zero_point_param\n",
    "                \n",
    "        bits_int = int(torch.round(bits).item())\n",
    "        scale_int = int(torch.round(scale).item())\n",
    "        zero_point_int = int(torch.round(zero_point).item())\n",
    "\n",
    "        #print(\"shape x: \", x.shape)\n",
    "        #print(\"shape grad_output: \", grad_output.shape)\n",
    "\n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output\n",
    "        \n",
    "        # 1) Gradient wrt x: approximate with central difference\n",
    "        \"\"\"\n",
    "        grad_x = None\n",
    "        if True:\n",
    "            delta = 0.01            \n",
    "\n",
    "            f_plus2  = fake_float_truncate(x + 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_plus   = fake_float_truncate(x + 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus  = fake_float_truncate(x - 1.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "            f_minus2 = fake_float_truncate(x - 2.0*delta, e_bits_int, m_bits_int, s_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "            grad_x = grad_output * der\n",
    "        \"\"\"\n",
    "                \n",
    "        # 2) Gradient wrt bits: approximate with central difference\n",
    "        grad_bits = None\n",
    "        if bits_param.requires_grad:\n",
    "            if(bits_int < 2):\n",
    "                f_plus   = fake_fixed_truncate(x, bits_int + 1, scale_int, zero_point_int)\n",
    "                f_minus  = fake_fixed_truncate(x, bits_int    , scale_int, zero_point_int)\n",
    "                der = (f_plus - f_minus)\n",
    "            else:\n",
    "                f_plus2  = fake_fixed_truncate(x, bits_int + 2, scale_int, zero_point_int)\n",
    "                f_plus   = fake_fixed_truncate(x, bits_int + 1, scale_int, zero_point_int)\n",
    "                f_minus  = fake_fixed_truncate(x, bits_int - 1, scale_int, zero_point_int)\n",
    "                f_minus2 = fake_fixed_truncate(x, bits_int - 2, scale_int, zero_point_int)\n",
    "            \n",
    "                der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_bits = grad_output * der * bits\n",
    "        \n",
    "        # 3) Gradient wrt scale: approximate with central difference\n",
    "        grad_scale_bits = None\n",
    "        if scale_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_fixed_truncate(x, bits_int, scale_int + 2, zero_point_int)\n",
    "            f_plus   = fake_fixed_truncate(x, bits_int, scale_int + 1, zero_point_int)\n",
    "            f_minus  = fake_fixed_truncate(x, bits_int, scale_int - 1, zero_point_int)\n",
    "            f_minus2 = fake_fixed_truncate(x, bits_int, scale_int - 2, zero_point_int)\n",
    "        \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0\n",
    "            \n",
    "            grad_scale_bits = grad_output * der\n",
    "       \n",
    "        # 4) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_zero_point_bits = None\n",
    "        if zero_point_param.requires_grad:\n",
    "            \n",
    "            f_plus2  = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int + 2)\n",
    "            f_plus   = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int + 1)\n",
    "            f_minus  = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int - 1)\n",
    "            f_minus2 = fake_fixed_truncate(x, bits_int, scale_int, zero_point_int - 2)\n",
    "            \n",
    "            der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / 12.0 \n",
    "            grad_zero_point_bits = grad_output * der\n",
    "             \n",
    "        return grad_x, grad_bits, grad_scale_bits, grad_zero_point_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits  30  scale  2  zero_point  0\n",
      "in:  tensor([5.4418])  out  tensor([5.4419])\n",
      "bits  16  scale  0  zero_point  0\n",
      "in:  tensor([43.5466])  out  tensor([43.5469])\n",
      "bits  21  scale  -2  zero_point  0\n",
      "in:  tensor([5.7193])  out  tensor([5.7188])\n",
      "bits  17  scale  -1  zero_point  0\n",
      "in:  tensor([37.7809])  out  tensor([37.7812])\n",
      "bits  32  scale  1  zero_point  0\n",
      "in:  tensor([26.7881])  out  tensor([26.7891])\n",
      "bits  27  scale  1  zero_point  0\n",
      "in:  tensor([-7.6734])  out  tensor([-7.6733])\n",
      "bits  18  scale  -1  zero_point  0\n",
      "in:  tensor([23.2878])  out  tensor([23.2891])\n",
      "bits  10  scale  -1  zero_point  0\n",
      "in:  tensor([-21.5058])  out  tensor([-21.5000])\n",
      "bits  21  scale  0  zero_point  0\n",
      "in:  tensor([-31.9050])  out  tensor([-31.9053])\n",
      "bits  27  scale  -1  zero_point  0\n",
      "in:  tensor([-41.3346])  out  tensor([-41.3350])\n",
      "bits  19  scale  1  zero_point  0\n",
      "in:  tensor([-47.6712])  out  tensor([-47.6709])\n",
      "bits  27  scale  2  zero_point  0\n",
      "in:  tensor([-3.1075])  out  tensor([-3.1075])\n",
      "bits  18  scale  2  zero_point  0\n",
      "in:  tensor([-42.3760])  out  tensor([-42.3760])\n",
      "bits  6  scale  2  zero_point  0\n",
      "in:  tensor([21.9605])  out  tensor([0.9688])\n",
      "bits  28  scale  -2  zero_point  0\n",
      "in:  tensor([46.7174])  out  tensor([46.7188])\n",
      "bits  24  scale  2  zero_point  0\n",
      "in:  tensor([-5.6868])  out  tensor([-5.6868])\n",
      "bits  28  scale  -2  zero_point  0\n",
      "in:  tensor([7.6194])  out  tensor([7.6211])\n",
      "bits  15  scale  2  zero_point  0\n",
      "in:  tensor([31.4663])  out  tensor([31.4668])\n",
      "bits  20  scale  -2  zero_point  0\n",
      "in:  tensor([-17.3943])  out  tensor([-17.3945])\n",
      "bits  31  scale  1  zero_point  0\n",
      "in:  tensor([7.4884])  out  tensor([7.4883])\n",
      "bits  31  scale  -2  zero_point  0\n",
      "in:  tensor([39.8466])  out  tensor([39.8438])\n",
      "bits  25  scale  -1  zero_point  0\n",
      "in:  tensor([12.9669])  out  tensor([12.9668])\n",
      "bits  31  scale  2  zero_point  0\n",
      "in:  tensor([20.6937])  out  tensor([20.6934])\n",
      "bits  4  scale  -2  zero_point  0\n",
      "in:  tensor([38.0527])  out  tensor([7.])\n",
      "bits  8  scale  1  zero_point  0\n",
      "in:  tensor([-6.7832])  out  tensor([-4.])\n",
      "bits  26  scale  1  zero_point  0\n",
      "in:  tensor([37.4910])  out  tensor([37.4910])\n",
      "bits  15  scale  1  zero_point  0\n",
      "in:  tensor([-16.3896])  out  tensor([-16.3906])\n",
      "bits  13  scale  -1  zero_point  0\n",
      "in:  tensor([13.5280])  out  tensor([13.5312])\n",
      "bits  18  scale  -1  zero_point  0\n",
      "in:  tensor([22.0257])  out  tensor([22.0273])\n",
      "bits  9  scale  -2  zero_point  0\n",
      "in:  tensor([18.8763])  out  tensor([19.])\n",
      "bits  22  scale  0  zero_point  0\n",
      "in:  tensor([21.8592])  out  tensor([21.8594])\n",
      "bits  13  scale  -1  zero_point  0\n",
      "in:  tensor([7.1324])  out  tensor([7.1250])\n",
      "bits  26  scale  -2  zero_point  0\n",
      "in:  tensor([41.9852])  out  tensor([41.9844])\n",
      "bits  11  scale  0  zero_point  0\n",
      "in:  tensor([-24.3994])  out  tensor([-24.4062])\n",
      "bits  31  scale  0  zero_point  0\n",
      "in:  tensor([-4.9952])  out  tensor([-4.9961])\n",
      "bits  23  scale  1  zero_point  0\n",
      "in:  tensor([-48.5058])  out  tensor([-48.5059])\n",
      "bits  23  scale  -1  zero_point  0\n",
      "in:  tensor([32.2514])  out  tensor([32.2520])\n",
      "bits  4  scale  1  zero_point  0\n",
      "in:  tensor([21.5793])  out  tensor([0.8750])\n",
      "bits  3  scale  2  zero_point  0\n",
      "in:  tensor([48.3722])  out  tensor([0.3750])\n",
      "bits  1  scale  0  zero_point  0\n",
      "in:  tensor([2.8445])  out  tensor([0.])\n",
      "bits  8  scale  -1  zero_point  0\n",
      "in:  tensor([24.5846])  out  tensor([15.8750])\n",
      "bits  8  scale  1  zero_point  0\n",
      "in:  tensor([-14.6249])  out  tensor([-4.])\n",
      "bits  10  scale  0  zero_point  0\n",
      "in:  tensor([39.0843])  out  tensor([15.9688])\n",
      "bits  23  scale  0  zero_point  0\n",
      "in:  tensor([36.4200])  out  tensor([36.4199])\n",
      "bits  2  scale  2  zero_point  0\n",
      "in:  tensor([-31.4371])  out  tensor([-0.2500])\n",
      "bits  16  scale  -1  zero_point  0\n",
      "in:  tensor([-38.3623])  out  tensor([-38.3594])\n",
      "bits  24  scale  -2  zero_point  0\n",
      "in:  tensor([-43.7184])  out  tensor([-43.7188])\n",
      "bits  12  scale  0  zero_point  0\n",
      "in:  tensor([-18.1009])  out  tensor([-18.0938])\n",
      "bits  0  scale  1  zero_point  0\n",
      "in:  tensor([-26.2737])  out  tensor([-0.2500])\n",
      "bits  11  scale  0  zero_point  0\n",
      "in:  tensor([15.8978])  out  tensor([15.9062])\n",
      "bits  3  scale  -1  zero_point  0\n",
      "in:  tensor([13.7956])  out  tensor([3.])\n",
      "bits  13  scale  -1  zero_point  0\n",
      "in:  tensor([23.5631])  out  tensor([23.5625])\n",
      "bits  15  scale  0  zero_point  0\n",
      "in:  tensor([-34.3894])  out  tensor([-34.3906])\n",
      "bits  20  scale  2  zero_point  0\n",
      "in:  tensor([32.7682])  out  tensor([32.7683])\n",
      "bits  1  scale  0  zero_point  0\n",
      "in:  tensor([-34.9190])  out  tensor([-1.])\n",
      "bits  9  scale  2  zero_point  0\n",
      "in:  tensor([-4.9597])  out  tensor([-4.])\n",
      "bits  17  scale  2  zero_point  0\n",
      "in:  tensor([31.8014])  out  tensor([31.8018])\n",
      "bits  16  scale  1  zero_point  0\n",
      "in:  tensor([-29.0452])  out  tensor([-29.0449])\n",
      "bits  4  scale  1  zero_point  0\n",
      "in:  tensor([28.8432])  out  tensor([0.8750])\n",
      "bits  26  scale  2  zero_point  0\n",
      "in:  tensor([-29.3534])  out  tensor([-29.3534])\n",
      "bits  28  scale  -1  zero_point  0\n",
      "in:  tensor([-31.7846])  out  tensor([-31.7842])\n",
      "bits  7  scale  -1  zero_point  0\n",
      "in:  tensor([15.9026])  out  tensor([15.7500])\n",
      "bits  19  scale  -1  zero_point  0\n",
      "in:  tensor([-14.1750])  out  tensor([-14.1758])\n",
      "bits  31  scale  0  zero_point  0\n",
      "in:  tensor([7.9288])  out  tensor([7.9297])\n",
      "bits  30  scale  0  zero_point  0\n",
      "in:  tensor([34.4000])  out  tensor([34.4004])\n",
      "bits  16  scale  1  zero_point  0\n",
      "in:  tensor([14.5148])  out  tensor([14.5156])\n",
      "bits  16  scale  0  zero_point  0\n",
      "in:  tensor([-13.1878])  out  tensor([-13.1875])\n",
      "bits  16  scale  0  zero_point  0\n",
      "in:  tensor([36.6027])  out  tensor([36.6016])\n",
      "bits  31  scale  2  zero_point  0\n",
      "in:  tensor([29.0300])  out  tensor([29.0303])\n",
      "bits  16  scale  -1  zero_point  0\n",
      "in:  tensor([-33.8641])  out  tensor([-33.8672])\n",
      "bits  19  scale  2  zero_point  0\n",
      "in:  tensor([-30.0686])  out  tensor([-30.0684])\n",
      "bits  12  scale  0  zero_point  0\n",
      "in:  tensor([-7.5259])  out  tensor([-7.5312])\n",
      "bits  29  scale  -1  zero_point  0\n",
      "in:  tensor([-13.5111])  out  tensor([-13.5117])\n",
      "bits  30  scale  -1  zero_point  0\n",
      "in:  tensor([33.7209])  out  tensor([33.7227])\n",
      "bits  29  scale  0  zero_point  0\n",
      "in:  tensor([-29.5445])  out  tensor([-29.5449])\n",
      "bits  15  scale  0  zero_point  0\n",
      "in:  tensor([-47.4217])  out  tensor([-47.4219])\n",
      "bits  8  scale  2  zero_point  0\n",
      "in:  tensor([-32.8967])  out  tensor([-2.])\n",
      "bits  29  scale  1  zero_point  0\n",
      "in:  tensor([-37.5805])  out  tensor([-37.5806])\n",
      "bits  30  scale  0  zero_point  0\n",
      "in:  tensor([47.2641])  out  tensor([47.2637])\n",
      "bits  1  scale  1  zero_point  0\n",
      "in:  tensor([23.3972])  out  tensor([0.])\n",
      "bits  6  scale  2  zero_point  0\n",
      "in:  tensor([-1.4753])  out  tensor([-1.])\n",
      "bits  7  scale  -2  zero_point  0\n",
      "in:  tensor([-42.5110])  out  tensor([-32.])\n",
      "bits  31  scale  2  zero_point  0\n",
      "in:  tensor([-31.8789])  out  tensor([-31.8789])\n",
      "bits  1  scale  1  zero_point  0\n",
      "in:  tensor([-44.8475])  out  tensor([-0.5000])\n",
      "bits  3  scale  -2  zero_point  0\n",
      "in:  tensor([-17.4713])  out  tensor([-8.])\n",
      "bits  10  scale  1  zero_point  0\n",
      "in:  tensor([-25.2370])  out  tensor([-8.])\n",
      "bits  24  scale  -1  zero_point  0\n",
      "in:  tensor([-28.7038])  out  tensor([-28.7041])\n",
      "bits  4  scale  -1  zero_point  0\n",
      "in:  tensor([39.9861])  out  tensor([3.5000])\n",
      "bits  25  scale  -1  zero_point  0\n",
      "in:  tensor([-17.0055])  out  tensor([-17.0054])\n",
      "bits  15  scale  -1  zero_point  0\n",
      "in:  tensor([38.3508])  out  tensor([38.3438])\n",
      "bits  5  scale  -2  zero_point  0\n",
      "in:  tensor([30.0869])  out  tensor([15.])\n",
      "bits  31  scale  -1  zero_point  0\n",
      "in:  tensor([-43.6332])  out  tensor([-43.6328])\n",
      "bits  4  scale  -2  zero_point  0\n",
      "in:  tensor([-13.9306])  out  tensor([-8.])\n",
      "bits  29  scale  -2  zero_point  0\n",
      "in:  tensor([-20.7603])  out  tensor([-20.7617])\n",
      "bits  12  scale  2  zero_point  0\n",
      "in:  tensor([34.6421])  out  tensor([7.9961])\n",
      "bits  3  scale  2  zero_point  0\n",
      "in:  tensor([-30.1295])  out  tensor([-0.5000])\n",
      "bits  23  scale  0  zero_point  0\n",
      "in:  tensor([49.4635])  out  tensor([49.4634])\n",
      "bits  27  scale  0  zero_point  0\n",
      "in:  tensor([-25.0797])  out  tensor([-25.0796])\n",
      "bits  13  scale  2  zero_point  0\n",
      "in:  tensor([32.9617])  out  tensor([15.9961])\n",
      "bits  26  scale  0  zero_point  0\n",
      "in:  tensor([30.4756])  out  tensor([30.4756])\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    bits = int(torch.round(torch.rand(1)*32).item())\n",
    "    scale = int(torch.round((torch.rand(1) - 0.5)*5.0).item())\n",
    "    zero_point = int(torch.round((torch.rand(1) - 0.5)*0.0).item())\n",
    "    in_test = (torch.rand(1)-0.5)*100.0\n",
    "    out_test = fake_fixed_truncate(in_test, bits, scale, zero_point)\n",
    "    print(\"bits \", bits, \" scale \", scale, \" zero_point \", zero_point)\n",
    "    print(\"in: \", in_test, \" out \", out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### differentiable Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoundSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        return grad_output\n",
    "    \n",
    "class RoundFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.round(input + 2*delta)\n",
    "        f_plus   = torch.round(input + 1*delta)\n",
    "        f_minus  = torch.round(input - 1*delta)\n",
    "        f_minus2 = torch.round(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "class RoundSIG(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function that does a hard round in forward,\n",
    "    but uses a sigmoid-based approximation for the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, alpha=10.0):\n",
    "        \"\"\"\n",
    "        Forward pass: returns torch.round(input).\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.alpha = alpha\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: approximate the gradient of round(x)\n",
    "        with the derivative of a sigmoid centered at the fractional midpoint (0.5).\n",
    "        \"\"\"\n",
    "        (input,) = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "\n",
    "        # Fractional part\n",
    "        frac = input - torch.floor(input)\n",
    "\n",
    "        # Sigmoid of (fractional_part - 0.5), scaled by alpha\n",
    "        s = torch.sigmoid(alpha * (frac - 0.5))\n",
    "\n",
    "        # Derivative of sigmoid = alpha * s * (1 - s)\n",
    "        grad_input = alpha * s * (1 - s) * grad_output\n",
    "        return grad_input, None  # alpha is not a tensor that requires grad\n",
    "    \n",
    "def diff_round(x):\n",
    "    return RoundSTE.apply(x)\n",
    "    #return RoundFDE.apply(x)\n",
    "    #return RoundSIG.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable Floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloorSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass uses standard floor\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-through pass: just return the gradient as-is\n",
    "        return grad_output\n",
    "\n",
    "class FloorFDE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Forward pass: use the usual rounding\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.floor(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: pass the gradient unchanged (STE)\n",
    "        (input, ) = ctx.saved_tensors\n",
    "        delta = 1.0\n",
    "        f_plus2  = torch.floor(input + 2*delta)\n",
    "        f_plus   = torch.floor(input + 1*delta)\n",
    "        f_minus  = torch.floor(input - 1*delta)\n",
    "        f_minus2 = torch.floor(input - 2*delta)\n",
    "        # der = (f_plus - f_minus)/2.0\n",
    "        der = (-f_plus2 + 8*f_plus - 8*f_minus + f_minus2) / (12.0 * delta)\n",
    "        \n",
    "        return der * grad_output\n",
    "\n",
    "def diff_floor(input):\n",
    "    return FloorSTE.apply(input)\n",
    "    #return FloorFDE.apply(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxObserver(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We store running min/max\n",
    "        self.register_buffer(\"min_val\", torch.tensor(float(\"inf\")))\n",
    "        self.register_buffer(\"max_val\", torch.tensor(float(\"-inf\")))\n",
    "        # You could also store averaging stats, etc.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update running min/max\n",
    "        self.min_val = torch.min(self.min_val, x.detach().min())\n",
    "        self.max_val = torch.max(self.max_val, x.detach().max())\n",
    "        return x  # Just pass through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed point quanizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, observer, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.observer = observer\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        \n",
    "        # 1) Get min/max from observer\n",
    "        min_val = self.observer.min_val\n",
    "        max_val = self.observer.max_val\n",
    "\n",
    "        # If they're not valid, skip\n",
    "        #if min_val >= max_val:\n",
    "        #    return x\n",
    "\n",
    "        # 2) Compute scale and zero_point\n",
    "        # For an unsigned 4-bit range, we can hold values 0..15\n",
    "        # qmin, qmax = 0, (1 << b_int) - 1  # e.g. 0..15\n",
    "        qmin, qmax = torch.tensor(float(0)), 2**b_int - 1  # e.g. 0..15\n",
    "        \n",
    "        qmin = qmin.to(x.device)\n",
    "        #qmax = qmax.to(x.device)\n",
    "        max_val = max_val.to(x.device)\n",
    "        min_val = min_val.to(x.device)\n",
    "\n",
    "        # Typical formula for scale/zero-point:\n",
    "        scale = (max_val - min_val) / float(qmax - qmin)\n",
    "        zero_point = qmin - diff_round(min_val / scale)\n",
    "\n",
    "        # 3) Quantize (in floating point)\n",
    "        # clamp to range of [qmin, qmax]\n",
    "        q_x = torch.clamp(diff_round(x / scale + zero_point), qmin, qmax)\n",
    "\n",
    "        # 4) Dequantize back to float\n",
    "        fq_x = (q_x - zero_point) * scale\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"bits: \", self.bits.detach().item())\n",
    "        \n",
    "class FixedPointFakeQuantize2(nn.Module):\n",
    "    def __init__(self, bits=32, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.bits = nn.Parameter(torch.tensor(float(bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(bits//2)), requires_grad=requires_grad)\n",
    "        self.zero_point = nn.Parameter(torch.tensor(float(2**(bits//2-1)-1)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        bits_int = torch.clamp(diff_round(self.bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        zero_point_int = diff_round(self.zero_point)\n",
    "        \n",
    "        qmin = torch.tensor(float(0)).to(x.device)\n",
    "        qmax = 2**bits_int - 1  # e.g. 0..15\n",
    "        \n",
    "        #from float to fixed point, and quantize accordingly\n",
    "        q_x = torch.clamp(diff_round(x * 2**scale_int + zero_point_int), qmin, qmax)\n",
    "\n",
    "        # from quantized fixed point to float\n",
    "        fq_x = (q_x - zero_point_int) / 2**scale_int\n",
    "        \n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"bits: \", self.bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())\n",
    "        print(\"zero point: \", self.zero_point.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatingPointFakeQuantize(nn.Module):\n",
    "    def __init__(self, m_bits=23, e_bits=8, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.e_bits = nn.Parameter(torch.tensor(float(e_bits)), requires_grad=requires_grad)\n",
    "        self.m_bits = nn.Parameter(torch.tensor(float(m_bits)), requires_grad=requires_grad)\n",
    "        self.scale = nn.Parameter(torch.tensor(float(0)), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e_bits_int = torch.clamp(diff_round(self.e_bits), 0, 32)\n",
    "        m_bits_int = torch.clamp(diff_round(self.m_bits), 1, 32)\n",
    "        scale_int = diff_round(self.scale)\n",
    "        \n",
    "        sign = x.sign()\n",
    "        abs_x = x.abs().clamp(min=1e-45)\n",
    "\n",
    "        #recover the floatint point representation\n",
    "        #exponent \\in {-2**7,..,2**7-1}\n",
    "        #mantissa \\in {1.0,...,2.0}\n",
    "\n",
    "        exponent = diff_floor(torch.log2(abs_x)).clamp(min=1e-45)\n",
    "        mantissa = abs_x / (2**exponent)\n",
    "    \n",
    "        # truncate exponent\n",
    "        # lets parameterize the exponent as a constant value + a variable value\n",
    "        # the constant part is 2**7-1 in standar floating point, but we will learn it\n",
    "        # the variable part \\in {0,..,2**8-1}\n",
    "        # lets say exponent = v_exponent - 2**(bits-1)-1 + c_exponent\n",
    "        # so v_exponent = exponent + 2**(bits-1)-1 - c_exponent\n",
    "        c_exponent = scale_int\n",
    "        v_exponent = exponent + (2**(e_bits_int-1)-1) - c_exponent\n",
    "        \n",
    "        # the valriable part is clamped to the alloted bits\n",
    "        q_min = torch.tensor(float(0)).to(x.device)\n",
    "        q_max = 2**e_bits_int-1\n",
    "        q_exponent = torch.clamp(v_exponent, q_min, q_max) - (2**(e_bits_int-1)-1) + c_exponent\n",
    "    \n",
    "        # truncate mantissa\n",
    "        # this just removes the less significant bits\n",
    "        m_scale = 2.0 ** m_bits_int\n",
    "        q_mantissa = diff_floor(mantissa * m_scale) / m_scale\n",
    "    \n",
    "        # from quantized floatint point to float\n",
    "        fq_x = sign * (2**q_exponent) * q_mantissa\n",
    "        return fq_x\n",
    "\n",
    "    def getBits(self):\n",
    "        return [self.e_bits, self.m_bits]\n",
    "\n",
    "    def printParams(self):\n",
    "        print(\"e_bits: \", self.e_bits.detach().item())\n",
    "        print(\"m_bits: \", self.m_bits.detach().item())\n",
    "        print(\"scale: \", self.scale.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float32 example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * input_size[1]//8 * input_size[2]//8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantWrapper(nn.Module):\n",
    "    def __init__(self, module, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.observer = MinMaxObserver()\n",
    "        self.fake_quant_input = FixedPointFakeQuantize(self.observer, requires_grad=optimizeQuant)\n",
    "        self.fake_quant_weight = FixedPointFakeQuantize(self.observer, requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_input = FixedPointFakeQuantize2(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_weight = FixedPointFakeQuantize2(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_input = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "        #self.fake_quant_weight = FloatingPointFakeQuantize(requires_grad=optimizeQuant)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.observer(x)\n",
    "        x = self.fake_quant_input(x)\n",
    "        w = self.fake_quant_weight(self.module.weight)\n",
    "        b = self.module.bias\n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            return F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            return F.linear(x, w, b)\n",
    "        else:\n",
    "            return self.module(x)\n",
    "        \n",
    "    def getBits(self):\n",
    "        return self.fake_quant_input.getBits() + self.fake_quant_weight.getBits()\n",
    "        #return self.fake_quant_weight.getBits()\n",
    "    \n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        self.fake_quant_input.printParams()\n",
    "        print(\"weight quant params: \")\n",
    "        self.fake_quant_weight.printParams()\n",
    "\n",
    "class QuantWrapperFloatingPoint(nn.Module):\n",
    "    def __init__(self, module, e_bits=5, m_bits=10, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "        self.input_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        self.input_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        self.input_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "        \n",
    "        self.weight_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "        \n",
    "        #self.bias_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=False)\n",
    "        #self.bias_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=False)\n",
    "        #self.bias_scale = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        \n",
    "        #self.output_e_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(e_bits))), requires_grad=optimizeQuant)\n",
    "        #self.output_m_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(m_bits))), requires_grad=optimizeQuant)\n",
    "        #self.output_scale = nn.Parameter(torch.tensor(0.0), requires_grad=optimizeQuant)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = FakeFloatFunction.apply(x,   self.input_e_bits_param, self.input_m_bits_param, self.input_scale)\n",
    "        \n",
    "        if hasattr(self.module, 'weight') and self.module.weight != None:\n",
    "            w = FakeFloatFunction.apply(self.module.weight, self.weight_e_bits_param, self.weight_m_bits_param, self.weight_scale)\n",
    "        else:\n",
    "            w = None\n",
    "        \n",
    "        if hasattr(self.module, 'bias') and self.module.bias != None:\n",
    "            b = self.module.bias #FakeFloatFunction.apply(self.module.bias, self.bias_e_bits_param, self.bias_m_bits_param, self.bias_scale)\n",
    "        else:\n",
    "            b = None\n",
    "        \n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            out = F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            out = F.linear(x, w, b)\n",
    "        #else:\n",
    "        #    out = self.module(x)\n",
    "        \n",
    "        #out = FakeFloatFunction.apply(out, self.output_e_bits_param, self.output_m_bits_param, self.output_scale)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def getBits(self):\n",
    "        return [param_to_bit(self.input_e_bits_param) + param_to_bit(self.input_m_bits_param) + 1, param_to_bit(self.weight_e_bits_param) + param_to_bit(self.weight_m_bits_param) + 1]\n",
    "        #return [param_to_bit(self.weight_e_bits_param) + param_to_bit(self.weight_m_bits_param) + 1, param_to_bit(self.bias_e_bits_param) + param_to_bit(self.bias_m_bits_param) + 1, param_to_bit(self.output_e_bits_param) + param_to_bit(self.output_m_bits_param) + 1]\n",
    "        #return [param_to_bit(self.weight_e_bits_param) + param_to_bit(self.weight_m_bits_param) + 1, param_to_bit(self.output_e_bits_param) + param_to_bit(self.output_m_bits_param) + 1]\n",
    "\n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        print(\"e bits: \", param_to_bit(self.input_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.input_m_bits_param).detach().item(), \" scale \", self.input_scale.detach().item())\n",
    "        print(\"weight quant params: \")\n",
    "        print(\"e bits \", param_to_bit(self.weight_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.weight_m_bits_param).detach().item(), \" scale \", self.weight_scale.detach().item())\n",
    "        #print(\"bias quant params: \")\n",
    "        #print(\"e bits \", param_to_bit(self.bias_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.bias_m_bits_param).detach().item(), \" scale \", self.bias_scale.detach().item())\n",
    "        #print(\"output quant params: \")\n",
    "        #print(\"e bits: \", param_to_bit(self.output_e_bits_param).detach().item(), \" m bits \", param_to_bit(self.output_m_bits_param).detach().item(), \" scale \", self.output_scale.detach().item())\n",
    "        \n",
    "class QuantWrapperFixedPoint(nn.Module):\n",
    "    def __init__(self, module, bits=32, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "        self.input_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        self.input_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.input_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "    \n",
    "        self.weight_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        self.weight_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        self.weight_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        \n",
    "        self.bias_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=False)\n",
    "        self.bias_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=False)\n",
    "        self.bias_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=False)\n",
    "    \n",
    "        #self.output_bits_param = nn.Parameter(bit_to_param(torch.tensor(float(bits))), requires_grad=optimizeQuant)\n",
    "        #self.output_scale = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "        #self.output_zero_point = nn.Parameter(torch.tensor(float(0)), requires_grad=optimizeQuant)\n",
    "            \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = FakeFixedFunction.apply(x, self.input_bits_param, self.input_scale, self.input_zero_point)\n",
    "        \n",
    "        if hasattr(self.module, 'weight') and self.module.weight != None:\n",
    "            w = FakeFixedFunction.apply(self.module.weight, self.weight_bits_param, self.weight_scale, self.weight_zero_point)\n",
    "        else:\n",
    "            w = None\n",
    "\n",
    "        if hasattr(self.module, 'bias') and self.module.bias != None:\n",
    "            b = FakeFixedFunction.apply(self.module.bias, self.bias_bits_param, self.bias_scale, self.bias_zero_point)\n",
    "        else:\n",
    "            b = None\n",
    "        \n",
    "        if isinstance(self.module, nn.Conv2d):\n",
    "            out = F.conv2d(x, w, b, stride=self.module.stride, padding=self.module.padding, dilation=self.module.dilation, groups=self.module.groups)\n",
    "        elif isinstance(self.module, nn.Linear):\n",
    "            out = F.linear(x, w, b)\n",
    "        #else:\n",
    "        #    out = self.module(x)\n",
    "        \n",
    "        #out = FakeFixedFunction.apply(out, self.output_bits_param, self.output_scale, self.output_zero_point)    \n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def getBits(self):\n",
    "        return [param_to_bit(self.input_bits_param), param_to_bit(self.weight_bits_param)]\n",
    "        #return [param_to_bit(self.weight_bits_param), param_to_bit(self.bias_bits_param), param_to_bit(self.output_bits_param)]\n",
    "        #return [param_to_bit(self.weight_bits_param), param_to_bit(self.output_bits_param)]\n",
    "\n",
    "    def printQuantParams(self):\n",
    "        print(\"input quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.input_bits_param).detach().item(), \" scale \", self.input_scale.detach().item(), \" zero point \", self.input_zero_point.detach().item())\n",
    "        print(\"weight quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.weight_bits_param).detach().item(), \" scale \", self.weight_scale.detach().item(), \" zero point \", self.weight_zero_point.detach().item())\n",
    "        print(\"bias quant params: \")\n",
    "        print(\"bits: \", param_to_bit(self.bias_bits_param).detach().item(), \" scale \", self.bias_scale.detach().item(), \" zero point \", self.bias_zero_point.detach().item())\n",
    "        #print(\"output quant params: \")\n",
    "        #print(\"bits: \", param_to_bit(self.output_bits_param).detach().item(), \" scale \", self.output_scale.detach().item(), \" zero point \", self.output_zero_point.detach().item())\n",
    "        \n",
    "class QuantSimpleCIFAR10Model(nn.Module):\n",
    "    def __init__(self, QuantClass, num_classes=10, optimizeQuant=False):\n",
    "        super().__init__()\n",
    "        #self.input = QuantClass(nn.Identity(), optimizeQuant=optimizeQuant)\n",
    "        self.conv1 = QuantClass(nn.Conv2d(input_size[0], 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = QuantClass(nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = QuantClass(nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = QuantClass(nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5 = QuantClass(nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = QuantClass(nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False), optimizeQuant=optimizeQuant)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.fc7 = QuantClass(nn.Linear(256 * input_size[1]//4 * input_size[2]//4, 512), optimizeQuant=optimizeQuant)\n",
    "        self.bn7 = nn.BatchNorm1d(512)\n",
    "        self.fc8 = QuantClass(nn.Linear(512, 512), optimizeQuant=optimizeQuant)\n",
    "        self.bn8 = nn.BatchNorm1d(512)\n",
    "        self.fc9 = QuantClass(nn.Linear(512, num_classes), optimizeQuant=optimizeQuant)\n",
    "        #self.output = QuantClass(nn.Identity(), optimizeQuant=optimizeQuant)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = 2.0 * x - torch.tensor([1.0], device=x.device)\n",
    "        #x = self.input(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x)\n",
    "        x = self.fc9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class squared_hinge_loss(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, predictions, targets):\n",
    "        ctx.save_for_backward(predictions, targets)\n",
    "        output = 1. - predictions.mul(targets)\n",
    "        output[output.le(0.)] = 0.\n",
    "        loss = torch.mean(output.mul(output))\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        predictions, targets = ctx.saved_tensors\n",
    "        output = 1. - predictions.mul(targets)\n",
    "        output[output.le(0.)] = 0.\n",
    "        grad_output.resize_as_(predictions).copy_(targets).mul_(-2.).mul_(output)\n",
    "        grad_output.mul_(output.ne(0).float())\n",
    "        grad_output.div_(predictions.numel())\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "class SqrHingeLoss(nn.Module):\n",
    "    # Squared Hinge Loss\n",
    "    def __init__(self):\n",
    "        super(SqrHingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return squared_hinge_loss.apply(input, target)\n",
    "    \n",
    "def label_smoothing_loss(pred, target, smoothing=0.1):\n",
    "    confidence = 1.0 - smoothing\n",
    "    log_probs = F.log_softmax(pred, dim=-1)\n",
    "    nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "    nll_loss = nll_loss.squeeze(1)\n",
    "    smooth_loss = -log_probs.mean(dim=-1)\n",
    "    loss = confidence * nll_loss + smoothing * smooth_loss\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwidth_squared(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += bit ** 2\n",
    "                c += 1\n",
    "    if c == 0:\n",
    "        return 0\n",
    "    return s/c\n",
    "\n",
    "def bitwidth_sum(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += bit\n",
    "                c += 1\n",
    "    if c==0:\n",
    "        return 0\n",
    "    return s/c\n",
    "\n",
    "def bitwidth_round_sum(model):\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            for bit in module.getBits():\n",
    "                s += torch.round(bit)\n",
    "                c += 1\n",
    "    if c==0:\n",
    "        return 0\n",
    "    return s/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBitWidths(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantWrapperFixedPoint) or isinstance(module, QuantWrapperFloatingPoint):\n",
    "            print(\"module: \", name)\n",
    "            module.printQuantParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, bit_width_criterion, scheduler=None, lambda_bw=1e-1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        if isinstance(criterion, SqrHingeLoss):\n",
    "            target = target.unsqueeze(1)\n",
    "            target_onehot = torch.Tensor(target.size(0), len(classes)).to(device, non_blocking=True)\n",
    "            target_onehot.fill_(-1)\n",
    "            target_onehot.scatter_(1, target, 1)\n",
    "            target = target.squeeze()\n",
    "            target = target_onehot\n",
    "                    \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_ce = criterion(output, target)\n",
    "        penalty_bw = bit_width_criterion(model) \n",
    "        loss = loss_ce + lambda_bw*penalty_bw\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #if batch_idx % 200 == 0:\n",
    "        #    print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "        #          f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Train set: Average loss: {train_loss:.4f}\")\n",
    "\n",
    "def test(model, device, test_loader, criterion, bit_width_criterion, lambda_bw=1e-1):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    penalty_bw = bit_width_criterion(model) \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            if isinstance(criterion, SqrHingeLoss):\n",
    "                target = target.unsqueeze(1)\n",
    "                target_onehot = torch.Tensor(target.size(0), len(classes)).to(device, non_blocking=True)\n",
    "                target_onehot.fill_(-1)\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                target = target.squeeze()\n",
    "                target = target_onehot\n",
    "\n",
    "            loss_ce = criterion(output, target)\n",
    "            loss = loss_ce + lambda_bw*penalty_bw\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy} ({100.0*accuracy:.2f}%) bit penalty {penalty_bw}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using divice  cuda\n",
      "epoch:  1\n",
      "Train set: Average loss: 1.3475\n",
      "Test set: Average loss: 1.0135, Accuracy: 0.985 (98.50%) bit penalty 9.694915771484375\n",
      "epoch:  2\n",
      "Train set: Average loss: 0.8302\n",
      "Test set: Average loss: 0.7192, Accuracy: 0.983 (98.30%) bit penalty 6.703521251678467\n",
      "epoch:  3\n",
      "Train set: Average loss: 0.6166\n",
      "Test set: Average loss: 1.7887, Accuracy: 0.752 (75.20%) bit penalty 5.147805690765381\n",
      "epoch:  4\n",
      "Train set: Average loss: 0.4936\n",
      "Test set: Average loss: 0.4986, Accuracy: 0.9799 (97.99%) bit penalty 4.213968276977539\n",
      "epoch:  5\n",
      "Train set: Average loss: 0.4091\n",
      "Test set: Average loss: 0.3781, Accuracy: 0.993 (99.30%) bit penalty 3.555903673171997\n",
      "epoch:  6\n",
      "Train set: Average loss: 0.3511\n",
      "Test set: Average loss: 0.6835, Accuracy: 0.9118 (91.18%) bit penalty 3.152806282043457\n",
      "epoch:  7\n",
      "Train set: Average loss: 0.3245\n",
      "Test set: Average loss: 0.3627, Accuracy: 0.9804 (98.04%) bit penalty 2.8929126262664795\n",
      "epoch:  8\n",
      "Train set: Average loss: 0.3084\n",
      "Test set: Average loss: 0.3986, Accuracy: 0.9651 (96.51%) bit penalty 2.726633310317993\n",
      "epoch:  9\n",
      "Train set: Average loss: 0.2925\n",
      "Test set: Average loss: 0.3142, Accuracy: 0.9851 (98.51%) bit penalty 2.608691930770874\n",
      "epoch:  10\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"using divice \", device)\n",
    "\n",
    "QuantClass = QuantWrapperFloatingPoint\n",
    "#QuantClass = QuantWrapperFixedPoint\n",
    "\n",
    "base_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_base_model.pth\"\n",
    "best_accuracy_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_best_accuracy_model.pth\"\n",
    "less_bits_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_less_bits_model.pth\"\n",
    "\n",
    "load_model_path = None\n",
    "if(os.path.isfile(best_accuracy_model_path)):\n",
    "    load_model_path = best_accuracy_model_path\n",
    "elif(os.path.isfile(base_model_path)):\n",
    "    load_model_path = base_model_path   \n",
    "else:\n",
    "    best_accuracy_model_path = base_model_path\n",
    "    less_bits_model_path = base_model_path\n",
    "\n",
    "if(load_model_path):\n",
    "    # Create model\n",
    "    # model = SimpleQuantizedMLP(e_bits=4.0, m_bits=4.0, num_classes=len(classes)).to(device)\n",
    "    model = QuantSimpleCIFAR10Model(QuantClass, num_classes=len(classes), optimizeQuant=True).to(device)\n",
    "    #model = SimpleCIFAR10Model(num_classes=len(classes)).to(device)\n",
    "    model.load_state_dict(torch.load(load_model_path, weights_only=True))\n",
    "else:\n",
    "    model = QuantSimpleCIFAR10Model(QuantClass, num_classes=len(classes), optimizeQuant=False).to(device)\n",
    "\n",
    "#criterion = SqrHingeLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = label_smoothing_loss\n",
    "\n",
    "bit_width_criterion = bitwidth_sum\n",
    "\n",
    "# Create optimizer (SGD or Adam)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)  # Adjusted Cosine Annealing with warm-up strategy\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_penalty_bw = 100000.0\n",
    "# Train for some epochs\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(\"epoch: \", epoch)\n",
    "    train(model, device, train_loader, optimizer, criterion, bit_width_criterion, epoch)\n",
    "    if scheduler != None:\n",
    "        scheduler.step()\n",
    "    accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "    penalty_bw = bitwidth_sum(model)\n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), best_accuracy_model_path)\n",
    "    if(penalty_bw < best_penalty_bw):\n",
    "        best_penalty_bw = penalty_bw\n",
    "        torch.save(model.state_dict(), less_bits_model_path)\n",
    "    #printBitWidths(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(base_model_path, weights_only=True))\n",
    "base_accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "base_penalty_bw = bitwidth_round_sum(model).detach().item()\n",
    "model.load_state_dict(torch.load(best_accuracy_model_path, weights_only=True))\n",
    "best_accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "best_accuracy_penalty_bw = bitwidth_round_sum(model).detach().item()\n",
    "model.load_state_dict(torch.load(less_bits_model_path, weights_only=True))\n",
    "lest_bits_accuracy = test(model, device, test_loader, criterion, bit_width_criterion)\n",
    "less_bits_penalty_bw = bitwidth_round_sum(model).detach().item()\n",
    "print(\"base model accuracy: \", base_accuracy, \" penalty bw \", base_penalty_bw)\n",
    "print(\"best accuracy model accuracy: \", best_accuracy, \" penalty bw \", best_accuracy_penalty_bw)\n",
    "print(\"less bits model accuracy: \", lest_bits_accuracy, \" penalty bw \", less_bits_penalty_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_bits_model_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}_best_accuracy_model.pth\"\n",
    "pickle_path = f\"train_weights_and_quant_{QuantClass.__name__}_{dataset}.pkl\"\n",
    "state_dict = torch.load(best_accuracy_model_path, weights_only=True)\n",
    "state_dict_numpy = {}\n",
    "\n",
    "for key in state_dict:\n",
    "    #print(f\"{key}: {type(state_dict[key])}\")\n",
    "    state_dict_numpy[key] = state_dict[key].cpu().detach().numpy().tolist()\n",
    "    #print(key)\n",
    "    #print(state_dict_numpy[key])\n",
    "\n",
    "with open(\"mnist_cnn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(state_dict_numpy, f)\n",
    "\n",
    "printBitWidths(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
