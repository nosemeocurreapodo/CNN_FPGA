{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 1) MNIST Dataset & Dataloaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=1000, shuffle=False)\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'sis', 'seven', 'eight', 'nine')\n",
    "\n",
    "input_size = (28, 28)\n",
    "\n",
    "#train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "#test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "#test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64,shuffle=False, num_workers=2)\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#input_size = (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_bit(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(x)\n",
    "\n",
    "def bit_to_param(x : torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(x)\n",
    "\n",
    "def fake_float_truncate(x: torch.Tensor, e_bits: int, m_bits: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate 'float' with e_bits exponent bits and m_bits mantissa bits.\n",
    "    Simplified approach: unbiased exponent in integer range + truncated mantissa.\n",
    "    \"\"\"\n",
    "    eps = 1e-45\n",
    "    abs_x = x.abs().clamp(min=eps)\n",
    "    sign = x.sign()\n",
    "    \n",
    "    # exponent\n",
    "    e = torch.floor(torch.log2(abs_x))\n",
    "    min_e = -(2**(e_bits-1)) + 1\n",
    "    max_e =  (2**(e_bits-1)) - 1\n",
    "    e_clamped = torch.clamp(e, min_e, max_e)\n",
    "    \n",
    "    # fraction in [1,2) if x >= eps\n",
    "    frac = abs_x / (2.0 ** e_clamped)\n",
    "    \n",
    "    # truncate mantissa\n",
    "    scale = 2.0 ** m_bits\n",
    "    frac_trunc = torch.floor(frac * scale) / scale\n",
    "    \n",
    "    return sign * (2.0 ** e_clamped) * frac_trunc\n",
    "\n",
    "\n",
    "class FakeFloatFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd for 'fake-float' exponent+mantissa truncation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, e_bits_param, m_bits_param):\n",
    "        \n",
    "        # save for backward\n",
    "        ctx.save_for_backward(x, e_bits_param, m_bits_param)\n",
    "        \n",
    "        # Round e_bits, m_bits to nearest integer for the forward pass\n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).clamp(min=1.0).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).clamp(min=1.0).item())\n",
    "        \n",
    "        out = fake_float_truncate(x, e_bits_int, m_bits_int)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, e_bits_param, m_bits_param = ctx.saved_tensors\n",
    "        \n",
    "        # 1) Gradient wrt x: straight-through\n",
    "        grad_x = grad_output.clone()\n",
    "        \n",
    "        e_bits_int = int(torch.round(param_to_bit(e_bits_param)).clamp(min=1.0).item())\n",
    "        m_bits_int = int(torch.round(param_to_bit(m_bits_param)).clamp(min=1.0).item())\n",
    "        \n",
    "        # 2) Gradient wrt e_bits: approximate with central difference\n",
    "        grad_e_bits = None\n",
    "        if e_bits_param.requires_grad:\n",
    "            delta = 0.1\n",
    "            e_plus2_int  = int(torch.round(param_to_bit(e_bits_param + 2*delta)).clamp(min=1.0).item())\n",
    "            e_plus_int   = int(torch.round(param_to_bit(e_bits_param +   delta)).clamp(min=1.0).item())\n",
    "            e_minus_int  = int(torch.round(param_to_bit(e_bits_param -   delta)).clamp(min=1.0).item())\n",
    "            e_minus2_int = int(torch.round(param_to_bit(e_bits_param - 2*delta)).clamp(min=1.0).item())\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_plus2_int,  m_bits_int)\n",
    "            f_plus   = fake_float_truncate(x, e_plus_int,   m_bits_int)\n",
    "            f_minus  = fake_float_truncate(x, e_minus_int,  m_bits_int)\n",
    "            f_minus2 = fake_float_truncate(x, e_minus2_int, m_bits_int)\n",
    "            \n",
    "            #diff_e = (f_plus - f_minus) * grad_output\n",
    "            #grad_e_bits = diff_e.sum() / (2.0 * delta)\n",
    "            \n",
    "            diff_e = -f_plus2 + 8*f_plus - 8*f_minus + f_minus2\n",
    "            grad_e_bits = diff_e.sum() / (12.0 * delta)\n",
    "        \n",
    "        # 3) Gradient wrt m_bits: approximate with central difference\n",
    "        grad_m_bits = None\n",
    "        if m_bits_param.requires_grad:\n",
    "            delta = 0.1\n",
    "            m_plus2_int  = int(torch.round(param_to_bit(m_bits_param + 2*delta)).clamp(min=1.0).item())\n",
    "            m_plus_int   = int(torch.round(param_to_bit(m_bits_param +   delta)).clamp(min=1.0).item())\n",
    "            m_minus_int  = int(torch.round(param_to_bit(m_bits_param -   delta)).clamp(min=1.0).item())\n",
    "            m_minus2_int = int(torch.round(param_to_bit(m_bits_param - 2*delta)).clamp(min=1.0).item())\n",
    "            \n",
    "            f_plus2  = fake_float_truncate(x, e_bits_int, m_plus2_int)\n",
    "            f_plus   = fake_float_truncate(x, e_bits_int, m_plus_int)\n",
    "            f_minus  = fake_float_truncate(x, e_bits_int, m_minus_int)\n",
    "            f_minus2 = fake_float_truncate(x, e_bits_int, m_minus2_int)\n",
    "            \n",
    "            #diff_e = (f_plus - f_minus) * grad_output\n",
    "            #grad_e_bits = diff_e.sum() / (2.0 * delta)\n",
    "            \n",
    "            diff_e = -f_plus2 + 8*f_plus - 8*f_minus + f_minus2\n",
    "            grad_m_bits = diff_e.sum() / (12.0 * delta)\n",
    "        \n",
    "        return grad_x, grad_e_bits, grad_m_bits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQuantizedMLP(nn.Module):\n",
    "    def __init__(self, e_bits=4.0, m_bits=4.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, len(classes))\n",
    "\n",
    "        # Now we make them trainable:\n",
    "        self.e_bits = nn.Parameter(torch.tensor(e_bits))\n",
    "        self.m_bits = nn.Parameter(torch.tensor(m_bits))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        w1 = FakeFloatFunction.apply(self.fc1.weight, self.e_bits, self.m_bits)\n",
    "        b1 = FakeFloatFunction.apply(self.fc1.bias,   self.e_bits, self.m_bits)\n",
    "        x  = F.relu(F.linear(x, w1, b1))\n",
    "\n",
    "        w2 = FakeFloatFunction.apply(self.fc2.weight, self.e_bits, self.m_bits)\n",
    "        b2 = FakeFloatFunction.apply(self.fc2.bias,   self.e_bits, self.m_bits)\n",
    "        x  = F.linear(x, w2, b2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SimpleQuantizedCNN(nn.Module):\n",
    "    def __init__(self, e_bits=8.0, m_bits=23.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,  out_channels=32, kernel_size=3, padding=1)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, len(classes))\n",
    "        \n",
    "        self.i_e_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),\n",
    "        ])\n",
    "        \n",
    "        self.i_m_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),\n",
    "        ])\n",
    "        \n",
    "        self.w_e_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),\n",
    "        ])\n",
    "        \n",
    "        self.w_m_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),\n",
    "        ])\n",
    "          \n",
    "        self.b_e_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(e_bits)), requires_grad=True),\n",
    "        ])\n",
    "        \n",
    "        self.b_m_bits_param = nn.ParameterList([\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),  # layer 0\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),  # layer 1\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),\n",
    "            nn.Parameter(bit_to_param(torch.tensor(m_bits)), requires_grad=True),\n",
    "        ])\n",
    "              \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = FakeFloatFunction.apply(x, self.i_e_bits_param[0], self.i_m_bits_param[0])\n",
    "        w1 = FakeFloatFunction.apply(self.conv1.weight, self.w_e_bits_param[0], self.w_m_bits_param[0])\n",
    "        b1 = FakeFloatFunction.apply(self.conv1.bias,   self.b_e_bits_param[0], self.b_m_bits_param[0]) if self.conv1.bias is not None else None\n",
    "        x  = F.relu(F.conv2d(x, w1, b1, stride=1, padding=1))\n",
    "        x = F.max_pool2d(x, 2)  # 32x32 -> 16x16\n",
    "        \n",
    "        x = FakeFloatFunction.apply(x, self.i_e_bits_param[1], self.i_m_bits_param[1])\n",
    "        w2 = FakeFloatFunction.apply(self.conv2.weight, self.w_e_bits_param[1], self.w_m_bits_param[1])\n",
    "        b2 = FakeFloatFunction.apply(self.conv2.bias,   self.b_e_bits_param[1], self.b_m_bits_param[1]) if self.conv2.bias is not None else None\n",
    "        x  = F.relu(F.conv2d(x, w2, b2, stride=1, padding=1))\n",
    "        x = F.max_pool2d(x, 2)  # 16x16 -> 8x8\n",
    "        \n",
    "        x  = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = FakeFloatFunction.apply(x, self.i_e_bits_param[2], self.i_m_bits_param[2])\n",
    "        w_fc1 = FakeFloatFunction.apply(self.fc1.weight, self.w_e_bits_param[2], self.w_m_bits_param[2])\n",
    "        b_fc1 = FakeFloatFunction.apply(self.fc1.bias,   self.b_e_bits_param[2], self.b_m_bits_param[2])\n",
    "        x  = F.linear(x, w_fc1, b_fc1)\n",
    "\n",
    "        x = FakeFloatFunction.apply(x, self.i_e_bits_param[3], self.i_m_bits_param[3])\n",
    "        w_fc2 = FakeFloatFunction.apply(self.fc2.weight, self.w_e_bits_param[3], self.w_m_bits_param[3])\n",
    "        b_fc2 = FakeFloatFunction.apply(self.fc2.bias,   self.b_e_bits_param[3], self.b_m_bits_param[3])\n",
    "        x  = F.linear(x, w_fc2, b_fc2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def printBitWidths(self):\n",
    "        for i, (eb, mb) in enumerate(zip(self.i_e_bits_param, self.i_m_bits_param)):\n",
    "            print(f\"Layer {i} input e_bits (float) = {param_to_bit(eb).item()},  m_bits (float) = {param_to_bit(mb).item()}\")\n",
    "        for i, (eb, mb) in enumerate(zip(self.w_e_bits_param, self.w_m_bits_param)):\n",
    "            print(f\"Layer {i} weight e_bits (float) = {param_to_bit(eb).item()},  m_bits (float) = {param_to_bit(mb).item()}\")\n",
    "        for i, (eb, mb) in enumerate(zip(self.b_e_bits_param, self.b_m_bits_param)):\n",
    "            print(f\"Layer {i} bias e_bits (float) = {param_to_bit(eb).item()},  m_bits (float) = {param_to_bit(mb).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwidth_penalty(model, lambda_bw=1e-3):\n",
    "    \"\"\"\n",
    "    Computes a penalty term for the bitwidth parameters in 'model'.\n",
    "    'lambda_bw' is the weight/scale for this regularization.\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "    \n",
    "    # If the model has multiple layers with e_bits and m_bits in a ParameterList:\n",
    "    for eb, mb in zip(model.w_e_bits_param, model.w_m_bits_param):\n",
    "        # Option A: Penalize the raw float value (the \"continuous\" version)\n",
    "        \n",
    "        eb_ = eb - bit_to_param(torch.tensor(1.0))\n",
    "        mb_ = mb - bit_to_param(torch.tensor(1.0))\n",
    "        \n",
    "        penalty += eb_*eb_ + mb_*mb_\n",
    "        \n",
    "        # Option B (alternative): Penalize the rounded integer version\n",
    "        # penalty += torch.round(eb) + torch.round(mb)\n",
    "    \n",
    "    for eb, mb in zip(model.b_e_bits_param, model.b_m_bits_param):\n",
    "        eb_ = eb - bit_to_param(torch.tensor(1.0))\n",
    "        mb_ = mb - bit_to_param(torch.tensor(1.0))\n",
    "        penalty += eb_*eb_ + mb_*mb_\n",
    "\n",
    "    for eb, mb in zip(model.i_e_bits_param, model.i_m_bits_param):\n",
    "        eb_ = eb - bit_to_param(torch.tensor(1.0))\n",
    "        mb_ = mb - bit_to_param(torch.tensor(1.0))\n",
    "        penalty += eb_*eb_ + mb_*mb_\n",
    "                    \n",
    "    return lambda_bw * penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, lambda_bw=1e-3):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_ce = F.cross_entropy(output, target)\n",
    "        penalty_bw = bitwidth_penalty(model, lambda_bw) \n",
    "        loss = loss_ce + penalty_bw\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "                  f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}\"\n",
    "          f\" ({accuracy:.2f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.488237\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.361442\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.252461\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.274749\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.115431\n",
      "\n",
      "Test set: Average loss: 0.0516, Accuracy: 9837/10000 (98.37%)\n",
      "\n",
      "Layer 0 input e_bits (float) = 3.4134433269500732,  m_bits (float) = 18.93144416809082\n",
      "Layer 1 input e_bits (float) = 5.547675132751465,  m_bits (float) = 4.0709357261657715\n",
      "Layer 2 input e_bits (float) = 5.518855571746826,  m_bits (float) = 4.070084095001221\n",
      "Layer 3 input e_bits (float) = 3.8000473976135254,  m_bits (float) = 5.490313529968262\n",
      "Layer 0 weight e_bits (float) = 3.4714467525482178,  m_bits (float) = 7.524622440338135\n",
      "Layer 1 weight e_bits (float) = 3.8684537410736084,  m_bits (float) = 17.12986946105957\n",
      "Layer 2 weight e_bits (float) = 4.358256816864014,  m_bits (float) = 4.055737018585205\n",
      "Layer 3 weight e_bits (float) = 3.1102652549743652,  m_bits (float) = 8.874690055847168\n",
      "Layer 0 bias e_bits (float) = 3.2837061882019043,  m_bits (float) = 9.214698791503906\n",
      "Layer 1 bias e_bits (float) = 3.5657315254211426,  m_bits (float) = 10.116086959838867\n",
      "Layer 2 bias e_bits (float) = 3.109066963195801,  m_bits (float) = 8.972818374633789\n",
      "Layer 3 bias e_bits (float) = 3.4444057941436768,  m_bits (float) = 9.647500038146973\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.090506\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.105779\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.080248\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.112816\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.097277\n",
      "\n",
      "Test set: Average loss: 0.0473, Accuracy: 9842/10000 (98.42%)\n",
      "\n",
      "Layer 0 input e_bits (float) = 2.7626893520355225,  m_bits (float) = 18.936315536499023\n",
      "Layer 1 input e_bits (float) = 5.596075534820557,  m_bits (float) = 4.07171106338501\n",
      "Layer 2 input e_bits (float) = 5.500611305236816,  m_bits (float) = 4.07167387008667\n",
      "Layer 3 input e_bits (float) = 3.4416542053222656,  m_bits (float) = 4.07160758972168\n",
      "Layer 0 weight e_bits (float) = 3.1216044425964355,  m_bits (float) = 4.542872905731201\n",
      "Layer 1 weight e_bits (float) = 3.8695950508117676,  m_bits (float) = 17.129404067993164\n",
      "Layer 2 weight e_bits (float) = 4.278465270996094,  m_bits (float) = 4.071099758148193\n",
      "Layer 3 weight e_bits (float) = 3.166370153427124,  m_bits (float) = 4.069930553436279\n",
      "Layer 0 bias e_bits (float) = 3.1645686626434326,  m_bits (float) = 4.108381748199463\n",
      "Layer 1 bias e_bits (float) = 3.8683009147644043,  m_bits (float) = 9.394200325012207\n",
      "Layer 2 bias e_bits (float) = 3.1655545234680176,  m_bits (float) = 4.065515041351318\n",
      "Layer 3 bias e_bits (float) = 2.262655019760132,  m_bits (float) = 4.0699896812438965\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.075118\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.121310\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.163886\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.064254\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Create model\n",
    "# model = SimpleQuantizedMLP(e_bits=4.0, m_bits=4.0).to(device)\n",
    "model = SimpleQuantizedCNN(e_bits=8.0, m_bits=23.0).to(device)\n",
    "\n",
    "# Create optimizer (SGD or Adam)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train for some epochs\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    model.printBitWidths()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
