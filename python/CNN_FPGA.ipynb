{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pynq import Overlay, allocate, PL\n",
    "import struct\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pickle\n",
    "from array import array\n",
    "from os.path  import join\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "PL.reset()\n",
    "overlay = Overlay('cnn_fpga.bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP blocks : ['conv2d_3x3_0', 'dot_0', 'maxpooling2d_0', 'relu_0', 'axi_dma_conv2d', 'axi_dma_dot_in1', 'axi_dma_dot_in2', 'axi_dma_maxpool2d', 'axi_dma_relu', 'processing_system7_0']\n"
     ]
    }
   ],
   "source": [
    "print('IP blocks :', list(overlay.ip_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = './data/mnist/'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "        \n",
    "#\n",
    "# Show some random training and test images \n",
    "#\n",
    "#images_2_show = []\n",
    "#titles_2_show = []\n",
    "#for i in range(0, 10):\n",
    "#    r = random.randint(1, 60000)\n",
    "#    images_2_show.append(x_train[r])\n",
    "#    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "#for i in range(0, 5):\n",
    "#    r = random.randint(1, 10000)\n",
    "#    images_2_show.append(x_test[r])        \n",
    "#    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "#show_images(images_2_show, titles_2_show)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "uint8\n",
      "1.0\n",
      "float32\n",
      "(1, 28, 28)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x_test[0])\n",
    "\n",
    "print(np.max(x))\n",
    "print(x.dtype)\n",
    "\n",
    "x = x[None, :, :]\n",
    "x = x.astype(np.float32)/255.0\n",
    "\n",
    "print(np.max(x))\n",
    "print(x.dtype)\n",
    "\n",
    "y = y_test[0]\n",
    "\n",
    "print(x.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    c = x.max()\n",
    "    logsumexp = np.log(np.exp(x - c).sum())\n",
    "    return x - c - logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x00\\x00 A\\x00\\x000A\\x00\\x00@A\\x00\\x00PA\\x00\\x00`A\\x00\\x00pA\\x00\\x00\\x80A\\x00\\x00\\x88A\\x00\\x00\\x90A'\n",
      "[10. 11. 12. 13. 14. 15. 16. 17. 18.]\n"
     ]
    }
   ],
   "source": [
    "def float_to_hex(f):\n",
    "    return struct.unpack('I', struct.pack('f', f))[0]\n",
    "\n",
    "def hex_to_float(f):\n",
    "    return struct.unpack('f', struct.pack('I', f))[0]\n",
    "\n",
    "def numpy_to_hex(f):\n",
    "    #assert f.shape\n",
    "    format_string = f'{f.shape[0]}f'\n",
    "    packed_data = struct.pack(format_string, *f)\n",
    "    return packed_data\n",
    "\n",
    "def hex_to_numpy(f, length):\n",
    "    format_string = f'{length}f'\n",
    "    unpacked_data = struct.unpack(format_string, f)\n",
    "    return np.array(unpacked_data)\n",
    "\n",
    "data_in = np.array([10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0], dtype=np.float32)\n",
    "data_hex = numpy_to_hex(data_in)\n",
    "print(data_hex)\n",
    "data_out = hex_to_numpy(data_hex, 9)\n",
    "print(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n",
      "conv1.weight: <class 'list'>\n",
      "conv1.bias: <class 'list'>\n",
      "conv2.weight: <class 'list'>\n",
      "conv2.bias: <class 'list'>\n",
      "fc1.weight: <class 'list'>\n",
      "fc1.bias: <class 'list'>\n",
      "fc2.weight: <class 'list'>\n",
      "fc2.bias: <class 'list'>\n",
      "32\n",
      "1\n",
      "[[0.03899523243308067, -0.4488823413848877, -0.34650886058807373], [0.14574839174747467, -0.34101587533950806, 0.293231338262558], [0.06066122651100159, 0.4158423840999603, 0.31059861183166504]]\n",
      "[[ 0.03899523 -0.44888234 -0.34650886]\n",
      " [ 0.14574839 -0.34101588  0.29323134]\n",
      " [ 0.06066123  0.41584238  0.31059861]]\n",
      "128\n",
      "12544\n",
      "0.0047676535323262215\n",
      "0.0047676535323262215\n"
     ]
    }
   ],
   "source": [
    "# Load the .pt file\n",
    "#with open(\"mnist_cnn.pt\", \"rb\") as f:\n",
    "#    data = pickle.load(f)\n",
    "\n",
    "#load the pre-processed pickle\n",
    "with open(\"mnist_cnn.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "# Print the loaded data structure\n",
    "print(type(data))\n",
    "print(data.keys())\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key in data:\n",
    "        print(f\"{key}: {type(data[key])}\")\n",
    "\n",
    "# Example: Convert a specific weight tensor to a NumPy array\n",
    "weight_key = \"conv1.weight\"  # Replace with the key you want to inspect\n",
    "\n",
    "weight_tensor = data[weight_key]\n",
    "print(len(weight_tensor))\n",
    "print(len(weight_tensor[0]))\n",
    "print(weight_tensor[0][0])\n",
    "# Convert to a NumPy array\n",
    "weight_array = np.array(weight_tensor[0][0])\n",
    "print(weight_array)\n",
    "\n",
    "# Example: Convert a specific weight tensor to a NumPy array\n",
    "weight_key = \"fc1.weight\"  # Replace with the key you want to inspect\n",
    "\n",
    "weight_tensor = data[weight_key]\n",
    "print(len(weight_tensor))\n",
    "print(len(weight_tensor[0]))\n",
    "print(weight_tensor[0][0])\n",
    "# Convert to a NumPy array\n",
    "weight_array = np.array(weight_tensor[0][0])\n",
    "print(weight_array)\n",
    "    \n",
    "#if hasattr(data[weight_key], 'numpy'):\n",
    "#    weight_array = data[weight_key].numpy()  # Convert directly to NumPy array\n",
    "#else:\n",
    "#    print(\"The data format isn't directly convertible. Raw data might need more processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __init__(self, overlay):\n",
    "        self.overlay = overlay\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.output_buffer = None\n",
    "        self.layer_ip = None\n",
    "        self.ip_dict = None\n",
    "        self.dma_send = None\n",
    "        self.dma_recv = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        start_time = time.time()\n",
    "        output = self.forward(x)\n",
    "        proc_time = time.time() - start_time\n",
    "        print(type(self).__name__, \" proc time: \", proc_time)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    #def get_register_offset(self, ip, parameter):\n",
    "    #    #print(self.overlay.ip_dict[ip]['registers'])\n",
    "    #    return self.overlay.ip_dict[ip]['registers'][parameter]['address_offset']\n",
    "        \n",
    "    def read_param_float(self, param):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        data = self.layer_ip.read(address)\n",
    "        return hex_to_float(data)\n",
    "        \n",
    "    def read_param_hex(self, param):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        data = self.layer_ip.read(address)\n",
    "        return data\n",
    "\n",
    "    def write_param_float(self, param, value):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        self.layer_ip.write(address, float_to_hex(value))\n",
    "    \n",
    "    def write_param_hex(self, param, value):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        self.layer_ip.write(address, value)\n",
    "    \n",
    "    def write_param_numpy(self, param, values):\n",
    "        #address = self.ip_dict['registers'][param]['address_offset']\n",
    "        #self.layer_ip.write(address, numpy_to_hex(values.reshape(-1)))\n",
    "        \n",
    "        val = values.reshape(-1)\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        for i in range(val.size):\n",
    "            #print(\"writting: \", val[i], \" to: \", address+4*i)\n",
    "            self.layer_ip.write(address+4*i, float_to_hex(val[i]))\n",
    "        \n",
    "    def read_param_numpy(self, param, length=1):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        data = []\n",
    "        for i in range(length):\n",
    "            data_ = self.layer_ip.read(offset=address + 4*i)\n",
    "            data.append(hex_to_float(data_))\n",
    "        #return hex_to_numpy(data)\n",
    "        #return hex_to_float(data)\n",
    "        return np.array(data, dtype=np.float32)\n",
    "        \n",
    "    def process_ip(self, in_buffer, out_buffer):\n",
    "        self.layer_ip.write(0x0, 0x01)\n",
    "        self.dma_send.transfer(in_buffer)\n",
    "        self.dma_recv.transfer(out_buffer)\n",
    "        #print(type(self).__name__, \" sending\")\n",
    "        self.dma_send.wait()\n",
    "        #print(type(self).__name__, \" recieving\")\n",
    "        self.dma_recv.wait()\n",
    "    \n",
    "    \n",
    "class Linear_(Module):\n",
    "    def __init__(self, overlay, in_size, out_size):\n",
    "        Module.__init__(self, overlay)\n",
    "        \n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.output_buffer = []\n",
    "        self.output_buffer.append(allocate(shape=(out_size,), dtype=np.float32))\n",
    "    \n",
    "        self.layer_ip = overlay.Linear_0\n",
    "        self.ip_dict = overlay.ip_dict['Linear_0']\n",
    "\n",
    "        self.dma_send = overlay.axi_dma_linear.sendchannel\n",
    "        self.dma_recv = overlay.axi_dma_linear.recvchannel\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert len(x) == 1 and x[0].shape[0] == self.in_size\n",
    "        \n",
    "        #print(\"weights: \", self.weights[0])\n",
    "        #print(\"bias: \", self.bias[0])\n",
    "                \n",
    "        self.write_param_numpy('Memory_weights', self.weights.reshape(-1))\n",
    "        self.write_param_numpy('Memory_bias', self.bias.reshape(-1))   \n",
    "        self.write_param_hex('in_size', self.in_size)\n",
    "        self.write_param_hex('out_size', self.out_size)\n",
    "        \n",
    "        self.process_ip(x[0], self.output_buffer[0])\n",
    "                            \n",
    "        return self.output_buffer\n",
    "   \n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, overlay, in_size, out_size):\n",
    "        Module.__init__(self, overlay)\n",
    "        \n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.output_buffer = allocate(shape=(out_size,), dtype=np.float32)\n",
    "        self.aux_weight_buffer = allocate(shape=(in_size, ), dtype=np.float32)\n",
    "\n",
    "        self.layer_ip = overlay.dot_0\n",
    "        self.ip_dict = overlay.ip_dict['dot_0']\n",
    "\n",
    "        self.dma_send_1 = overlay.axi_dma_dot_in1.sendchannel\n",
    "        self.dma_send_2 = overlay.axi_dma_dot_in2.sendchannel\n",
    "    \n",
    "    def setWeightsAndBias(self, weights, bias):\n",
    "        assert len(weights.shape) == 2\n",
    "        assert weights.shape[0] == self.out_size\n",
    "        assert weights.shape[1] == self.in_size\n",
    "        \n",
    "        assert len(bias.shape) == 1\n",
    "        assert bias.shape[0] == self.out_size\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def process_ip(self, in_buffer1, in_buffer2):\n",
    "        self.layer_ip.write(0x0, 0x01)\n",
    "        self.dma_send_1.transfer(in_buffer1)\n",
    "        self.dma_send_2.transfer(in_buffer2)\n",
    "        #print(type(self).__name__, \" sending\")\n",
    "        self.dma_send_1.wait()\n",
    "        self.dma_send_2.wait()\n",
    "        #print(type(self).__name__, \" recieving\")\n",
    "        #self.dma_recv.wait()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert len(x.shape) == 1\n",
    "        assert x.shape[0] == self.in_size\n",
    "        \n",
    "        self.write_param_hex('in_size', self.in_size)\n",
    "                \n",
    "        for i in range(self.out_size):         \n",
    "            #print(\"weights: \", self.weights[i, :])\n",
    "            #print(\"bias: \", self.bias[i])\n",
    "\n",
    "            self.aux_weight_buffer[:] = self.weights[i, :]\n",
    "            self.process_ip(x, self.aux_weight_buffer)\n",
    "\n",
    "            self.output_buffer[i] = self.read_param_float(\"result\") + self.bias[i]\n",
    "                    \n",
    "        return self.output_buffer\n",
    "        \n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self, overlay, in_height, in_width, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        Module.__init__(self, overlay)\n",
    "        \n",
    "        assert kernel_size == 3\n",
    "        assert stride == 1\n",
    "        \n",
    "        self.layer_ip = overlay.conv2d_3x3_0\n",
    "        self.ip_dict = overlay.ip_dict['conv2d_3x3_0']\n",
    "        self.dma_send = self.overlay.axi_dma_conv2d.sendchannel\n",
    "        self.dma_recv = self.overlay.axi_dma_conv2d.recvchannel\n",
    "        \n",
    "        self.in_width = in_width\n",
    "        self.in_height = in_height\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_width = in_width\n",
    "        self.out_height = in_height\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.output_buffer = allocate(shape=(self.out_channels, self.out_height, self.out_width), dtype=np.float32)\n",
    "        self.aux_in_buffer = allocate(shape=(self.in_height, self.in_width), dtype=np.float32)\n",
    "        self.aux_out_buffer = allocate(shape=(self.out_height, self.out_width), dtype=np.float32)\n",
    "        \n",
    "    def setWeightsAndBias(self, weights, bias):\n",
    "        assert len(weights.shape) == 4\n",
    "        assert weights.shape[0] == self.out_channels\n",
    "        assert weights.shape[1] == self.in_channels\n",
    "        assert weights.shape[2] == 3\n",
    "        assert weights.shape[3] == 3\n",
    "        \n",
    "        assert len(bias.shape) == 1\n",
    "        assert bias.shape[0] == self.out_channels\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert len(x.shape) == 3 \n",
    "        assert x.shape[0] == self.in_channels\n",
    "        assert x.shape[1] == self.in_height\n",
    "        assert x.shape[2] == self.in_width\n",
    "        \n",
    "        axi_lite_time = 0.0\n",
    "        memory_time = 0.0\n",
    "        process_time = 0.0\n",
    "        add_time = 0.0\n",
    "\n",
    "        for in_channel in range(self.in_channels):  \n",
    "                \n",
    "            start_time = time.time()\n",
    "            self.aux_in_buffer[:, :] = x[in_channel, :, :]\n",
    "            memory_time += time.time() - start_time\n",
    "            \n",
    "            for out_channel in range(self.out_channels):   \n",
    "                #print(\"weights: \", self.weights[out_channel, in_channel, :, :])\n",
    "                #print(\"bias: \", self.bias[out_channel])\n",
    "                start_time = time.time()\n",
    "                self.write_param_numpy('Memory_weights', self.weights[out_channel, in_channel, :, :])  \n",
    "                axi_lite_time += time.time() - start_time\n",
    "                #self.write_param_float('bias', self.bias[out_channel])   \n",
    "                #print(self.read_param_numpy('Memory_weights', 9))                \n",
    "                start_time = time.time()\n",
    "                self.process_ip(self.aux_in_buffer, self.aux_out_buffer)\n",
    "                process_time += time.time() - start_time\n",
    "                \n",
    "                start_time = time.time()\n",
    "                if in_channel == 0:\n",
    "                    self.output_buffer[out_channel, :, :] = self.aux_out_buffer\n",
    "                else:\n",
    "                    self.output_buffer[out_channel, :, :] += self.aux_out_buffer\n",
    "                add_time += time.time() - start_time\n",
    "        \n",
    "        for out_channel in range(self.out_channels):      \n",
    "            self.output_buffer[out_channel, :, :] += self.bias[out_channel]\n",
    "        \n",
    "        print(\"axi_lite_time: \", axi_lite_time)\n",
    "        print(\"memory_time: \", memory_time)\n",
    "        print(\"process_time: \", process_time)\n",
    "        print(\"add_time: \", add_time)\n",
    "        \n",
    "        return self.output_buffer        \n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self, overlay, data_size):\n",
    "        Module.__init__(self, overlay)\n",
    "    \n",
    "        #fpga specific\n",
    "        self.layer_ip = overlay.relu_0\n",
    "        self.ip_dict = overlay.ip_dict['relu_0']\n",
    "        self.dma_send = overlay.axi_dma_relu.sendchannel\n",
    "        self.dma_recv = overlay.axi_dma_relu.recvchannel\n",
    "        \n",
    "        self.data_size = data_size\n",
    "        self.output_buffer = allocate(shape=(self.data_size,), dtype=np.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert x.size == self.data_size \n",
    "        \n",
    "        self.write_param_hex('data_size', self.data_size)\n",
    "\n",
    "        self.process_ip(x.reshape(-1), self.output_buffer)\n",
    "        self.output_buffer = self.output_buffer.reshape(x.shape)\n",
    "            \n",
    "        return self.output_buffer\n",
    "\n",
    "\n",
    "class MaxPooling2D(Module):\n",
    "    def __init__(self, overlay, in_height, in_width, in_channels):\n",
    "        Module.__init__(self, overlay)\n",
    "    \n",
    "        self.layer_ip = overlay.maxpooling2d_0\n",
    "        self.ip_dict = overlay.ip_dict['maxpooling2d_0']\n",
    "        self.dma_send = overlay.axi_dma_maxpool2d.sendchannel\n",
    "        self.dma_recv = overlay.axi_dma_maxpool2d.recvchannel\n",
    "        \n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_height = int(in_height/2)\n",
    "        self.out_width = int(in_width/2)\n",
    "        self.out_channels = in_channels\n",
    "                \n",
    "        self.output_buffer = allocate(shape=(self.out_channels, self.out_height, self.out_width), dtype=np.float32)\n",
    "        \n",
    "        self.aux_in_buffer = allocate(shape=(self.in_height, self.in_width), dtype=np.float32)\n",
    "        self.aux_out_buffer = allocate(shape=(self.out_height, self.out_width), dtype=np.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert len(x.shape) == 3\n",
    "        assert x.shape[0] == self.in_channels\n",
    "        assert x.shape[1] == self.in_height\n",
    "        assert x.shape[2] == self.in_width\n",
    "        \n",
    "        for channel in range(self.in_channels):\n",
    "            self.aux_in_buffer[:, :] = x[channel, :, :]\n",
    "            self.process_ip(self.aux_in_buffer, self.aux_out_buffer)\n",
    "            self.output_buffer[channel, :, :] = self.aux_out_buffer[:, :]\n",
    "        \n",
    "        return self.output_buffer\n",
    "\n",
    "    \n",
    "class Net(Module):\n",
    "    def __init__(self, overlay):\n",
    "        #super(Net, self).__init__()\n",
    "        Module.__init__(self, overlay)\n",
    "        self.conv1 = Conv2d(overlay, 28, 28, 1, 32, 3, 1)\n",
    "        self.relu1 = ReLU(overlay, 28*28*32)\n",
    "        self.conv2 = Conv2d(overlay, 28, 28, 32, 64, 3, 1)\n",
    "        self.relu2 = ReLU(overlay, 28*28*64)\n",
    "        self.max_pool2d = MaxPooling2D(overlay, 28, 28, 64)\n",
    "                \n",
    "        #self.dropout1 = Dropout(0.25)\n",
    "        #self.dropout2 = Dropout(0.5)\n",
    "        self.fc1 = Linear(overlay, 14*14*64, 128)\n",
    "        self.relu3 = ReLU(overlay, 128)\n",
    "        self.fc2 = Linear(overlay, 128, 10)\n",
    "            \n",
    "    def load_state_dict(self, pickle_file):\n",
    "        #load the pre-processed pickle\n",
    "        with open(pickle_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "    \n",
    "        conv1_weight = np.array(data[\"conv1.weight\"], dtype=np.float32)\n",
    "        conv1_bias = np.array(data[\"conv1.bias\"], dtype=np.float32)\n",
    "        \n",
    "        self.conv1.setWeightsAndBias(conv1_weight, conv1_bias)\n",
    "        \n",
    "        conv2_weight = np.array(data[\"conv2.weight\"], dtype=np.float32)\n",
    "        conv2_bias = np.array(data[\"conv2.bias\"], dtype=np.float32)\n",
    "        \n",
    "        self.conv2.setWeightsAndBias(conv2_weight, conv2_bias)\n",
    "          \n",
    "        fc1_weight = np.array(data[\"fc1.weight\"], dtype=np.float32)\n",
    "        fc1_bias = np.array(data[\"fc1.bias\"], dtype=np.float32)\n",
    "        \n",
    "        #print(\"weight shape: \", fc1_weight.shape)\n",
    "        #print(\"bias shape: \", fc1_bias.shape)\n",
    "        self.fc1.setWeightsAndBias(fc1_weight, fc1_bias)\n",
    "        \n",
    "        fc2_weight = np.array(data[\"fc2.weight\"], dtype=np.float32)\n",
    "        fc2_bias = np.array(data[\"fc2.bias\"], dtype=np.float32)\n",
    "        \n",
    "        self.fc2.setWeightsAndBias(fc2_weight, fc2_bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #print(\"input: \", x)\n",
    "        x = self.conv1(x)\n",
    "        #print(\"after conv1: \", x)\n",
    "        x = self.relu1(x)\n",
    "        #print(\"after relu: \", x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        #x = self.dropout1(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        x = x.reshape(-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = log_softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axi_lite_time:  0.018476009368896484\n",
      "memory_time:  0.00035881996154785156\n",
      "process_time:  0.037863731384277344\n",
      "add_time:  0.006281137466430664\n",
      "Conv2d  proc time:  0.12586045265197754\n",
      "ReLU  proc time:  0.0018815994262695312\n",
      "axi_lite_time:  0.9954249858856201\n",
      "memory_time:  0.008063077926635742\n",
      "process_time:  1.774604320526123\n",
      "add_time:  0.8684813976287842\n",
      "Conv2d  proc time:  3.716779947280884\n",
      "ReLU  proc time:  0.0032351016998291016\n",
      "MaxPooling2D  proc time:  0.08457684516906738\n",
      "Linear  proc time:  0.19442224502563477\n",
      "ReLU  proc time:  0.0027070045471191406\n",
      "Linear  proc time:  0.012608528137207031\n",
      "Net  proc time:  4.159952402114868\n",
      "gt:  7  pred:  7\n",
      "axi_lite_time:  0.015521526336669922\n",
      "memory_time:  0.00025081634521484375\n",
      "process_time:  0.027582406997680664\n",
      "add_time:  0.0054073333740234375\n",
      "Conv2d  proc time:  0.06194782257080078\n",
      "ReLU  proc time:  0.0014662742614746094\n",
      "axi_lite_time:  0.9802343845367432\n",
      "memory_time:  0.00795602798461914\n",
      "process_time:  1.7399275302886963\n",
      "add_time:  0.85587477684021\n",
      "Conv2d  proc time:  3.6540608406066895\n",
      "ReLU  proc time:  0.003086090087890625\n",
      "MaxPooling2D  proc time:  0.08325529098510742\n",
      "Linear  proc time:  0.19341492652893066\n",
      "ReLU  proc time:  0.0027065277099609375\n",
      "Linear  proc time:  0.012570619583129883\n",
      "Net  proc time:  4.016000509262085\n",
      "gt:  2  pred:  2\n",
      "axi_lite_time:  0.015452384948730469\n",
      "memory_time:  0.000247955322265625\n",
      "process_time:  0.027329683303833008\n",
      "add_time:  0.0053844451904296875\n",
      "Conv2d  proc time:  0.061574459075927734\n",
      "ReLU  proc time:  0.0014755725860595703\n",
      "axi_lite_time:  0.9825658798217773\n",
      "memory_time:  0.007951021194458008\n",
      "process_time:  1.7341620922088623\n",
      "add_time:  0.8555338382720947\n",
      "Conv2d  proc time:  3.650397539138794\n",
      "ReLU  proc time:  0.0031461715698242188\n",
      "MaxPooling2D  proc time:  0.08310437202453613\n",
      "Linear  proc time:  0.19272136688232422\n",
      "ReLU  proc time:  0.002902507781982422\n",
      "Linear  proc time:  0.012497901916503906\n",
      "Net  proc time:  4.011179447174072\n",
      "gt:  1  pred:  1\n",
      "axi_lite_time:  0.01547861099243164\n",
      "memory_time:  0.0002510547637939453\n",
      "process_time:  0.027410268783569336\n",
      "add_time:  0.005368471145629883\n",
      "Conv2d  proc time:  0.06170296669006348\n",
      "ReLU  proc time:  0.0014722347259521484\n",
      "axi_lite_time:  0.9809737205505371\n",
      "memory_time:  0.00800323486328125\n",
      "process_time:  1.7412941455841064\n",
      "add_time:  0.8587799072265625\n",
      "Conv2d  proc time:  3.658841609954834\n",
      "ReLU  proc time:  0.00322723388671875\n",
      "MaxPooling2D  proc time:  0.08369016647338867\n",
      "Linear  proc time:  0.19284296035766602\n",
      "ReLU  proc time:  0.0027756690979003906\n",
      "Linear  proc time:  0.012478113174438477\n",
      "Net  proc time:  4.020426511764526\n",
      "gt:  0  pred:  0\n",
      "axi_lite_time:  0.01544809341430664\n",
      "memory_time:  0.0002498626708984375\n",
      "process_time:  0.02747201919555664\n",
      "add_time:  0.005410432815551758\n",
      "Conv2d  proc time:  0.06179356575012207\n",
      "ReLU  proc time:  0.0015420913696289062\n",
      "axi_lite_time:  0.9808790683746338\n",
      "memory_time:  0.008013010025024414\n",
      "process_time:  1.7368743419647217\n",
      "add_time:  0.8640058040618896\n",
      "Conv2d  proc time:  3.65991473197937\n",
      "ReLU  proc time:  0.0018007755279541016\n",
      "MaxPooling2D  proc time:  0.0850822925567627\n",
      "Linear  proc time:  0.19316530227661133\n",
      "ReLU  proc time:  0.0027055740356445312\n",
      "Linear  proc time:  0.012479066848754883\n",
      "Net  proc time:  4.021876096725464\n",
      "gt:  4  pred:  4\n",
      "axi_lite_time:  0.015418052673339844\n",
      "memory_time:  0.00024890899658203125\n",
      "process_time:  0.027329206466674805\n",
      "add_time:  0.005301237106323242\n",
      "Conv2d  proc time:  0.06146883964538574\n",
      "ReLU  proc time:  0.0014841556549072266\n",
      "axi_lite_time:  0.9812686443328857\n",
      "memory_time:  0.008045673370361328\n",
      "process_time:  1.7374932765960693\n",
      "add_time:  0.8642714023590088\n",
      "Conv2d  proc time:  3.661064624786377\n",
      "ReLU  proc time:  0.0030727386474609375\n",
      "MaxPooling2D  proc time:  0.0838165283203125\n",
      "Linear  proc time:  0.1927204132080078\n",
      "ReLU  proc time:  0.0027036666870117188\n",
      "Linear  proc time:  0.01260685920715332\n",
      "Net  proc time:  4.022340297698975\n",
      "gt:  1  pred:  1\n",
      "axi_lite_time:  0.01549530029296875\n",
      "memory_time:  0.0002465248107910156\n",
      "process_time:  0.02740335464477539\n",
      "add_time:  0.005312681198120117\n",
      "Conv2d  proc time:  0.06158447265625\n",
      "ReLU  proc time:  0.0014803409576416016\n",
      "axi_lite_time:  0.9825363159179688\n",
      "memory_time:  0.008230447769165039\n",
      "process_time:  1.7477309703826904\n",
      "add_time:  0.8634331226348877\n",
      "Conv2d  proc time:  3.67223858833313\n",
      "ReLU  proc time:  0.003079652786254883\n",
      "MaxPooling2D  proc time:  0.08360409736633301\n",
      "Linear  proc time:  0.1925508975982666\n",
      "ReLU  proc time:  0.0027217864990234375\n",
      "Linear  proc time:  0.012562990188598633\n",
      "Net  proc time:  4.033249855041504\n",
      "gt:  4  pred:  4\n",
      "axi_lite_time:  0.015488624572753906\n",
      "memory_time:  0.00024700164794921875\n",
      "process_time:  0.02738809585571289\n",
      "add_time:  0.0053522586822509766\n",
      "Conv2d  proc time:  0.061654090881347656\n",
      "ReLU  proc time:  0.0014851093292236328\n",
      "axi_lite_time:  0.9942512512207031\n",
      "memory_time:  0.007974386215209961\n",
      "process_time:  1.7441060543060303\n",
      "add_time:  0.8627645969390869\n",
      "Conv2d  proc time:  3.6791815757751465\n",
      "ReLU  proc time:  0.0030755996704101562\n",
      "MaxPooling2D  proc time:  0.08356809616088867\n",
      "Linear  proc time:  0.19257187843322754\n",
      "ReLU  proc time:  0.0027124881744384766\n",
      "Linear  proc time:  0.012543916702270508\n",
      "Net  proc time:  4.040190696716309\n",
      "gt:  9  pred:  9\n",
      "axi_lite_time:  0.015517234802246094\n",
      "memory_time:  0.0002474784851074219\n",
      "process_time:  0.027367353439331055\n",
      "add_time:  0.0052988529205322266\n",
      "Conv2d  proc time:  0.06160569190979004\n",
      "ReLU  proc time:  0.0014886856079101562\n",
      "axi_lite_time:  0.9881424903869629\n",
      "memory_time:  0.007900476455688477\n",
      "process_time:  1.7434179782867432\n",
      "add_time:  0.8594827651977539\n",
      "Conv2d  proc time:  3.668639898300171\n",
      "ReLU  proc time:  0.003061532974243164\n",
      "MaxPooling2D  proc time:  0.0834648609161377\n",
      "Linear  proc time:  0.1921706199645996\n",
      "ReLU  proc time:  0.002696514129638672\n",
      "Linear  proc time:  0.012485980987548828\n",
      "Net  proc time:  4.029006719589233\n",
      "gt:  5  pred:  5\n",
      "axi_lite_time:  0.015422582626342773\n",
      "memory_time:  0.00024509429931640625\n",
      "process_time:  0.027256488800048828\n",
      "add_time:  0.005339860916137695\n",
      "Conv2d  proc time:  0.06139349937438965\n",
      "ReLU  proc time:  0.0014846324920654297\n",
      "axi_lite_time:  0.989893913269043\n",
      "memory_time:  0.008020877838134766\n",
      "process_time:  1.743567705154419\n",
      "add_time:  0.8672337532043457\n",
      "Conv2d  proc time:  3.6788218021392822\n",
      "ReLU  proc time:  0.0030829906463623047\n",
      "MaxPooling2D  proc time:  0.08374261856079102\n",
      "Linear  proc time:  0.19196748733520508\n",
      "ReLU  proc time:  0.002752542495727539\n",
      "Linear  proc time:  0.012459278106689453\n",
      "Net  proc time:  4.039095401763916\n",
      "gt:  9  pred:  9\n"
     ]
    }
   ],
   "source": [
    "model = Net(overlay)\n",
    "model.load_state_dict(\"mnist_cnn.pkl\")\n",
    "\n",
    "input_buffer = allocate(shape=(1,28,28), dtype=np.float32)\n",
    "\n",
    "for i in range(10):\n",
    "    input_buffer[0, :, :] = np.array(x_test[i])\n",
    "    input_buffer = input_buffer/255.0\n",
    "    output = model(input_buffer)\n",
    "    prediction = np.argmax(output)\n",
    "    print(\"gt: \", y_test[i], \" pred: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image = x_test[1]\n",
    "imgplot = plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
