{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pynq import Overlay, allocate, PL\n",
    "import struct\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pickle\n",
    "from array import array\n",
    "from os.path  import join\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "PL.reset()\n",
    "overlay = Overlay('cnn_fpga.bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP blocks : ['conv2D_3x3_32x28x28', 'MaxPooling2D_32x28x28', 'ReLU', 'Linear_12544x128', 'Linear_128x10', 'axi_dma_conv2d_32x28x28_weights', 'axi_dma_conv2d_32x28x28_input', 'axi_dma_maxpool2d_32x28x28_input', 'axi_dma_conv2d_32x28x28_output', 'axi_dma_maxpool2d_32x28x28_output', 'axi_dma_relu_output', 'axi_dma_relu_input', 'axi_dma_linear_12544x128_weights', 'axi_dma_linear_12544x128_bias', 'axi_dma_linear_12544x128_input', 'axi_dma_linear_12544x128_output', 'axi_dma_linear_128x10_weights', 'axi_dma_linear_128x10_bias', 'axi_dma_linear_128x10_input', 'axi_dma_linear_128x10_output', 'processing_system7_0']\n"
     ]
    }
   ],
   "source": [
    "print('IP blocks :', list(overlay.ip_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = './data/mnist/'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "        \n",
    "#\n",
    "# Show some random training and test images \n",
    "#\n",
    "#images_2_show = []\n",
    "#titles_2_show = []\n",
    "#for i in range(0, 10):\n",
    "#    r = random.randint(1, 60000)\n",
    "#    images_2_show.append(x_train[r])\n",
    "#    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "#for i in range(0, 5):\n",
    "#    r = random.randint(1, 10000)\n",
    "#    images_2_show.append(x_test[r])        \n",
    "#    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "#show_images(images_2_show, titles_2_show)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "uint8\n",
      "1.0\n",
      "float32\n",
      "(1, 28, 28)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x_test[0])\n",
    "\n",
    "print(np.max(x))\n",
    "print(x.dtype)\n",
    "\n",
    "x = x[None, :, :]\n",
    "x = x.astype(np.float32)/255.0\n",
    "\n",
    "print(np.max(x))\n",
    "print(x.dtype)\n",
    "\n",
    "y = y_test[0]\n",
    "\n",
    "print(x.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    c = x.max()\n",
    "    logsumexp = np.log(np.exp(x - c).sum())\n",
    "    return x - c - logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x00\\x00 A\\x00\\x000A\\x00\\x00@A\\x00\\x00PA\\x00\\x00`A\\x00\\x00pA\\x00\\x00\\x80A\\x00\\x00\\x88A\\x00\\x00\\x90A'\n",
      "[10. 11. 12. 13. 14. 15. 16. 17. 18.]\n"
     ]
    }
   ],
   "source": [
    "def float_to_hex(f):\n",
    "    return struct.unpack('I', struct.pack('f', f))[0]\n",
    "\n",
    "def hex_to_float(f):\n",
    "    return struct.unpack('f', struct.pack('I', f))[0]\n",
    "\n",
    "def numpy_to_hex(f):\n",
    "    #assert f.shape\n",
    "    format_string = f'{f.shape[0]}f'\n",
    "    packed_data = struct.pack(format_string, *f)\n",
    "    return packed_data\n",
    "\n",
    "def hex_to_numpy(f, length):\n",
    "    format_string = f'{length}f'\n",
    "    unpacked_data = struct.unpack(format_string, f)\n",
    "    return np.array(unpacked_data)\n",
    "\n",
    "data_in = np.array([10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0], dtype=np.float32)\n",
    "data_hex = numpy_to_hex(data_in)\n",
    "print(data_hex)\n",
    "data_out = hex_to_numpy(data_hex, 9)\n",
    "print(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n",
      "conv1.weight: <class 'list'>\n",
      "conv1.bias: <class 'list'>\n",
      "conv2.weight: <class 'list'>\n",
      "conv2.bias: <class 'list'>\n",
      "fc1.weight: <class 'list'>\n",
      "fc1.bias: <class 'list'>\n",
      "fc2.weight: <class 'list'>\n",
      "fc2.bias: <class 'list'>\n",
      "32\n",
      "1\n",
      "[[0.03838984668254852, -0.39478814601898193, -0.3092019557952881], [0.15461517870426178, -0.3323744535446167, 0.2565317153930664], [0.03815767541527748, 0.3500036299228668, 0.26200997829437256]]\n",
      "[[ 0.03838985 -0.39478815 -0.30920196]\n",
      " [ 0.15461518 -0.33237445  0.25653172]\n",
      " [ 0.03815768  0.35000363  0.26200998]]\n",
      "128\n",
      "12544\n",
      "0.0047676535323262215\n",
      "0.0047676535323262215\n"
     ]
    }
   ],
   "source": [
    "# Load the .pt file\n",
    "#with open(\"mnist_cnn.pt\", \"rb\") as f:\n",
    "#    data = pickle.load(f)\n",
    "\n",
    "#load the pre-processed pickle\n",
    "with open(\"mnist_cnn.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "# Print the loaded data structure\n",
    "print(type(data))\n",
    "print(data.keys())\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key in data:\n",
    "        print(f\"{key}: {type(data[key])}\")\n",
    "\n",
    "# Example: Convert a specific weight tensor to a NumPy array\n",
    "weight_key = \"conv1.weight\"  # Replace with the key you want to inspect\n",
    "\n",
    "weight_tensor = data[weight_key]\n",
    "print(len(weight_tensor))\n",
    "print(len(weight_tensor[0]))\n",
    "print(weight_tensor[0][0])\n",
    "# Convert to a NumPy array\n",
    "weight_array = np.array(weight_tensor[0][0])\n",
    "print(weight_array)\n",
    "\n",
    "# Example: Convert a specific weight tensor to a NumPy array\n",
    "weight_key = \"fc1.weight\"  # Replace with the key you want to inspect\n",
    "\n",
    "weight_tensor = data[weight_key]\n",
    "print(len(weight_tensor))\n",
    "print(len(weight_tensor[0]))\n",
    "print(weight_tensor[0][0])\n",
    "# Convert to a NumPy array\n",
    "weight_array = np.array(weight_tensor[0][0])\n",
    "print(weight_array)\n",
    "    \n",
    "#if hasattr(data[weight_key], 'numpy'):\n",
    "#    weight_array = data[weight_key].numpy()  # Convert directly to NumPy array\n",
    "#else:\n",
    "#    print(\"The data format isn't directly convertible. Raw data might need more processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __init__(self, overlay):\n",
    "        self.overlay = overlay\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.output_buffer = None\n",
    "        self.layer_ip = None\n",
    "        self.ip_dict = None\n",
    "        self.dma_send = None\n",
    "        self.dma_recv = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        start_time = time.time()\n",
    "        output = self.forward(x)\n",
    "        proc_time = time.time() - start_time\n",
    "        print(type(self).__name__, \" proc time: \", proc_time)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    #def get_register_offset(self, ip, parameter):\n",
    "    #    #print(self.overlay.ip_dict[ip]['registers'])\n",
    "    #    return self.overlay.ip_dict[ip]['registers'][parameter]['address_offset']\n",
    "        \n",
    "    def read_param_float(self, param):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        data = self.layer_ip.read(address)\n",
    "        return hex_to_float(data)\n",
    "        \n",
    "    def read_param_hex(self, param):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        data = self.layer_ip.read(address)\n",
    "        return data\n",
    "\n",
    "    def write_param_float(self, param, value):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        self.layer_ip.write(address, float_to_hex(value))\n",
    "    \n",
    "    def write_param_hex(self, param, value):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        self.layer_ip.write(address, value)\n",
    "    \n",
    "    def write_param_numpy(self, param, values):\n",
    "        #address = self.ip_dict['registers'][param]['address_offset']\n",
    "        #self.layer_ip.write(address, numpy_to_hex(values.reshape(-1)))\n",
    "        \n",
    "        val = values.reshape(-1)\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        for i in range(val.size):\n",
    "            #print(\"writting: \", val[i], \" to: \", address+4*i)\n",
    "            self.layer_ip.write(address+4*i, float_to_hex(val[i]))\n",
    "        \n",
    "    def read_param_numpy(self, param, length=1):\n",
    "        address = self.ip_dict['registers'][param]['address_offset']\n",
    "        data = []\n",
    "        for i in range(length):\n",
    "            data_ = self.layer_ip.read(offset=address + 4*i)\n",
    "            data.append(hex_to_float(data_))\n",
    "        #return hex_to_numpy(data)\n",
    "        #return hex_to_float(data)\n",
    "        return np.array(data, dtype=np.float32)\n",
    "        \n",
    "    def process_ip(self, in_buffer, out_buffer):\n",
    "        self.layer_ip.write(0x0, 0x01)\n",
    "        self.dma_send.transfer(in_buffer)\n",
    "        self.dma_recv.transfer(out_buffer)\n",
    "        #print(type(self).__name__, \" sending\")\n",
    "        self.dma_send.wait()\n",
    "        #print(type(self).__name__, \" recieving\")\n",
    "        self.dma_recv.wait()\n",
    "\n",
    "        \n",
    "class Linear(Module):\n",
    "    def __init__(self, overlay, in_size, out_size):\n",
    "        Module.__init__(self, overlay)\n",
    "        \n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.output_buffer = allocate(shape=(out_size,), dtype=np.float32)\n",
    "        self.weights_buffer = allocate(shape=(out_size, in_size), dtype=np.float32)\n",
    "        self.bias_buffer = allocate(shape=(out_size,), dtype=np.float32)\n",
    "\n",
    "        if in_size == 12544 and out_size == 128:\n",
    "            self.layer_ip = overlay.Linear_12544x128\n",
    "            self.ip_dict = overlay.ip_dict['Linear_12544x128']\n",
    "            self.dma_send_input = overlay.axi_dma_linear_12544x128_input.sendchannel\n",
    "            self.dma_send_weights = overlay.axi_dma_linear_12544x128_weights.sendchannel\n",
    "            self.dma_send_bias = overlay.axi_dma_linear_12544x128_bias.sendchannel\n",
    "            self.dma_recv_output = overlay.axi_dma_linear_12544x128_output.recvchannel\n",
    "        \n",
    "        if in_size == 128 and out_size == 10:\n",
    "            self.layer_ip = overlay.Linear_128x10\n",
    "            self.ip_dict = overlay.ip_dict['Linear_128x10']\n",
    "            self.dma_send_input = overlay.axi_dma_linear_128x10_input.sendchannel\n",
    "            self.dma_send_weights = overlay.axi_dma_linear_128x10_weights.sendchannel\n",
    "            self.dma_send_bias = overlay.axi_dma_linear_128x10_bias.sendchannel\n",
    "            self.dma_recv_output = overlay.axi_dma_linear_128x10_output.recvchannel           \n",
    "\n",
    "    \n",
    "    def setWeightsAndBias(self, weights, bias):\n",
    "        assert len(weights.shape) == 2\n",
    "        assert weights.shape[0] == self.out_size\n",
    "        assert weights.shape[1] == self.in_size\n",
    "        \n",
    "        assert len(bias.shape) == 1\n",
    "        assert bias.shape[0] == self.out_size\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "        #weights should be input:output\n",
    "        self.weights_buffer[:] = np.transpose(weights, (1, 0))\n",
    "        self.bias_buffer[:] = bias\n",
    "        \n",
    "    def process_ip(self, weights_buffer, bias_buffer, input_buffer, output_buffer):\n",
    "        self.layer_ip.write(0x0, 0x01)\n",
    "        self.dma_send_input.transfer(input_buffer)\n",
    "        self.dma_send_weights.transfer(weights_buffer)\n",
    "        self.dma_send_bias.transfer(bias_buffer)\n",
    "        self.dma_recv_output.transfer(output_buffer)\n",
    "        #print(type(self).__name__, \" sending\")\n",
    "        #self.dma_send_input.wait()\n",
    "        #self.dma_send_weights.wait()\n",
    "        #self.dma_send_bias.wait()\n",
    "        self.dma_recv_output.wait()\n",
    "        #print(type(self).__name__, \" recieving\")\n",
    "        #self.dma_recv.wait()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert len(x.shape) == 1\n",
    "        assert x.shape[0] == self.in_size\n",
    "        \n",
    "        self.process_ip(self.weights_buffer, self.bias_buffer, x, self.output_buffer)\n",
    "            \n",
    "        return self.output_buffer\n",
    "        \n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self, overlay, in_height, in_width, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        Module.__init__(self, overlay)\n",
    "        \n",
    "        assert kernel_size == 3\n",
    "        assert stride == 1\n",
    "        assert padding == 1\n",
    "        assert out_channels % 32 == 0\n",
    "        \n",
    "        self.layer_ip = overlay.conv2D_3x3_32x28x28\n",
    "        self.ip_dict = overlay.ip_dict['conv2D_3x3_32x28x28']\n",
    "        self.dma_send_input = self.overlay.axi_dma_conv2d_32x28x28_input.sendchannel\n",
    "        self.dma_send_weights = self.overlay.axi_dma_conv2d_32x28x28_weights.sendchannel\n",
    "        self.dma_recv_output = self.overlay.axi_dma_conv2d_32x28x28_output.recvchannel\n",
    "        \n",
    "        self.in_width = in_width\n",
    "        self.in_height = in_height\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_width = in_width\n",
    "        self.out_height = in_height\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.output_buffer = allocate(shape=(self.out_channels, self.out_height, self.out_width), dtype=np.float32)\n",
    "        self.weights_buffer = allocate(shape=(32, 3, 3), dtype=np.float32)\n",
    "        self.aux_in_buffer = allocate(shape=(self.in_height, self.in_width), dtype=np.float32)\n",
    "        self.aux_out_buffer = allocate(shape=(self.out_height, self.out_width, 32), dtype=np.float32)\n",
    "        \n",
    "    def setWeightsAndBias(self, weights, bias):\n",
    "        assert len(weights.shape) == 4\n",
    "        assert weights.shape[0] == self.out_channels\n",
    "        assert weights.shape[1] == self.in_channels\n",
    "        assert weights.shape[2] == 3\n",
    "        assert weights.shape[3] == 3\n",
    "        \n",
    "        assert len(bias.shape) == 1\n",
    "        assert bias.shape[0] == self.out_channels\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def process_ip(self, in_buffer, weights_buffer, out_buffer):\n",
    "        self.layer_ip.write(0x0, 0x01)\n",
    "        self.dma_send_input.transfer(in_buffer)\n",
    "        self.dma_send_weights.transfer(weights_buffer)\n",
    "        self.dma_recv_output.transfer(out_buffer)\n",
    "        #print(type(self).__name__, \" sending\")\n",
    "        #self.dma_send_input.wait()\n",
    "        #self.dma_send_weights.wait()\n",
    "        self.dma_recv_output.wait()\n",
    "        #print(type(self).__name__, \" recieving\")\n",
    "        #self.dma_recv.wait()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert len(x.shape) == 3 \n",
    "        assert x.shape[0] == self.in_channels\n",
    "        assert x.shape[1] == self.in_height\n",
    "        assert x.shape[2] == self.in_width\n",
    "        \n",
    "        axi_lite_time = 0.0\n",
    "        memory_time = 0.0\n",
    "        process_time = 0.0\n",
    "        add_time = 0.0\n",
    "\n",
    "        for in_channel in range(self.in_channels):  \n",
    "    \n",
    "            for i in range(self.out_channels//32):\n",
    "        \n",
    "                start_time = time.time()\n",
    "                self.aux_in_buffer[:] = x[in_channel, :, :]\n",
    "                self.weights_buffer[:] = self.weights[i*32:(i+1)*32, in_channel, :, :]\n",
    "                memory_time += time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                self.process_ip(self.aux_in_buffer, self.weights_buffer, self.aux_out_buffer)\n",
    "                process_time += time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                if in_channel == 0:\n",
    "                    self.output_buffer[i*32:(i+1)*32, :, :] = np.transpose(self.aux_out_buffer, (2, 0, 1))\n",
    "                else:\n",
    "                    self.output_buffer[i*32:(i+1)*32, :, :] += np.transpose(self.aux_out_buffer, (2, 0, 1))\n",
    "                add_time += time.time() - start_time\n",
    "            \n",
    "        for out_channel in range(self.out_channels):      \n",
    "            self.output_buffer[out_channel, :, :] += self.bias[out_channel]\n",
    "        \n",
    "        print(\"axi_lite_time: \", axi_lite_time)\n",
    "        print(\"memory_time: \", memory_time)\n",
    "        print(\"process_time: \", process_time)\n",
    "        print(\"add_time: \", add_time)\n",
    "        \n",
    "        return self.output_buffer        \n",
    "    \n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self, overlay, data_size):\n",
    "        Module.__init__(self, overlay)\n",
    "    \n",
    "        #fpga specific\n",
    "        self.layer_ip = overlay.ReLU\n",
    "        self.ip_dict = overlay.ip_dict['ReLU']\n",
    "        self.dma_send = overlay.axi_dma_relu_input.sendchannel\n",
    "        self.dma_recv = overlay.axi_dma_relu_output.recvchannel\n",
    "        \n",
    "        self.data_size = data_size\n",
    "        self.output_buffer = allocate(shape=(self.data_size,), dtype=np.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert x.size == self.data_size \n",
    "        \n",
    "        self.write_param_hex('data_size', self.data_size)\n",
    "\n",
    "        self.process_ip(x.reshape(-1), self.output_buffer)\n",
    "        self.output_buffer = self.output_buffer.reshape(x.shape)\n",
    "            \n",
    "        return self.output_buffer\n",
    "\n",
    "\n",
    "class MaxPooling2D(Module):\n",
    "    def __init__(self, overlay, in_height, in_width, in_channels):\n",
    "        Module.__init__(self, overlay)\n",
    "    \n",
    "        assert in_channels % 32 == 0\n",
    "    \n",
    "        self.layer_ip = overlay.MaxPooling2D_32x28x28\n",
    "        self.ip_dict = overlay.ip_dict['MaxPooling2D_32x28x28']\n",
    "        self.dma_send = overlay.axi_dma_maxpool2d_32x28x28_input.sendchannel\n",
    "        self.dma_recv = overlay.axi_dma_maxpool2d_32x28x28_output.recvchannel\n",
    "        \n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_height = int(in_height/2)\n",
    "        self.out_width = int(in_width/2)\n",
    "        self.out_channels = in_channels\n",
    "                \n",
    "        self.output_buffer = allocate(shape=(self.out_channels, self.out_height, self.out_width), dtype=np.float32)\n",
    "        \n",
    "        self.aux_in_buffer = allocate(shape=(32, self.in_height, self.in_width), dtype=np.float32)\n",
    "        self.aux_out_buffer = allocate(shape=(32, self.out_height, self.out_width), dtype=np.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert len(x.shape) == 3\n",
    "        assert x.shape[0] == self.in_channels\n",
    "        assert x.shape[1] == self.in_height\n",
    "        assert x.shape[2] == self.in_width\n",
    "        \n",
    "        for i in range(self.out_channels//32):\n",
    "            self.aux_in_buffer[:, :, :] = x[i*32:(i + 1)*32, :, :]\n",
    "            self.process_ip(self.aux_in_buffer, self.aux_out_buffer)\n",
    "            self.output_buffer[i*32:(i+1)*32, :, :] = self.aux_out_buffer[:, :, :]\n",
    "        \n",
    "        return self.output_buffer\n",
    "\n",
    "    \n",
    "class Net(Module):\n",
    "    def __init__(self, overlay):\n",
    "        #super(Net, self).__init__()\n",
    "        Module.__init__(self, overlay)\n",
    "        self.conv1 = Conv2d(overlay, 28, 28, 1, 32, 3, 1)\n",
    "        self.relu1 = ReLU(overlay, 28*28*32)\n",
    "        self.conv2 = Conv2d(overlay, 28, 28, 32, 64, 3, 1)\n",
    "        self.relu2 = ReLU(overlay, 28*28*64)\n",
    "        self.max_pool2d = MaxPooling2D(overlay, 28, 28, 64)\n",
    "                \n",
    "        #self.dropout1 = Dropout(0.25)\n",
    "        #self.dropout2 = Dropout(0.5)\n",
    "        self.fc1 = Linear(overlay, 14*14*64, 128)\n",
    "        self.relu3 = ReLU(overlay, 128)\n",
    "        self.fc2 = Linear(overlay, 128, 10)\n",
    "            \n",
    "    def load_state_dict(self, pickle_file):\n",
    "        #load the pre-processed pickle\n",
    "        with open(pickle_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "    \n",
    "        conv1_weight = np.array(data[\"conv1.weight\"], dtype=np.float32)\n",
    "        conv1_bias = np.array(data[\"conv1.bias\"], dtype=np.float32)\n",
    "        \n",
    "        self.conv1.setWeightsAndBias(conv1_weight, conv1_bias)\n",
    "        \n",
    "        conv2_weight = np.array(data[\"conv2.weight\"], dtype=np.float32)\n",
    "        conv2_bias = np.array(data[\"conv2.bias\"], dtype=np.float32)\n",
    "        \n",
    "        self.conv2.setWeightsAndBias(conv2_weight, conv2_bias)\n",
    "          \n",
    "        fc1_weight = np.array(data[\"fc1.weight\"], dtype=np.float32)\n",
    "        fc1_bias = np.array(data[\"fc1.bias\"], dtype=np.float32)\n",
    "        \n",
    "        #print(\"weight shape: \", fc1_weight.shape)\n",
    "        #print(\"bias shape: \", fc1_bias.shape)\n",
    "        self.fc1.setWeightsAndBias(fc1_weight, fc1_bias)\n",
    "        \n",
    "        fc2_weight = np.array(data[\"fc2.weight\"], dtype=np.float32)\n",
    "        fc2_bias = np.array(data[\"fc2.bias\"], dtype=np.float32)\n",
    "        \n",
    "        self.fc2.setWeightsAndBias(fc2_weight, fc2_bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #print(\"input: \", x)\n",
    "        x = self.conv1(x)\n",
    "        #print(\"after conv1: \", x)\n",
    "        x = self.relu1(x)\n",
    "        #print(\"after relu: \", x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        #x = self.dropout1(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        x = x.reshape(-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = log_softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0005064010620117188\n",
      "process_time:  0.009675025939941406\n",
      "add_time:  0.0038187503814697266\n",
      "Conv2d  proc time:  0.03976607322692871\n",
      "ReLU  proc time:  0.006289482116699219\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0263364315032959\n",
      "process_time:  0.08896088600158691\n",
      "add_time:  0.2664346694946289\n",
      "Conv2d  proc time:  0.4081134796142578\n",
      "ReLU  proc time:  0.0031456947326660156\n",
      "MaxPooling2D  proc time:  0.004704713821411133\n",
      "Linear  proc time:  0.02150583267211914\n",
      "ReLU  proc time:  0.0013344287872314453\n",
      "Linear  proc time:  0.0014414787292480469\n",
      "Net  proc time:  0.508115291595459\n",
      "gt:  7  pred:  4\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0003974437713623047\n",
      "process_time:  0.0014996528625488281\n",
      "add_time:  0.0022013187408447266\n",
      "Conv2d  proc time:  0.016576051712036133\n",
      "ReLU  proc time:  0.0015299320220947266\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.02517843246459961\n",
      "process_time:  0.08627176284790039\n",
      "add_time:  0.26387572288513184\n",
      "Conv2d  proc time:  0.4012744426727295\n",
      "ReLU  proc time:  0.0032219886779785156\n",
      "MaxPooling2D  proc time:  0.004484891891479492\n",
      "Linear  proc time:  0.01850581169128418\n",
      "ReLU  proc time:  0.0012993812561035156\n",
      "Linear  proc time:  0.0013871192932128906\n",
      "Net  proc time:  0.4512782096862793\n",
      "gt:  2  pred:  5\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0003826618194580078\n",
      "process_time:  0.0013515949249267578\n",
      "add_time:  0.0021848678588867188\n",
      "Conv2d  proc time:  0.01619434356689453\n",
      "ReLU  proc time:  0.0014889240264892578\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.02496623992919922\n",
      "process_time:  0.08639717102050781\n",
      "add_time:  0.26163816452026367\n",
      "Conv2d  proc time:  0.3990492820739746\n",
      "ReLU  proc time:  0.006063699722290039\n",
      "MaxPooling2D  proc time:  0.004350185394287109\n",
      "Linear  proc time:  0.018442392349243164\n",
      "ReLU  proc time:  0.001237630844116211\n",
      "Linear  proc time:  0.0013689994812011719\n",
      "Net  proc time:  0.4510931968688965\n",
      "gt:  1  pred:  5\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0003604888916015625\n",
      "process_time:  0.0012936592102050781\n",
      "add_time:  0.002167940139770508\n",
      "Conv2d  proc time:  0.01559901237487793\n",
      "ReLU  proc time:  0.0014574527740478516\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.02860426902770996\n",
      "process_time:  0.08198356628417969\n",
      "add_time:  0.259052038192749\n",
      "Conv2d  proc time:  0.3946092128753662\n",
      "ReLU  proc time:  0.00312042236328125\n",
      "MaxPooling2D  proc time:  0.004443645477294922\n",
      "Linear  proc time:  0.01846480369567871\n",
      "ReLU  proc time:  0.0012354850769042969\n",
      "Linear  proc time:  0.0013208389282226562\n",
      "Net  proc time:  0.44315099716186523\n",
      "gt:  0  pred:  9\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0003685951232910156\n",
      "process_time:  0.0012941360473632812\n",
      "add_time:  0.002226114273071289\n",
      "Conv2d  proc time:  0.01575922966003418\n",
      "ReLU  proc time:  0.001432180404663086\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0250246524810791\n",
      "process_time:  0.08446073532104492\n",
      "add_time:  0.2593247890472412\n",
      "Conv2d  proc time:  0.3947455883026123\n",
      "ReLU  proc time:  0.003148317337036133\n",
      "MaxPooling2D  proc time:  0.004478931427001953\n",
      "Linear  proc time:  0.01851940155029297\n",
      "ReLU  proc time:  0.00130462646484375\n",
      "Linear  proc time:  0.0014371871948242188\n",
      "Net  proc time:  0.44386887550354004\n",
      "gt:  4  pred:  9\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.00038814544677734375\n",
      "process_time:  0.0013496875762939453\n",
      "add_time:  0.002180337905883789\n",
      "Conv2d  proc time:  0.0162808895111084\n",
      "ReLU  proc time:  0.0017113685607910156\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.025426864624023438\n",
      "process_time:  0.0853431224822998\n",
      "add_time:  0.26175379753112793\n",
      "Conv2d  proc time:  0.39852023124694824\n",
      "ReLU  proc time:  0.0031642913818359375\n",
      "MaxPooling2D  proc time:  0.004492521286010742\n",
      "Linear  proc time:  0.01849842071533203\n",
      "ReLU  proc time:  0.001294851303100586\n",
      "Linear  proc time:  0.0013854503631591797\n",
      "Net  proc time:  0.44843578338623047\n",
      "gt:  1  pred:  1\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.00038695335388183594\n",
      "process_time:  0.001352071762084961\n",
      "add_time:  0.002190113067626953\n",
      "Conv2d  proc time:  0.016220808029174805\n",
      "ReLU  proc time:  0.0015668869018554688\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.02484416961669922\n",
      "process_time:  0.08386087417602539\n",
      "add_time:  0.26442456245422363\n",
      "Conv2d  proc time:  0.39813780784606934\n",
      "ReLU  proc time:  0.0031905174255371094\n",
      "MaxPooling2D  proc time:  0.004360198974609375\n",
      "Linear  proc time:  0.018467187881469727\n",
      "ReLU  proc time:  0.0012354850769042969\n",
      "Linear  proc time:  0.001312255859375\n",
      "Net  proc time:  0.447368860244751\n",
      "gt:  4  pred:  4\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0003707408905029297\n",
      "process_time:  0.0012993812561035156\n",
      "add_time:  0.002176046371459961\n",
      "Conv2d  proc time:  0.015703201293945312\n",
      "ReLU  proc time:  0.001428365707397461\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.023930788040161133\n",
      "process_time:  0.08099150657653809\n",
      "add_time:  0.2583763599395752\n",
      "Conv2d  proc time:  0.3883330821990967\n",
      "ReLU  proc time:  0.0032906532287597656\n",
      "MaxPooling2D  proc time:  0.00448298454284668\n",
      "Linear  proc time:  0.01846599578857422\n",
      "ReLU  proc time:  0.0012276172637939453\n",
      "Linear  proc time:  0.0013082027435302734\n",
      "Net  proc time:  0.4371302127838135\n",
      "gt:  9  pred:  8\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0003662109375\n",
      "process_time:  0.0013585090637207031\n",
      "add_time:  0.0021703243255615234\n",
      "Conv2d  proc time:  0.01574420928955078\n",
      "ReLU  proc time:  0.0014336109161376953\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.023855924606323242\n",
      "process_time:  0.08080554008483887\n",
      "add_time:  0.2590484619140625\n",
      "Conv2d  proc time:  0.3886525630950928\n",
      "ReLU  proc time:  0.0031423568725585938\n",
      "MaxPooling2D  proc time:  0.0044231414794921875\n",
      "Linear  proc time:  0.018453121185302734\n",
      "ReLU  proc time:  0.0012331008911132812\n",
      "Linear  proc time:  0.0013129711151123047\n",
      "Net  proc time:  0.4372823238372803\n",
      "gt:  5  pred:  9\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0003619194030761719\n",
      "process_time:  0.001283884048461914\n",
      "add_time:  0.0024461746215820312\n",
      "Conv2d  proc time:  0.015990018844604492\n",
      "ReLU  proc time:  0.0014536380767822266\n",
      "axi_lite_time:  0.0\n",
      "memory_time:  0.0240478515625\n",
      "process_time:  0.08049845695495605\n",
      "add_time:  0.2589585781097412\n",
      "Conv2d  proc time:  0.38849496841430664\n",
      "ReLU  proc time:  0.003127574920654297\n",
      "MaxPooling2D  proc time:  0.0044460296630859375\n",
      "Linear  proc time:  0.01843857765197754\n",
      "ReLU  proc time:  0.0012297630310058594\n",
      "Linear  proc time:  0.0013065338134765625\n",
      "Net  proc time:  0.437361478805542\n",
      "gt:  9  pred:  2\n"
     ]
    }
   ],
   "source": [
    "model = Net(overlay)\n",
    "model.load_state_dict(\"mnist_cnn.pkl\")\n",
    "\n",
    "input_buffer = allocate(shape=(1,28,28), dtype=np.float32)\n",
    "\n",
    "for i in range(10):\n",
    "    input_buffer[0, :, :] = np.array(x_test[i])\n",
    "    input_buffer = input_buffer/255.0\n",
    "    output = model(input_buffer)\n",
    "    prediction = np.argmax(output)\n",
    "    print(\"gt: \", y_test[i], \" pred: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAANxUlEQVR4nO3de4xU93nG8ecBc7EwtqFgSjGygwOycSpDsiJx3YstN6nDH8GRckOJgyNHpGrcJhJSYrmV4igXWVVst1WjVCRGIZUvcn2JqWIlJsSR6wRhLy4BbJJAXOpgVmDEpuBWhd312z/2UG3wzpll5sycMe/3I41m5rxzznk18OyZmd+c+TkiBODsN6nuBgB0B2EHkiDsQBKEHUiCsANJnNPNnU31tJiuGd3cJZDK/+q/dTJOeLxaW2G3fYOkv5c0WdK3IuLOssdP1wy909e3s0sAJbbFloa1ll/G254s6euS3itpqaTVtpe2uj0AndXOe/YVkvZFxEsRcVLSg5JWVdMWgKq1E/YFkn495v6BYtlvsb3Wdr/t/iGdaGN3ANrRTtjH+xDgDd+9jYj1EdEXEX1TNK2N3QFoRzthPyBp4Zj7F0s62F47ADqlnbA/J2mx7bfYnirpI5I2VdMWgKq1PPQWEcO2b5X0A40OvW2IiBcq6wxApdoaZ4+IJyQ9UVEvADqIr8sCSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEV39KGq3Z/+WrS+sj0xtPzjn3yldL19161SMt9XTKZT/6RGl95rPnNqzN+4eftrVvnBmO7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsPWDwe4tL67uX/WPH9j3UeIh+Qn5+3bdK6/f1zW9Ye2jzn5SuO7Jnb0s9YXwc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZu6DZOPpPlj3YsX3/028Wldbv3vru0vqll5SfD//k0kdL6x+dOdCw9pWb55Suu+jzjLNXqa2w294v6bikEUnDEdFXRVMAqlfFkf26iDhSwXYAdBDv2YEk2g17SHrS9nbba8d7gO21tvtt9w/pRJu7A9Cqdl/GXxMRB21fJGmz7Z9HxNNjHxAR6yWtl6TzPbvN0y4AtKqtI3tEHCyuD0t6TNKKKpoCUL2Ww257hu2Zp25Leo+k3VU1BqBa7byMnyfpMduntnN/RHy/kq7eZIavf0dp/UdXfb3JFqaUVv9ucElp/akPl4x4Hjxcuu6Swf7S+qTp00vrX932+6X12+fsalgbnjVcui6q1XLYI+IlSVdV2AuADmLoDUiCsANJEHYgCcIOJEHYgSQ4xbUCry2YWlqf1ORvarOhtR+/r3x4a+SlX5TW27Hvi8tL6/fPvqvJFqY1rFz8fY413cSzDSRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM5egQu/s7W0/oH+j5XWPXistD48sP9MW6rMJ1f+sLR+3qTG4+joLRzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtm7YOTFX9bdQkP7v3J1af2WC7/WZAvlPzW9buBdDWszf7indN2RJnvGmeHIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM5+lvvNTeXj6D/5ePk4+gWTysfRt56YXFrf8eXGvzt/7rFnS9dFtZoe2W1vsH3Y9u4xy2bb3mx7b3E9q7NtAmjXRF7Gf1vSDactu03SlohYLGlLcR9AD2sa9oh4WtLR0xavkrSxuL1R0o3VtgWgaq1+QDcvIgYkqbi+qNEDba+13W+7f0gnWtwdgHZ1/NP4iFgfEX0R0TelZJI/AJ3VatgP2Z4vScX14epaAtAJrYZ9k6Q1xe01kh6vph0AndJ0nN32A5KulTTH9gFJX5B0p6SHbN8i6WVJH+xkk2jdkbdHab3ZOHoza378ydL6ku8ylt4rmoY9IlY3KF1fcS8AOoivywJJEHYgCcIOJEHYgSQIO5AEp7ieBU5uvqRhbevldzVZu3zo7aqta0rrV6z7VWmdn4PuHRzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtnfBM5ZdGlp/Utv/ZeGtVlNTmHd3uSXwi75UvlI+cjgYPkG0DM4sgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoyzvwlc9tArpfXlU1v/m716y5+X1pf87LmWt43ewpEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0HDK65urT+xXnNfvt9WsPKmv1/WrrmFZ/bV1rnd9/PHk2P7LY32D5se/eYZXfYfsX2juKysrNtAmjXRF7Gf1vSDeMsvycilhWXJ6ptC0DVmoY9Ip6WdLQLvQDooHY+oLvV9s7iZf6sRg+yvdZ2v+3+ITX5wTMAHdNq2L8h6TJJyyQNSGr4CVJErI+Ivojom1LyQRKAzmop7BFxKCJGIuJ1Sd+UtKLatgBUraWw254/5u77Je1u9FgAvaHpOLvtByRdK2mO7QOSviDpWtvLJIWk/ZI+1bkW3/zOWfB7pfU/+qttpfXzJrX+9mfri28trS8Z5Hz1LJqGPSJWj7P43g70AqCD+LoskARhB5Ig7EAShB1IgrADSXCKaxfsuX1haf27v/uvbW3/ul0fbFjjFFacwpEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0Ltr/vniaPaO8XfC74i9cb1oYHB9vaNs4eHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2c8CQ/MuaFibcnJBFzt5o5FXjzSsxYny6cA8rfz7B5PnzmmpJ0kamXthaX3vuqktb3siYsQNa5f/ZZPfIDh2rKV9cmQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZz8LfO/hDXW30NAf/Pt4kwCPOnLo/NJ1Z809Xlrf9o77W+qp1y39m1tL64s+t7Wl7TY9stteaPsp23tsv2D7M8Xy2bY3295bXM9qqQMAXTGRl/HDktZFxBWS3iXp07aXSrpN0paIWCxpS3EfQI9qGvaIGIiI54vbxyXtkbRA0ipJG4uHbZR0Y4d6BFCBM/qAzvalkpZL2iZpXkQMSKN/ECRd1GCdtbb7bfcPqfy70AA6Z8Jht32epEckfTYiJvxN/IhYHxF9EdE3pc0fVgTQugmF3fYUjQb9voh4tFh8yPb8oj5f0uHOtAigCk2H3mxb0r2S9kTE3WNKmyStkXRncf14Rzo8C6x68aOl9S1ve7hLnXTfT5c/UNu+/ydONqwNReOf356IlTtvLq3/147WT79d8Mxwy+uWmcg4+zWSbpK0y/aOYtntGg35Q7ZvkfSypMaThAOoXdOwR8QzkhqdaX99te0A6BS+LgskQdiBJAg7kARhB5Ig7EASnOLaBef+2X+U1q/8avkpjdHBf6WZlx8trXfyNNIr/+0TpfV4eUZb21/08GuNi8/uamvbs7S3rXodOLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKOiK7t7HzPjneaE+WATtkWW3Qsjo57lipHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiiadhtL7T9lO09tl+w/Zli+R22X7G9o7is7Hy7AFo1kekHhiWti4jnbc+UtN325qJ2T0R8rXPtAajKROZnH5A0UNw+bnuPpAWdbgxAtc7oPbvtSyUtl7StWHSr7Z22N9ie1WCdtbb7bfcP6UR73QJo2YTDbvs8SY9I+mxEHJP0DUmXSVqm0SP/XeOtFxHrI6IvIvqmaFr7HQNoyYTCbnuKRoN+X0Q8KkkRcSgiRiLidUnflLSic20CaNdEPo23pHsl7YmIu8csnz/mYe+XtLv69gBUZSKfxl8j6SZJu2zvKJbdLmm17WWSQtJ+SZ/qQH8AKjKRT+OfkTTe71A/UX07ADqFb9ABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeScER0b2f2q5L+c8yiOZKOdK2BM9OrvfVqXxK9tarK3i6JiLnjFboa9jfs3O6PiL7aGijRq731al8SvbWqW73xMh5IgrADSdQd9vU1779Mr/bWq31J9NaqrvRW63t2AN1T95EdQJcQdiCJWsJu+wbbv7C9z/ZtdfTQiO39tncV01D319zLBtuHbe8es2y27c229xbX486xV1NvPTGNd8k047U+d3VPf9719+y2J0v6paR3Szog6TlJqyPixa420oDt/ZL6IqL2L2DY/mNJr0n6TkS8rVj2t5KORsSdxR/KWRHx+R7p7Q5Jr9U9jXcxW9H8sdOMS7pR0s2q8bkr6etD6sLzVseRfYWkfRHxUkSclPSgpFU19NHzIuJpSUdPW7xK0sbi9kaN/mfpuga99YSIGIiI54vbxyWdmma81ueupK+uqCPsCyT9esz9A+qt+d5D0pO2t9teW3cz45gXEQPS6H8eSRfV3M/pmk7j3U2nTTPeM89dK9Oft6uOsI83lVQvjf9dExFvl/ReSZ8uXq5iYiY0jXe3jDPNeE9odfrzdtUR9gOSFo65f7GkgzX0Ma6IOFhcH5b0mHpvKupDp2bQLa4P19zP/+ulabzHm2ZcPfDc1Tn9eR1hf07SYttvsT1V0kckbaqhjzewPaP44ES2Z0h6j3pvKupNktYUt9dIerzGXn5Lr0zj3WiacdX83NU+/XlEdP0iaaVGP5H/laS/rqOHBn0tkvSz4vJC3b1JekCjL+uGNPqK6BZJvyNpi6S9xfXsHurtnyXtkrRTo8GaX1Nvf6jRt4Y7Je0oLivrfu5K+urK88bXZYEk+AYdkARhB5Ig7EAShB1IgrADSRB2IAnCDiTxfy43Cn4r4LvDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image = x_test[1]\n",
    "imgplot = plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
